{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "![transformerz](https://cdn.collider.com/wp-content/uploads/2017/06/transformers-5-optimus-prime-bumblebee.jpg)\n",
    "\n",
    "No... not this kind\n",
    "\n",
    "The Transformer is a non-recurrent model which has achieved SoTA results in sequence-to-sequence transduction problems. It is based entirely on attention. The recurrent nature of RNNs means that parallelisation is not possible as every hidden state relies on the hidden state before it. Impressively, because the Transformer is based on FCNNs, the possibility of parallelising the model is possible.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/500/1*do7YDFF2sads0p9BnjzrWA.png)\n",
    "\n",
    "The Transformer follows a typical Encoder/Decoder architecture. It takes an input sequence of symbols $(x_1, ..., x_n)$ and maps this to a sequence of continuous representations $\\mathbf{h} = (h_1, ..., h_n)$. Given $h$, the decoder generates a symbol one element at a time $(y_1, ..., y_m)$.\n",
    "\n",
    "### Encoder\n",
    "Each encoder layer in the Transformer consists of two sub-layers. The first of these layers is the _Multi-Head Self-Attention_ module, and the second is a module called a _Position-Wise Feed-Forward Network_. Immediately following each one of these modules is a _Residual Layer Normalization_.\n",
    "\n",
    "### Decoder\n",
    "A decoder layer is similar to an encoder layer, but it has one extra module inserted: _Masked Multi-Head Attention_. We will discuss the decoder more in a further session. \n",
    "\n",
    "\n",
    "At the heart of the Transformer is this concept known as _self-attention_. Let's look at the Transformer holistically and then see exactly what this is, and why and how it solves sequence-to-sequence tasks so effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Holistic Transformer\n",
    "\n",
    "We are attempting to solve a sequence to sequence translation task: German to English using the Transformer:\n",
    "\n",
    "![whole_transformer](images/transformer.png)\n",
    "\n",
    "The Transformer is comprised of a stack of encoders and a stack of decoders. The output from the final layer of the encoder stack is sent to the decoder. The input to the first encoder is done via a special embedding module. We will look at the decoder and the embedding module in a future session.\n",
    "\n",
    "![transformer_high_level](images/transformer_high_level.png)\n",
    "\n",
    "Within the encoder stack we have a set of connected encoders. The output of one encoder is sent as input to the next encoder. An encoder consists of two sub-layers. The first sub-layer consists of a _Multi-Head Self-Attention_ module, and the second, a _Feed-Forward_ module. Within each module, a _Residual Connection_ followed by a _Layer Normalization_ is immediately applied.\n",
    "\n",
    "![encoder_first_few](images/encoder_first_few.png)\n",
    "\n",
    "Let's deconstruct what this new terminology means one by one. We'll start with _self attention_ and _multi-head self-attention_. Then we'll look at _residual connections_ followed by _layer normalization_. After we've covered the _feed-forward_ module, you've managed to understand most of the techniques in the Transformer! The encoder is simply the 6 aformenetioned things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "At the heart of the Transformer is self-attention. Self-attention is a mechanism that allows each input in a sequence to look at the whole sequence to compute a representation of the sequence.\n",
    "\n",
    "...what?\n",
    "\n",
    "Ok. Let's look at the following sentence: `the animal didn't cross the street because it was too tired`. What does the `it` refer to in this sentence? The street or the animal? This is trivial for us as humans to answer but not for a machine. Wouldn't it be nice if we could have some way of the computer understanding what `it` referred to?\n",
    "\n",
    "This is what self-attention attempts to do. As the model processes each word in the input sequence, self-attention allows us to look at other words in the input sequence for ideas as to what we want to encode in the representation of this word.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n",
    "\n",
    "It is given by the formula:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax \\left( \\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "\n",
    "To make this clearer, think of how a hidden state in an RNN incorporates the representation of the previous words into the current representation. Self-attention is how the Transformer attempts to use other words (not just the previous words) to encode the meaning of a particular word\n",
    "\n",
    "\n",
    "![encoder_multihead](images/encoder_multihead.png)\n",
    "\n",
    "![vector_attn_score](images/vector_attn_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention in detail\n",
    "\n",
    "Self-attention is a representation which you can think of as a score. The intent is to have a representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. A bit complicated right? Hopefully by the end of this post, this meaning will be clear.\n",
    "\n",
    "Before talking about matrices, let's talk in terms of vectors. We'll have the sequence length be of dimension $T$, and the encoding/embeddings of the word be $D$ dimensional.\n",
    "\n",
    "Note that because I'm focusing on the FIRST Encoder here, the encodings of our sequence is the embeddings. But as you move up the encoder stack, the encoding of the sequence is the output from the previous encoder. Again, this is $D$ dimensional.\n",
    "\n",
    "![word_encodings](images/word_encodings.png)\n",
    "\n",
    "\n",
    "Ok. Now we're going to create three vectors for EACH input: A query vector ($q$), a key vector ($k$), and a value vector ($v$). These will be $d_k$ dimensional. $d_k$ is typically $D/8$. As far as real values go, usually $D=512$, while $d_k=64$. In our example, what is $d_k$?\n",
    "\n",
    "Ok... so how how do we get $q$, $k$, $v$? We learn it of course!\n",
    "\n",
    "how... do we learn it? We need a weights matrix which will transform our encodings into these vectors.\n",
    "This means that $W^Q$, $W^K$ and $W^V$ are all $\\in \\mathbb{R}^{D×d_k}$\n",
    "\n",
    "![qkv_vectors](images/qkv_vectors.png)\n",
    "\n",
    "\n",
    "Ok, that's nice. We understand that $q$, $k$, $v$ are different projections of the same input now; but what do the query, key, value abstractions actually mean? They're useful terms we can use to think about attention. Let's look through the following so we can see the roles they play.\n",
    "\n",
    "Recall what we defined self-attention as earlier: A representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. For each word in our sequence, we will calculate a score by taking the dot product of the current word's query vector and the key vector for every word in the sequence.\n",
    "\n",
    "![w1_til_softmax](images/w1_til_softmax.png)\n",
    "\n",
    "Let's take stock of what we've done so far:\n",
    "- The $÷\\sqrt{d_k}$ is simply a practical scaling factor which leads to stabler gradients.\n",
    "- Softmax turns our scores into a probability distribution (each score is now between 0 and 1, and the sum of the scores = 1).\n",
    "\n",
    "We will now use these scores by multiplying them with their value vector. The intention here is that lower scoring words will have less weighting in the self-attention output as these words will now have a sense of \"irrelevantness\" (e.g. a low score like 0.0001 will \"cancel out\" its corresponding value vector).\n",
    "\n",
    "![w1_til_z1](images/w1_til_z1.png)\n",
    "\n",
    "Finally, the output for the current word is the summation of all the $softmax \\times v$ vectors. I.e. a weighted sum:\n",
    "\n",
    "![z_vector](images/z_vector.png)\n",
    "\n",
    "Ok. So that's self-attention in vector form. What about in terms of matrices?\n",
    "\n",
    "- Our input, $X$, is now a matrix of our sequence of words (i.e. $X \\in \\mathbb{R}^{T\\times D}$):\n",
    "![x_matrix](images/X_matrix_input.png)\n",
    "\n",
    "- $Q$, $K$, $V$ are now also matrices $\\in \\mathbb{R}^{T \\times d_k}$.\n",
    "- $W^Q$,$W^K$,$W^V$ stay $\\in \\mathbb{R}^{D \\times d_k}$.\n",
    "- We now simply obtain $Z \\in \\mathbb{R}^{T \\times d_k}$ by plugging $Q$, $K$, $V$ into our Attention formula.\n",
    "\n",
    "![Z_matrix](images/Z_matrix.png)\n",
    "\n",
    "\n",
    "- For the FIRST encoder, Q, K, V are determined by the embeddings of the input words\n",
    "- For the rest of the encoder stack, Q, K, V are determined by the output of the previous encoder\n",
    "- For the decoder stack, Q is determined in a similar fashion to the encoders. K and V, however, are passed from the final encoder to each of the decoders in the decoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) # (1, 3, 4)\n",
    "# Q_layer = nn.Linear(4, 3)\n",
    "# K_layer = nn.Linear(4, 3)\n",
    "# V_layer = nn.Linear(4, 3)\n",
    "\n",
    "# Q = Q_layer(encodings)\n",
    "# K = K_layer(encodings)\n",
    "# V = V_layer(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, dk=3):\n",
    "    Q_K_matmul = torch.matmul(Q, K.T)\n",
    "    matmul_scaled = Q_K_matmul/math.sqrt(dk)\n",
    "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attention(Q, K, V):\n",
    "    n_digits = 3\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    print ('Attention weights are:')\n",
    "    print (np.around(temp_attn, 2))\n",
    "    print ('Output is:')\n",
    "    print (np.around(temp_out, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k = torch.Tensor([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]])  # (4, 3)\n",
    "\n",
    "temp_v = torch.Tensor([[   1,0, 1],\n",
    "                      [  10,0, 2],\n",
    "                      [ 100,5, 0],\n",
    "                      [1000,6, 0]])  # (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0., 1., 0., 0.]])\n",
      "Output is:\n",
      "tensor([[10.,  0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = torch.Tensor([[0, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000]])\n",
      "Output is:\n",
      "tensor([[550.0000,   5.5000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = torch.Tensor([[0, 0, 10]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0.5000, 0.5000, 0.0000, 0.0000]])\n",
      "Output is:\n",
      "tensor([[5.5000, 0.0000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = torch.Tensor([[10, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000]])\n",
      "Output is:\n",
      "tensor([[550.0000,   5.5000,   0.0000],\n",
      "        [ 10.0000,   0.0000,   2.0000],\n",
      "        [  5.5000,   0.0000,   1.5000]])\n"
     ]
    }
   ],
   "source": [
    "temp_q = torch.Tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]])  # (3, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "Ok, we've just tackled self-attention, but the diagram tells us about something called multi-head attention. What is this?\n",
    "\n",
    "Conceptually, and even in terms of implementation, it's quite simple. Let's recap what we just obtained: $Z$. Each $z$ vector (i.e. for each vector $z_1, z_2, ..., z_T$ in $Z$) is a representation which bakes in all the words in the sequence including itself.\n",
    "\n",
    "A potential problem is that each $z_t$ _could_ be dominated by the representation for $t$'th word itself. This is what multihead attention solves.\n",
    "\n",
    "Instead of performing self-attention 1 time, multihead attention performs it multiple times. This means that we have multiple different attention representations. Of course to obtain these multiple different representations (i.e. $Z$s), we need to learn multiple $Q, K, V$s. Each $Q, K, V, Z$ pair is referred to as one head. Typically, we use 8 heads (i.e. perform self-attention 8 different times). To \"use\" these newly obtained $Z$s, we concatenate them and pass them through a linear layer to project them back to $D$ dimensionality\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, p_drop=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_k\n",
    "        self.d = d_model//num_heads\n",
    "        \n",
    "        # D\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Typically 8\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "        self.linear_Qs = [nn.Linear(d_model, self.d)\n",
    "                          for head in range(num_heads)]\n",
    "        self.linear_Ks = [nn.Linear(d_model, self.d)\n",
    "                          for head in range(num_heads)]\n",
    "        self.linear_Vs = [nn.Linear(d_model, self.d)\n",
    "                          for head in range(num_heads)]\n",
    "\n",
    "        self.mha_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        Q_K_matmul = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        matmul_scaled = Q_K_matmul/math.sqrt(self.d)\n",
    "\n",
    "        attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, input):\n",
    "        # shape(input) = [B x T x D]\n",
    "        \n",
    "        q = input\n",
    "        k = input\n",
    "        v = input\n",
    "        \n",
    "        # These will all be a list of Tensors\n",
    "        Q = [linear(q) for linear in self.linear_Qs]\n",
    "        K = [linear(k) for linear in self.linear_Ks]\n",
    "        V = [linear(v) for linear in self.linear_Vs]\n",
    "        # shape(Q) = shape(K) = shape(V) = [[B x T x d_k] * num_heads]\n",
    "\n",
    "        scores_per_head = []\n",
    "        attention_weights_per_head = []\n",
    "        for Q_, K_, V_ in zip(Q, K, V):\n",
    "            score, attention_weight = self.scaled_dot_product_attention(\n",
    "                Q_, K_, V_)\n",
    "            scores_per_head.append(score)\n",
    "            attention_weights_per_head.append(attention_weight)\n",
    "\n",
    "        concat_scores = torch.cat(scores_per_head, -1)\n",
    "        # shape(concat_scores) = [B x T x D]\n",
    "        output = self.dropout(self.mha_linear(concat_scores))\n",
    "        # shape(output) = [B x T x D]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Encodings:\n",
      "\t tensor([[[0.0000, 0.1000, 0.2000, 0.3000],\n",
      "         [1.0000, 1.1000, 1.2000, 1.3000],\n",
      "         [2.0000, 2.1000, 2.2000, 2.3000]]])\n"
     ]
    }
   ],
   "source": [
    "toy_encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
    "# shape(toy_encodings) = [B, T, D] = (1, 3, 4)\n",
    "print(\"Toy Encodings:\\n\\t\", toy_encodings)\n",
    "\n",
    "D_MODEL = toy_encodings.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy MHA: \n",
      "\t tensor([[[ 0.6393, -0.4274, -0.1076,  0.0000],\n",
      "         [ 0.0000, -0.0000, -0.1177,  0.2135],\n",
      "         [ 0.7447, -0.3604, -0.1274,  0.1824]]], grad_fn=<MulBackward0>)\n",
      "Toy MHA Shape: \n",
      "\t torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_MHA_layer = MultiHeadAttention(d_model=D_MODEL, num_heads=2)\n",
    "toy_MHA = toy_MHA_layer(toy_encodings)\n",
    "print(\"Toy MHA: \\n\\t\", toy_MHA)\n",
    "print(\"Toy MHA Shape: \\n\\t\", toy_MHA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some of these values zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "In a typical problem where we want to employ neural networks, we normalize (standardize) our data before feeding it into the network. This is usually done by subtracting the global mean and dividing by the global standard deviation of the data:\n",
    "$$x = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "It is also possible to normalize _within_ the activations or layers of the network. There are many approaches for doing so: _Batch Normalization, Layer Normalization, Instance Normalization_ and _Group Normalization_.\n",
    "\n",
    "We won't be diving into the details of layer normalization here as it's out of scope - the important thing to know is that some kind of normalization process over features internal to the model. A helper is provided for us in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.layer_norm = nn.LayerNorm(DIMENSIONALITY)\n",
    "# layer_normed = self.layer_norm(thing_to_layer_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read further about layer normalization, check out the following resources (order _mostly_ from most accessible to least). The first few resources touch on batch normalization in order to give a conceptual understanding of what layer normalization is attempting to do:\n",
    "- https://www.youtube.com/watch?v=DtEq44FTPM4\n",
    "- https://www.youtube.com/watch?v=tNIpEZLv_eg\n",
    "- https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/ (Recommended read)\n",
    "- https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "In theory, if we continue stacking layers in a neural network on top of each other, we expect the training error to decrease. However, the reality doesn't follow suit.\n",
    "\n",
    "[IAMEG]\n",
    "\n",
    "Resiudal connections solve these problems by introducing a _skip connection_ (or shortcut) between every other layer. All we need to do to implement a residual connection is add our residual input to our actual input:\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "To read further about residual connections and why they work, please refer to the following resources:\n",
    "- https://www.coursera.org/lecture/convolutional-neural-networks/resnets-HAhz9\n",
    "- https://www.coursera.org/lecture/convolutional-neural-networks/why-resnets-work-XAKNO\n",
    "- https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Implementing residual connections is deceivingly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, d_model=512, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x, res_input):\n",
    "        ln = self.layer_norm(res_input + x)\n",
    "        return self.dropout(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy AddNorm: \n",
      "\t tensor([[[ 0.0000, -0.0000, -0.3414,  0.5056],\n",
      "         [-1.2446, -0.5290, -0.6558,  2.4295],\n",
      "         [ 1.8030, -1.9345, -0.6962,  0.8276]]], grad_fn=<MulBackward0>)\n",
      "Toy AddNorm shape: \n",
      "\t torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_AddNorm_layer = AddNorm(d_model=D_MODEL)\n",
    "toy_AddNorm = toy_AddNorm_layer(toy_MHA, toy_encodings)\n",
    "print(\"Toy AddNorm: \\n\\t\", toy_AddNorm)\n",
    "print(\"Toy AddNorm shape: \\n\\t\", toy_AddNorm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking stock\n",
    "\n",
    "This brings us close to the end of our first sub-layer in the encoder (and about a 70% understanding of the new concepts in the Transformer).\n",
    "\n",
    "![encoder_first_few](images/encoder_first_few.png)\n",
    "\n",
    "The next sub-layer is relatively straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors pass their output from the first sublayer into a feed-forward network. They call this a position-wise feed-forward network because it \"is applied to each position separately and identically\". This means that they run a feed-forward network over a rank 3 tensor (including batch size), over the sequence dimension. \n",
    "\n",
    "All this means is that the SAME weights are applied to all tokens in the sequence.\n",
    "\n",
    "The authors use a two layered network given as:\n",
    "\n",
    "$$FFN(x) = max(0,xW_1+b_1)W_2+b_2$$\n",
    "\n",
    "With the first layer having a dimension of $d_{ff}=2048$ and the second as $D$. It's unclear why the inputs are projected to a larger dimension before being projected back down to $D$ dimensions, but [one source](https://graphdeeplearning.github.io/post/transformers-are-gnns/) suggests that it is a convergence trick which enables re-scaling of the feature vectors independently with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_FF = 2048\n",
    "D_FF = D_MODEL * 4\n",
    "P_DROP = 0.3\n",
    "\n",
    "class PointwiseFeedforward(nn.Module):\n",
    "    def __init__(self, d_model=D_MODEL, d_ff=D_FF, p_drop=P_DROP):\n",
    "        super().__init__()\n",
    "        self.pffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pffn(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy PFFN: \n",
      "\t tensor([[[ 0.0380,  0.0518, -0.3602,  0.0769],\n",
      "         [ 0.0561,  0.6201, -0.4566, -0.2010],\n",
      "         [ 0.3849,  0.3601, -0.7434, -0.1085]]], grad_fn=<AddBackward0>)\n",
      "Toy PFFN Shape: \n",
      "\t torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_PFFN_layer = PointwiseFeedforward(d_model=D_MODEL, d_ff=D_FF)\n",
    "toy_PFFN = toy_PFFN_layer(toy_AddNorm)\n",
    "print(\"Toy PFFN: \\n\\t\", toy_PFFN)\n",
    "print(\"Toy PFFN Shape: \\n\\t\", toy_PFFN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy AddNorm 2: \n",
      "\t tensor([[[ 0.0000,  0.0000, -0.0000,  1.8437],\n",
      "         [-1.2346,  0.0894, -0.0000,  0.0000],\n",
      "         [ 2.0159, -1.4088, -0.0000,  0.0000]]], grad_fn=<MulBackward0>)\n",
      "Toy AddNorm 2 Shape: \n",
      "\t torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_AddNorm_layer_2 = AddNorm(d_model=D_MODEL)\n",
    "toy_AddNorm_2 = toy_AddNorm_layer_2(toy_PFFN, toy_AddNorm)\n",
    "print(\"Toy AddNorm 2: \\n\\t\", toy_AddNorm_2)\n",
    "print(\"Toy AddNorm 2 Shape: \\n\\t\", toy_AddNorm_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "\n",
    "This is everything we require for one arbitrary Encoder layer! Let's code up a class which contains the aforementioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS=8\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=D_MODEL, num_heads=NUM_HEADS, d_ff=D_FF, p_drop=P_DROP):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "        self.MHA = MultiHeadAttention(\n",
    "            self.d_model, self.num_heads, self.p_drop)\n",
    "\n",
    "        self.addNorm1 = AddNorm(self.d_model, self.p_drop)\n",
    "        self.addNorm2 = AddNorm(self.d_model, self.p_drop)\n",
    "\n",
    "        self.PFFN = PointwiseFeedforward(\n",
    "            self.d_model, self.d_ff, self.p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mha, _ = self.MHA(x)\n",
    "        addNorm_1 = self.addNorm1(mha, x)\n",
    "\n",
    "        pffn = self.PFFN(addNorm_1)\n",
    "        addNorm_2 = self.addNorm2(pffn, addNorm_1)\n",
    "\n",
    "        return addNorm_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Lets talk about embedding the inputs\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "One thing we've not yet looked at is _Positional Encoding_. Recall that our model contains no recurrence and thus we need a way to make sense of the order of the sequence. To do so, we need to inject some information about the position of the tokens in the sequence. Positional Encoding is one strategy to do so.\n",
    "\n",
    "So, we want our model to add some information to each of our words (embeddings) which indicates its position in the sequence. What are some strategies for doing so?\n",
    "\n",
    "Well, we could just add the token position to the embedding (e.g. 1 for the first word, 2 for the second word, 3 for the third word). What are some issues with this approach?\n",
    "\n",
    "What about linearly assigning values between 0 and 1 to the token embedding? (e.g. for an eight-length sequence, the first word has 0.125 added to it, the second 0.25, the third 0.375 etc).\n",
    "\n",
    "The trick the authors propose is to add a $D$-dimensional vector to the embedding (which is also $D$-dimensional) instead of a single number. The vector which is added to the word embedding is __fixed__ - that is, it does NOT depend on the features of the word itself - only the position that it appears in.\n",
    "\n",
    "Let's dissect the formula it's given by:\n",
    "\n",
    "$$PE_{(pos,2i)}=sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n",
    "$$PE_{(pos,2i+1)}=cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n",
    "\n",
    "[IMAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, len_vocab, d_model=D_MODEL):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(len_vocab, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Embeddings: \n",
      "\t tensor([[[ 1.9631,  0.1769, -1.0451,  1.6996],\n",
      "         [ 1.2375,  1.9748,  1.8169,  0.1104],\n",
      "         [ 2.7404,  1.8952,  3.9096, -1.4101],\n",
      "         [-3.9316, -0.9797, -1.3554, -2.1137],\n",
      "         [-2.6030, -1.2361,  1.6496, -0.0630],\n",
      "         [-2.6030, -1.2361,  1.6496, -0.0630]]], grad_fn=<MulBackward0>)\n",
      "Toy Embeddings Shape: \n",
      "\t torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_vocab = torch.LongTensor([[1, 2, 3, 4, 0, 0]])\n",
    "\n",
    "toy_embedding_layer = Embeddings(5, d_model=D_MODEL)\n",
    "toy_embeddings = toy_embedding_layer(toy_vocab)\n",
    "print(\"Toy Embeddings: \\n\\t\", toy_embeddings)\n",
    "print(\"Toy Embeddings Shape: \\n\\t\", toy_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=D_MODEL, p_drop=P_DROP, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "\n",
    "        two_i = torch.arange(0, d_model, step=2).float()\n",
    "        div_term = torch.pow(10000, (two_i/d_model)).float()\n",
    "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # assigns the first argument to a class variable\n",
    "        # i.e. self.pe\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "        self.dropout = nn.Dropout(P_DROP)\n",
    "\n",
    "    # x is the input embedding\n",
    "    def forward(self, x):\n",
    "\n",
    "        # work through this line :S\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy PE: \n",
      "\t tensor([[[ 2.8044,  1.6813, -0.0000,  3.8566],\n",
      "         [ 2.9700,  0.0000,  2.6098,  1.5862],\n",
      "         [ 0.0000,  2.1130,  5.6137, -0.5861],\n",
      "         [-0.0000, -2.8139, -0.0000, -1.5916],\n",
      "         [-4.7997, -2.6996,  0.0000,  1.3375],\n",
      "         [-5.0885, -1.3606,  0.0000,  1.3368]]], grad_fn=<MulBackward0>)\n",
      "Toy PE Shape: \n",
      "\t torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_PE_layer = PositionalEncoding(d_model=D_MODEL)\n",
    "toy_PEs = toy_PE_layer(toy_embeddings)\n",
    "print(\"Toy PE: \\n\\t\", toy_PEs)\n",
    "print(\"Toy PE Shape: \\n\\t\", toy_PEs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder (and the Decoder)\n",
    "![](https://i.giphy.com/media/11GWLm7bE2fibC/giphy.webp)\n",
    "\n",
    "Cool! This brings us to the end of the Encoder part of the Transformer. There's one more thing we need to touch on regarding the Encoder, but we will look at that in a bit.\n",
    "\n",
    "Looking at the Transformer architecture again, we see the Decoder is quite similar to the Encoder. It has two subtle differences though. The __masked__ multi-head attention, and the multi-head attention module which receives inputs from the Encoder. Let's work through these respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Multihead Attention\n",
    "\n",
    "Think back to our Seq2Seq model from last week. During decoding, we have access to the whole source sentence, and the decoded tokens we've decoded SO FAR. Obviously we can't have access to future tokens because we don't know what they are yet...\n",
    "\n",
    "During training time however, we DO have access to future tokens because we have labelled pairs. To speed up training, the Transformer architecture enables us to feed in our whole target sequence to the model and use masking to tell the attention mechanism not to look at illegal (i.e. future) positions when considering the i'th token.\n",
    "\n",
    "Recall what Multihead Attention (MHA) did. It worked out a representation for each token in the sequence given all the tokens. Masking is the strategy we use to tell the network not to look at the positions past the most recent word that has been decoded.\n",
    "\n",
    "We will look at the Decoder Layer in its entirety in a bit, but for now let's focus on the masking process for MHA. We will implement this in our `MultiHeadAttention` module. Our mask is going to be the same shape as `Q_K_matmul` because this is what we are calculating attention over.\n",
    "\n",
    "The mask is a tensor with with a value of `-inf` at illegal locations. For one sample in our input, the shape of `Q_K_matumul` is `[T, T]`. Here, `T` refers to the sequence length of the target output. We will talk about test time later, so let's consider the training case right now. During training, we have access to the whole target sequence. So at every timestep (i.e. for every word) in the sequence, an illegal location would be all the future timesteps that we're not meant to have access to. Our matrix is `[T, T]`. So at the 1st timestep, we don't have access to any words from the 2nd timestep onwards. At the 2nd timestep, we don't have acess to any words from the 3rd timestep onwards.\n",
    "\n",
    "[IMAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(batch_size, seq_len):\n",
    "    mask = torch.ones((batch_size, seq_len, seq_len))\n",
    "    mask = mask.triu(1)\n",
    "    mask[mask == 1] = float('-inf')\n",
    "    return mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "toy_mask = create_mask(1, 10)\n",
    "print(\"TOY MASK: \\n\\t\", toy_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOY MATMUL:  tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "         [10., 11., 12., 13., 14., 15., 16., 17., 18., 19.],\n",
      "         [20., 21., 22., 23., 24., 25., 26., 27., 28., 29.],\n",
      "         [30., 31., 32., 33., 34., 35., 36., 37., 38., 39.],\n",
      "         [40., 41., 42., 43., 44., 45., 46., 47., 48., 49.],\n",
      "         [50., 51., 52., 53., 54., 55., 56., 57., 58., 59.],\n",
      "         [60., 61., 62., 63., 64., 65., 66., 67., 68., 69.],\n",
      "         [70., 71., 72., 73., 74., 75., 76., 77., 78., 79.],\n",
      "         [80., 81., 82., 83., 84., 85., 86., 87., 88., 89.],\n",
      "         [90., 91., 92., 93., 94., 95., 96., 97., 98., 99.]]])\n"
     ]
    }
   ],
   "source": [
    "toy_matmul = torch.arange(100).reshape(1, 10, 10).float()\n",
    "print(\"TOY MATMUL: \", toy_matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from our MultiHeadAttention class:\n",
    "def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "    Q_K_matmul = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    matmul_scaled = Q_K_matmul/math.sqrt(self.d)\n",
    "\n",
    "    if mask is not None:\n",
    "        matmul_scaled += mask\n",
    "\n",
    "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [10., 11., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [20., 21., 22., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [30., 31., 32., 33., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [40., 41., 42., 43., 44., -inf, -inf, -inf, -inf, -inf],\n",
       "         [50., 51., 52., 53., 54., 55., -inf, -inf, -inf, -inf],\n",
       "         [60., 61., 62., 63., 64., 65., 66., -inf, -inf, -inf],\n",
       "         [70., 71., 72., 73., 74., 75., 76., 77., -inf, -inf],\n",
       "         [80., 81., 82., 83., 84., 85., 86., 87., 88., -inf],\n",
       "         [90., 91., 92., 93., 94., 95., 96., 97., 98., 99.]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_matmul += toy_mask\n",
    "toy_matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention (again 🙄)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
