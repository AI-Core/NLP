{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math as m\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\"BATCH_SIZE\": 512, \n",
    "      \"D_MODEL\": 512, \n",
    "      \"P_DROP\": 0.1, \n",
    "      \"D_FF\": 2048, \n",
    "      \"HEADS\": 8, \n",
    "      \"LAYERS\": 6, \n",
    "      \"LR\": 1e-3, \n",
    "      \"EPOCHS\": 40}\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "![transformerz](https://cdn.collider.com/wp-content/uploads/2017/06/transformers-5-optimus-prime-bumblebee.jpg)\n",
    "\n",
    "No... not this kind\n",
    "\n",
    "The Transformer is a non-recurrent model which has achieved SoTA results in sequence-to-sequence transduction problems. It is based entirely on attention. The recurrent nature of RNNs means that parallelisation is not possible as every hidden state relies on the hidden state before it. Impressively, because the Transformer is based on FCNNs, the possibility of parallelising the model is possible.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/500/1*do7YDFF2sads0p9BnjzrWA.png)\n",
    "\n",
    "The Transformer follows a typical Encoder/Decoder architecture. It takes an input sequence of symbols $(x_1, ..., x_n)$ and maps this to a sequence of continuous representations $\\mathbf{h} = (h_1, ..., h_n)$. Given $h$, the decoder generates a symbol one element at a time $(y_1, ..., y_m)$.\n",
    "\n",
    "### Encoder\n",
    "Each encoder layer in the Transformer consists of two sub-layers. The first of these layers is the _Multi-Head Self-Attention_ module, and the second is a module called a _Position-Wise Feed-Forward Network_. Immediately following each one of these modules is a _Residual Layer Normalization_.\n",
    "\n",
    "### Decoder\n",
    "A decoder layer is similar to an encoder layer, but it has one extra module inserted: _Masked Multi-Head Attention_. We will discuss the decoder more in a further session. \n",
    "\n",
    "\n",
    "At the heart of the Transformer is this concept known as _self-attention_. Let's look at the Transformer holistically and then see exactly what this is, and why and how it solves sequence-to-sequence tasks so effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Holistic Transformer\n",
    "\n",
    "We are attempting to solve a sequence to sequence translation task: German to English using the Transformer:\n",
    "\n",
    "![whole_transformer](images/transformer.png)\n",
    "\n",
    "The Transformer is comprised of a stack of encoders and a stack of decoders. The output from the final layer of the encoder stack is sent to the decoder. The input to the first encoder is done via a special embedding module. We will look at the decoder and the embedding module in a future session.\n",
    "\n",
    "![transformer_high_level](images/transformer_high_level.png)\n",
    "\n",
    "Within the encoder stack we have a set of connected encoders. The output of one encoder is sent as input to the next encoder. An encoder consists of two sub-layers. The first sub-layer consists of a _Multi-Head Self-Attention_ module, and the second, a _Feed-Forward_ module. Within each module, a _Residual Connection_ followed by a _Layer Normalization_ is immediately applied.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/encoder_first_few.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Let's deconstruct what this new terminology means one by one. We'll start with _self attention_ and _multi-head self-attention_. Then we'll look at _residual connections_ followed by _layer normalization_. After we've covered the _feed-forward_ module, you've managed to understand most of the techniques in the Transformer! The encoder is simply the 6 aformenetioned things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "At the heart of the Transformer is self-attention. Self-attention is a mechanism that allows each input in a sequence to look at the whole sequence to compute a representation of the sequence.\n",
    "\n",
    "...what?\n",
    "\n",
    "Ok. Let's look at the following sentence: `the animal didn't cross the street because it was too tired`. What does the `it` refer to in this sentence? The street or the animal? This is trivial for us as humans to answer but not for a machine. Wouldn't it be nice if we could have some way of the computer understanding what `it` referred to?\n",
    "\n",
    "This is what self-attention attempts to do. As the model processes each word in the input sequence, self-attention allows us to look at other words in the input sequence for ideas as to what we want to encode in the representation of this word.\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization.png)\n",
    "\n",
    "It is given by the formula:\n",
    "\n",
    "$$Attention(Q,K,V) = softmax \\left( \\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "\n",
    "To make this clearer, think of how a hidden state in an RNN incorporates the representation of the previous words into the current representation. Self-attention is how the Transformer attempts to use other words (not just the previous words) to encode the meaning of a particular word\n",
    "\n",
    "\n",
    "![encoder_multihead](images/encoder_multihead.png)\n",
    "\n",
    "![vector_attn_score](images/vector_attn_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention in detail\n",
    "\n",
    "Self-attention is a representation which you can think of as a score. The intent is to have a representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. A bit complicated right? Hopefully by the end of this post, this meaning will be clear.\n",
    "\n",
    "Before talking about matrices, let's talk in terms of vectors. We'll have the sequence length be of dimension $T$, and the encoding/embeddings of the word be $D$ dimensional.\n",
    "\n",
    "Note that because I'm focusing on the FIRST Encoder here, the encodings of our sequence is the embeddings. But as you move up the encoder stack, the encoding of the sequence is the output from the previous encoder. Again, this is $D$ dimensional.\n",
    "\n",
    "![word_encodings](images/word_encodings.png)\n",
    "\n",
    "\n",
    "Ok. Now we're going to create three vectors for EACH input: A query vector ($q$), a key vector ($k$), and a value vector ($v$). These will be $d_k$ dimensional. $d_k$ is typically $D/8$. As far as real values go, usually $D=512$, while $d_k=64$. In our example, what is $d_k$?\n",
    "\n",
    "Ok... so how how do we get $q$, $k$, $v$? We learn it of course!\n",
    "\n",
    "how... do we learn it? We need a weights matrix which will transform our encodings into these vectors.\n",
    "This means that $W^Q$, $W^K$ and $W^V$ are all $\\in \\mathbb{R}^{DÃ—d_k}$\n",
    "\n",
    "![qkv_vectors](images/qkv_vectors.png)\n",
    "\n",
    "\n",
    "Ok, that's nice. We understand that $q$, $k$, $v$ are different projections of the same input now; but what do the query, key, value abstractions actually mean? They're useful terms we can use to think about attention. Let's look through the following so we can see the roles they play.\n",
    "\n",
    "Recall what we defined self-attention as earlier: A representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. For each word in our sequence, we will calculate a score by taking the dot product of the current word's query vector and the key vector for every word in the sequence.\n",
    "\n",
    "![w1_til_softmax](images/w1_til_softmax.png)\n",
    "\n",
    "Let's take stock of what we've done so far:\n",
    "- The $Ã·\\sqrt{d_k}$ is simply a practical scaling factor which leads to stabler gradients.\n",
    "- Softmax turns our scores into a probability distribution (each score is now between 0 and 1, and the sum of the scores = 1).\n",
    "\n",
    "We will now use these scores by multiplying them with their value vector. The intention here is that lower scoring words will have less weighting in the self-attention output as these words will now have a sense of \"irrelevantness\" (e.g. a low score like 0.0001 will \"cancel out\" its corresponding value vector).\n",
    "\n",
    "![w1_til_z1](images/w1_til_z1.png)\n",
    "\n",
    "Finally, the output for the current word is the summation of all the $softmax \\times v$ vectors. I.e. a weighted sum:\n",
    "\n",
    "![z_vector](images/z_vector.png)\n",
    "\n",
    "Ok. So that's self-attention in vector form. What about in terms of matrices?\n",
    "\n",
    "- Our input, $X$, is now a matrix of our sequence of words (i.e. $X \\in \\mathbb{R}^{T\\times D}$):\n",
    "![x_matrix](images/X_matrix_input.png)\n",
    "\n",
    "- $Q$, $K$, $V$ are now also matrices $\\in \\mathbb{R}^{T \\times d_k}$.\n",
    "- $W^Q$,$W^K$,$W^V$ stay $\\in \\mathbb{R}^{D \\times d_k}$.\n",
    "- We now simply obtain $Z \\in \\mathbb{R}^{T \\times d_k}$ by plugging $Q$, $K$, $V$ into our Attention formula.\n",
    "\n",
    "![Z_matrix](images/Z_matrix.png)\n",
    "\n",
    "\n",
    "- For the FIRST encoder, Q, K, V are determined by the embeddings of the input words\n",
    "- For the rest of the encoder stack, Q, K, V are determined by the output of the previous encoder\n",
    "- For the decoder stack, Q is determined in a similar fashion to the encoders. K and V, however, are passed from the final encoder to each of the decoders in the decoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) # (1, 3, 4)\n",
    "# Q_layer = nn.Linear(4, 3)\n",
    "# K_layer = nn.Linear(4, 3)\n",
    "# V_layer = nn.Linear(4, 3)\n",
    "\n",
    "# Q = Q_layer(encodings)\n",
    "# K = K_layer(encodings)\n",
    "# V = V_layer(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, dk=3):\n",
    "    Q_K_matmul = torch.matmul(Q, K.T)\n",
    "    matmul_scaled = Q_K_matmul/math.sqrt(dk)\n",
    "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attention(Q, K, V):\n",
    "    n_digits = 3\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    print ('Attention weights are:')\n",
    "    print (np.around(temp_attn, 2))\n",
    "    print ('Output is:')\n",
    "    print (np.around(temp_out, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k = torch.Tensor([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]])  # (4, 3)\n",
    "\n",
    "temp_v = torch.Tensor([[   1,0, 1],\n",
    "                      [  10,0, 2],\n",
    "                      [ 100,5, 0],\n",
    "                      [1000,6, 0]])  # (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0., 1., 0., 0.]])\n",
      "Output is:\n",
      "tensor([[10.,  0.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = torch.Tensor([[0, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000]])\n",
      "Output is:\n",
      "tensor([[550.0000,   5.5000,   0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = torch.Tensor([[0, 0, 10]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0.5000, 0.5000, 0.0000, 0.0000]])\n",
      "Output is:\n",
      "tensor([[5.5000, 0.0000, 1.5000]])\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = torch.Tensor([[10, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[0.0000, 0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000]])\n",
      "Output is:\n",
      "tensor([[550.0000,   5.5000,   0.0000],\n",
      "        [ 10.0000,   0.0000,   2.0000],\n",
      "        [  5.5000,   0.0000,   1.5000]])\n"
     ]
    }
   ],
   "source": [
    "temp_q = torch.Tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]])  # (3, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "Ok, we've just tackled self-attention, but the diagram tells us about something called multi-head attention. What is this?\n",
    "\n",
    "Conceptually, and even in terms of implementation, it's quite simple. Let's recap what we just obtained: $Z$. Each $z$ vector (i.e. for each vector $z_1, z_2, ..., z_T$ in $Z$) is a representation which bakes in all the words in the sequence including itself.\n",
    "\n",
    "A potential problem is that each $z_t$ _could_ be dominated by the representation for $t$'th word itself. This is what multihead attention solves.\n",
    "\n",
    "Instead of performing self-attention 1 time, multihead attention performs it multiple times. This means that we have multiple different attention representations. Of course to obtain these multiple different representations (i.e. $Z$s), we need to learn multiple $Q, K, V$s. Each $Q, K, V, Z$ pair is referred to as one head and is of $(D/\\text{num_heads})$ dimensionality. Typically, we use 8 heads (i.e. perform self-attention 8 different times). To \"use\" these newly obtained $Z$s, we concatenate them and pass them through a linear layer to project them back to $D$ dimensionality\n",
    "\n",
    "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], num_heads=hp[\"HEADS\"], dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_q, d_k, d_v\n",
    "        self.d = d_model//num_heads\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear_Qs = [nn.Linear(d_model, self.d).to(device) for _ in range(num_heads)]\n",
    "        self.linear_Ks = [nn.Linear(d_model, self.d).to(device) for _ in range(num_heads)]\n",
    "        self.linear_Vs = [nn.Linear(d_model, self.d).to(device) for _ in range(num_heads)]\n",
    "\n",
    "        self.mha_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # shape(Q) = [B x seq_len x D/num_heads]\n",
    "        # shape(K, V) = [B x seq_len x D/num_heads]\n",
    "\n",
    "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        scores = Q_K_matmul/m.sqrt(self.d)\n",
    "        # shape(scores) = [B x seq_len x seq_len]\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        # shape(attention_weights) = [B x seq_len x seq_len]\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # shape(output) = [B x seq_len x D/num_heads]\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, pre_q, pre_k, pre_v):\n",
    "        # shape(pre_q, pre_k, pre_v) = [B x seq_len x D]\n",
    "\n",
    "        Q = [linear_Q(pre_q) for linear_Q in self.linear_Qs]\n",
    "        K = [linear_K(pre_k) for linear_K in self.linear_Ks]\n",
    "        V = [linear_V(pre_v) for linear_V in self.linear_Vs]\n",
    "        # shape(Q, K, V) = [B x seq_len x D/num_heads] * num_heads\n",
    "\n",
    "        output_per_head = []\n",
    "        attn_weights_per_head = []\n",
    "        # shape(output_per_head) = [B x seq_len x D/num_heads] * num_heads\n",
    "        # shape(attn_weights_per_head) = [B x seq_len x seq_len] * num_heads\n",
    "        for Q_, K_, V_ in zip(Q, K, V):\n",
    "            output, attn_weight = self.scaled_dot_product_attention(\n",
    "                Q_, K_, V_)\n",
    "            # shape(output_per_head) = [B x seq_len x D/num_heads]\n",
    "            # shape(attn_weights_per_head) = [B x seq_len x seq_len]\n",
    "            output_per_head.append(output)\n",
    "            attn_weights_per_head.append(attn_weight)\n",
    "\n",
    "        output = torch.cat(output_per_head, -1)\n",
    "        attn_weights = torch.stack(attn_weights_per_head).permute(0, 3, 1, 2)\n",
    "        # shape(output) = [B x seq_len (K, V) x D]\n",
    "        # shape(attn_weights) = [B x num_heads x seq_len (K, V) x seq_len(Q)]\n",
    "\n",
    "        return self.mha_linear(self.dropout(output)), attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Encodings:\n",
      " tensor([[[0.0000, 0.1000, 0.2000, 0.3000],\n",
      "         [1.0000, 1.1000, 1.2000, 1.3000],\n",
      "         [2.0000, 2.1000, 2.2000, 2.3000]]])\n"
     ]
    }
   ],
   "source": [
    "toy_encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) \n",
    "# shape(toy_encodings) = [B, T, D] = (1, 3, 4)\n",
    "print(\"Toy Encodings:\\n\", toy_encodings)\n",
    "\n",
    "D_MODEL = toy_encodings.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy MHA: \n",
      " tensor([[[ 0.4249, -0.4863,  0.2636,  0.7792],\n",
      "         [ 0.4515, -0.4947,  0.2841,  0.8008],\n",
      "         [ 0.4782, -0.5032,  0.3046,  0.8226]]], grad_fn=<AddBackward0>)\n",
      "Toy MHA Shape: \n",
      " torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_MHA_layer = MultiHeadAttention_Encoder(d_model=D_MODEL, num_heads=2)\n",
    "toy_MHA, _ = toy_MHA_layer(toy_encodings, toy_encodings, toy_encodings)\n",
    "print(\"Toy MHA: \\n\", toy_MHA)\n",
    "print(\"Toy MHA Shape: \\n\", toy_MHA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are some of these values zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "In a typical problem where we want to employ neural networks, we normalize (standardize) our data before feeding it into the network. This is usually done by subtracting the global mean and dividing by the global standard deviation of the data:\n",
    "$$x = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "It is also possible to normalize _within_ the activations or layers of the network. There are many approaches for doing so: _Batch Normalization, Layer Normalization, Instance Normalization_ and _Group Normalization_.\n",
    "\n",
    "We won't be diving into the details of layer normalization here as it's out of scope - the important thing to know is that some kind of normalization process over features internal to the model. A helper is provided for us in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.layer_norm = nn.LayerNorm(DIMENSIONALITY)\n",
    "# layer_normed = self.layer_norm(thing_to_layer_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read further about layer normalization, check out the following resources (order _mostly_ from most accessible to least). The first few resources touch on batch normalization in order to give a conceptual understanding of what layer normalization is attempting to do:\n",
    "- https://www.youtube.com/watch?v=DtEq44FTPM4\n",
    "- https://www.youtube.com/watch?v=tNIpEZLv_eg\n",
    "- https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/ (Recommended read)\n",
    "- https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ln = self.layer_norm(x)\n",
    "        return self.dropout(ln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "In theory, if we continue stacking layers in a neural network on top of each other, we expect the training error to decrease. However, the reality doesn't follow suit.\n",
    "\n",
    "[IAMEG]\n",
    "\n",
    "Resiudal connections solve these problems by introducing a _skip connection_ (or shortcut) between every other layer. All we need to do to implement a residual connection is add our residual input to our actual input:\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "To read further about residual connections and why they work, please refer to the following resources:\n",
    "- https://www.coursera.org/lecture/convolutional-neural-networks/resnets-HAhz9\n",
    "- https://www.coursera.org/lecture/convolutional-neural-networks/why-resnets-work-XAKNO\n",
    "- https://arxiv.org/abs/1512.03385\n",
    "\n",
    "Implementing residual connections is deceivingly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Residual: \n",
      " tensor([[[-0.3198, -0.7426,  1.5980, -0.3745],\n",
      "         [-0.1725, -1.5926, -1.0669, -0.4305],\n",
      "         [ 1.2165, -1.5966, -1.2953,  0.2745]]], grad_fn=<AddBackward0>)\n",
      "Toy Residual shape: \n",
      " torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy__prev_x = torch.randn(1, 3, 4)\n",
    "# our residual connection is thus:\n",
    "toy_residual = toy__prev_x + toy_MHA\n",
    "\n",
    "print(\"Toy Residual: \\n\", toy_residual)\n",
    "print(\"Toy Residual shape: \\n\", toy_residual.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Norm: \n",
      " tensor([[[-0.4377, -0.9518,  1.8938, -0.5043],\n",
      "         [ 1.2893, -0.0000, -0.0000,  0.7720],\n",
      "         [ 1.5135, -1.2041, -0.9129,  0.6035]]], grad_fn=<MulBackward0>)\n",
      "Toy Norm shape: \n",
      " torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# and if we want to normalize it...\n",
    "toy_Norm_layer = Norm(d_model=D_MODEL)\n",
    "toy_norm = toy_Norm_layer(toy_residual)\n",
    "\n",
    "print(\"Toy Norm: \\n\", toy_norm)\n",
    "print(\"Toy Norm shape: \\n\", toy_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking stock\n",
    "\n",
    "This brings us close to the end of our first sub-layer in the encoder (and about a 70% understanding of the new concepts in the Transformer).\n",
    "\n",
    "![encoder_first_few](images/encoder_first_few.png)\n",
    "\n",
    "The next sub-layer is relatively straightforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors pass their output from the first sublayer into a feed-forward network. They call this a position-wise feed-forward network because it \"is applied to each position separately and identically\". This means that they run a feed-forward network over a rank 3 tensor (including batch size), over the sequence dimension. \n",
    "\n",
    "All this means is that the SAME weights are applied to all tokens in the sequence.\n",
    "\n",
    "The authors use a two layered network given as:\n",
    "\n",
    "$$FFN(x) = max(0,xW_1+b_1)W_2+b_2$$\n",
    "\n",
    "With the first layer having a dimension of $d_{ff}=2048$ and the second as $D$. It's unclear why the inputs are projected to a larger dimension before being projected back down to $D$ dimensions, but [one source](https://graphdeeplearning.github.io/post/transformers-are-gnns/) suggests that it is a convergence trick which enables re-scaling of the feature vectors independently with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # D_FF = 2048\n",
    "# D_FF = D_MODEL * 4\n",
    "# P_DROP = 0.3\n",
    "\n",
    "class PWFFN(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], d_ff=hp[\"D_FF\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "\n",
    "        ff = self.ff(x)\n",
    "        # shape(ff) = [B x seq_len x D]\n",
    "\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy PWFFN: \n",
      " tensor([[[-0.2890, -0.2307,  0.3671, -0.0121],\n",
      "         [ 0.1957, -0.4772,  0.3291,  0.3521],\n",
      "         [ 0.2405, -0.5672,  0.2820,  0.3192]]], grad_fn=<AddBackward0>)\n",
      "Toy PWFFN Shape: \n",
      " torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_PWFFN_layer = PWFFN(d_model=D_MODEL, d_ff=D_MODEL*4)\n",
    "toy_PWFFN = toy_PWFFN_layer(toy_norm)\n",
    "\n",
    "print(\"Toy PWFFN: \\n\", toy_PWFFN)\n",
    "print(\"Toy PWFFN Shape: \\n\", toy_PWFFN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "\n",
    "This is everything we require for one arbitrary Encoder layer! Let's code up a class which contains the aforementioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], num_heads=hp[\"HEADS\"], d_ff=hp[\"D_FF\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = PWFFN(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "\n",
    "        mha, encoder_attention_weights = self.mha(x, x, x, mask)\n",
    "        norm1 = self.norm_1(x + mha)\n",
    "        # shape(mha) = [B x seq_len x D]\n",
    "        # shape(encoder_attention_weights) = [B x num_heads x seq_len x seq_len]\n",
    "        # shape(norm1) = [B x seq_len x D]\n",
    "\n",
    "        ff = self.ff(norm1)\n",
    "        norm2 = self.norm_2(norm1 + ff)\n",
    "        # shape(ff) = [B x seq_len x D]\n",
    "        # shape(norm2) = [B x seq_len x D]\n",
    "\n",
    "        return norm2, encoder_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Lets talk about embedding the inputs\n",
    "\n",
    "[IMAGE]\n",
    "\n",
    "One thing we've not yet looked at is _Positional Encoding_. Recall that our model contains no recurrence and thus we need a way to make sense of the order of the sequence. To do so, we need to inject some information about the position of the tokens in the sequence. Positional Encoding is one strategy to do so.\n",
    "\n",
    "So, we want our model to add some information to each of our words (embeddings) which indicates its position in the sequence. What are some strategies for doing so?\n",
    "\n",
    "Well, we could just add the token position to the embedding (e.g. 1 for the first word, 2 for the second word, 3 for the third word). What are some issues with this approach?\n",
    "\n",
    "What about linearly assigning values between 0 and 1 to the token embedding? (e.g. for an eight-length sequence, the first word has 0.125 added to it, the second 0.25, the third 0.375 etc).\n",
    "\n",
    "The trick the authors propose is to add a $D$-dimensional vector to the embedding (which is also $D$-dimensional) instead of a single number. The vector which is added to the word embedding is __fixed__ - that is, it does NOT depend on the features of the word itself - only the position that it appears in.\n",
    "\n",
    "Let's dissect the formula it's given by:\n",
    "\n",
    "$$PE_{(pos,2i)}=sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n",
    "$$PE_{(pos,2i+1)}=cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$$\n",
    "\n",
    "[IMG]\n",
    "\n",
    "![postional_encoding_graph](https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_idx, d_model=hp[\"D_MODEL\"]):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape(x) = [B x seq_len]\n",
    "\n",
    "        embedding = self.embed(x)\n",
    "        # shape(embedding) = [B x seq_len x D]\n",
    "\n",
    "        return embedding * m.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Embeddings: \n",
      " tensor([[[-1.7563, -2.9034,  2.0407,  3.6623],\n",
      "         [-2.0049,  2.4615, -6.2541, -1.5537],\n",
      "         [ 0.4437,  0.0647, -0.0440,  1.3916],\n",
      "         [ 4.1713,  0.6166, -0.0546, -2.8450],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<MulBackward0>)\n",
      "Toy Embeddings Shape: \n",
      " torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_vocab = torch.LongTensor([[1, 2, 3, 4, 0, 0]])\n",
    "\n",
    "toy_embedding_layer = Embeddings(5, pad_idx=0, d_model=D_MODEL)\n",
    "toy_embeddings = toy_embedding_layer(toy_vocab)\n",
    "\n",
    "print(\"Toy Embeddings: \\n\", toy_embeddings)\n",
    "print(\"Toy Embeddings Shape: \\n\", toy_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], dropout=hp[\"P_DROP\"], max_seq_len=200):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model).to(device)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "\n",
    "        two_i = torch.arange(0, d_model, step=2).float()\n",
    "        div_term = torch.pow(10000, (two_i/d_model)).float()\n",
    "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # assigns the first argument to a class variable\n",
    "        # i.e. self.pe\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "        pe = self.pe[:, :x.shape[1]].detach()\n",
    "        x = x + pe\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy PE: \n",
      " tensor([[[-1.9515, -2.1149,  2.2674,  5.1803],\n",
      "         [-1.2927,  0.0000, -6.9379, -0.6153],\n",
      "         [ 1.5033, -0.3905, -0.0266,  2.6571],\n",
      "         [ 4.7916, -0.4148, -0.0273, -2.0505],\n",
      "         [-0.8409, -0.7263,  0.0444,  0.0000],\n",
      "         [-1.0655,  0.3152,  0.0555,  1.1097]]], grad_fn=<MulBackward0>)\n",
      "Toy PE Shape: \n",
      " torch.Size([1, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "toy_PE_layer = PositionalEncoding(d_model=D_MODEL)\n",
    "toy_PEs = toy_PE_layer(toy_embeddings)\n",
    "\n",
    "print(\"Toy PE: \\n\", toy_PEs)\n",
    "print(\"Toy PE Shape: \\n\", toy_PEs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder (and the Decoder)\n",
    "![](https://i.giphy.com/media/11GWLm7bE2fibC/giphy.webp)\n",
    "\n",
    "Cool! This brings us to the end of the Encoder part of the Transformer.\n",
    "\n",
    "We have enough to code up our Encoder class. In this implementation, the Encoder is responsible for the first layers embedding and encoding, and then simply runs a for loop over the number of encoders we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, Embedding: Embeddings, d_model=hp[\"D_MODEL\"], \n",
    "                 num_heads=hp[\"HEADS\"], num_layers=hp[\"LAYERS\"], \n",
    "                 d_ff=hp[\"D_FF\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Embedding = Embedding\n",
    "\n",
    "        self.PE = PositionalEncoding(\n",
    "            d_model)\n",
    "\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            d_ff,\n",
    "            dropout\n",
    "        ) for layer in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # shape(x) = [B x SRC_seq_len]\n",
    "\n",
    "        embeddings = self.Embedding(x)\n",
    "        encoding = self.PE(embeddings)\n",
    "        # shape(embeddings) = [B x SRC_seq_len x D]\n",
    "        # shape(encoding) = [B x SRC_seq_len x D]\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            encoding, encoder_attention_weights = encoder(encoding, mask)\n",
    "            # shape(encoding) = [B x SRC_seq_len x D]\n",
    "            # shape(encoder_attention_weights) = [B x SRC_seq_len x SRC_seq_len]\n",
    "\n",
    "        return encoding, encoder_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_encoder = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "\n",
    "Looking at the Transformer architecture again, we see the Decoder is quite similar to the Encoder. It has two subtle differences though. The __masked__ multi-head attention, and the multi-head attention module which receives inputs from the Encoder. Let's work through these respectively.\n",
    "\n",
    "[IMG["
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Multihead Attention\n",
    "\n",
    "Think back to our Seq2Seq model from last week. During decoding, we have access to the whole source sentence, and the decoded tokens we've decoded SO FAR. Obviously we can't have access to future tokens because we don't know what they are yet...\n",
    "\n",
    "During training time however, we DO have access to future tokens because we have labelled pairs. To speed up training, the Transformer architecture enables us to feed in our whole target sequence to the model and use masking to tell the attention mechanism not to look at illegal (i.e. future) positions when considering the i'th token.\n",
    "\n",
    "Recall what Multihead Attention (MHA) did. It worked out a representation for each token in the sequence given all the tokens. In other words, it was bidirectional. Masking is the strategy we use to tell the network not to look at the positions past the most recent word that has been decoded.\n",
    "\n",
    "We will look at the Decoder Layer in its entirety in a bit, but for now let's focus on the masking process for MHA. We will implement this in our `MultiHeadAttention` module. Our mask is going to be the same shape as `Q_K_matmul` because this is what we are calculating attention over.\n",
    "\n",
    "The mask is a tensor with with a value of `-inf` at illegal locations. For one sample in our input, the shape of `Q_K_matumul` is `[T, T]`. Here, `T` refers to the sequence length of the target output. We will talk about test time later, so let's consider the training case right now. During training, we have access to the whole target sequence. So at every timestep (i.e. for every word) in the sequence, an illegal location would be all the future timesteps that we're not meant to have access to. Our matrix is `[T, T]`. So at the 1st timestep, we don't have access to any words from the 2nd timestep onwards. At the 2nd timestep, we don't have acess to any words from the 3rd timestep onwards.\n",
    "\n",
    "[IMAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(size):\n",
    "    # since this mask is the same for a batch being fed into the model,\n",
    "    # we will the mask Tensor with the batch size = 1.\n",
    "    # Broadcasting will allow us to replicate this mask across all the other elements in the batch\n",
    "    mask = torch.ones((1, size, size)).triu(1)\n",
    "    mask = mask == 0\n",
    "    return(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOY MASK: \n",
      " tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True, False, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
      "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "toy_mask = create_mask(10)\n",
    "print(\"TOY MASK: \\n\", toy_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOY SCORES:  tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
      "         [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
      "         [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
      "         [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
      "         [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
      "         [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
      "         [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]])\n"
     ]
    }
   ],
   "source": [
    "toy_scores = torch.arange(100).reshape(1, 10, 10)\n",
    "print(\"TOY SCORES: \\n\", toy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from our Multihead Attention\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    # shape(Q) = [B x seq_len x D/num_heads]\n",
    "    # shape(K, V) = [B x seq_len x D/num_heads]\n",
    "\n",
    "    Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "    scores = Q_K_matmul/m.sqrt(self.d)\n",
    "    # shape(scores) = [B x seq_len x seq_len]\n",
    "    \n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    # shape(attention_weights) = [B x seq_len x seq_len]\n",
    "\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    # shape(output) = [B x seq_len x D/num_heads]\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "         [10, 11, -1, -1, -1, -1, -1, -1, -1, -1],\n",
       "         [20, 21, 22, -1, -1, -1, -1, -1, -1, -1],\n",
       "         [30, 31, 32, 33, -1, -1, -1, -1, -1, -1],\n",
       "         [40, 41, 42, 43, 44, -1, -1, -1, -1, -1],\n",
       "         [50, 51, 52, 53, 54, 55, -1, -1, -1, -1],\n",
       "         [60, 61, 62, 63, 64, 65, 66, -1, -1, -1],\n",
       "         [70, 71, 72, 73, 74, 75, 76, 77, -1, -1],\n",
       "         [80, 81, 82, 83, 84, 85, 86, 87, 88, -1],\n",
       "         [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_scores = toy_scores.masked_fill(toy_mask == 0, -1)\n",
    "toy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Attention (again ðŸ™„)\n",
    "\n",
    "Following the masked MHA, as was done in the encoder, we apply a skip-connection and a layer norm. The next stage in the decoding process runs attention between the source representing and target representation. This is done by taking the __key__ and __value__ tensors from the Encoder. The __query__ tensor comes from the previous step in the decoding process. In the image below, the orange arrow connecting the encoder and decoder represents the key and value from the final layer encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], num_heads=hp[\"HEADS\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_q, d_k, d_v\n",
    "        self.d = d_model//num_heads\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear_Qs = [nn.Linear(d_model, self.d).to(device)\n",
    "                          for _ in range(num_heads)]\n",
    "        self.linear_Ks = [nn.Linear(d_model, self.d).to(device)\n",
    "                          for _ in range(num_heads)]\n",
    "        self.linear_Vs = [nn.Linear(d_model, self.d).to(device)\n",
    "                          for _ in range(num_heads)]\n",
    "\n",
    "        self.mha_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # shape(Q) = [B x seq_len (Q) x D/num_heads]\n",
    "        # shape(K, V) = [B x seq_len (K, V) x D/num_heads]\n",
    "\n",
    "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        scores = Q_K_matmul/m.sqrt(self.d)\n",
    "        # shape(scores) = [B x ??_seq_len x SRC_seq_len]\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        # shape(attention_weights) = [B x seq_len (K, V) x seq_len (Q)]\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # shape(output) = [B x seq_len (K, V) x D/num_heads]\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, pre_q, pre_k, pre_v, mask=None):\n",
    "        # shape(pre_q (ENCODING)) = [B x SRC_seq_len x D]\n",
    "        # shape(pre_q (DECODING)) = [B x TRG_seq_len x D]\n",
    "        #\n",
    "        # shape(pre_k, pre_v (MASKED ATTENTION)) = [B x TRG_seq_len x D]\n",
    "        # shape(pre_k, pre_v (OTHERWISE)) = [B x SRC_seq_len x D]\n",
    "\n",
    "        Q = [linear_Q(pre_q) for linear_Q in self.linear_Qs]\n",
    "        K = [linear_K(pre_k) for linear_K in self.linear_Ks]\n",
    "        V = [linear_V(pre_v) for linear_V in self.linear_Vs]\n",
    "        # shape(Q) = [B x seq_len (Q) x D/num_heads] * num_heads\n",
    "        # shape(K) = [B x seq_len (K, V) x D/num_heads] * num_heads\n",
    "        # shape(V) = [B x seq_len (K, V) x D/num_heads] * num_heads\n",
    "\n",
    "        output_per_head = []\n",
    "        attn_weights_per_head = []\n",
    "        # shape(output_per_head) = [B x seq_len (K, V) x D/num_heads] * num_heads\n",
    "        # shape(attn_weights_per_head) = [B x seq_len (K, V) x seq_len (Q)] * num_heads\n",
    "        for Q_, K_, V_ in zip(Q, K, V):\n",
    "            output, attn_weight = self.scaled_dot_product_attention(\n",
    "                Q_, K_, V_, mask)\n",
    "            output_per_head.append(output)\n",
    "            attn_weights_per_head.append(attn_weight)\n",
    "\n",
    "        output = torch.cat(output_per_head, -1)\n",
    "        attn_weights = torch.stack(attn_weights_per_head).permute(0, 3, 1, 2)\n",
    "        # shape(output) = [B x seq_len (K, V) x D]\n",
    "        # shape(attn_weights) = [B x num_heads x seq_len (K, V) x seq_len(Q)]\n",
    "\n",
    "        return self.mha_linear(self.dropout(output)), attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "\n",
    "Following another residual layer normalization, we have a PWFFN as we did in the encoder. One arbitray decoder layer is thus the two aforementioned attention modules and the PWFFN with residual layer norms in between each step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=hp[\"D_MODEL\"], num_heads=hp[\"HEADS\"], d_ff=hp[\"D_FF\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "\n",
    "        self.mha_1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha_2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = PWFFN(d_model, d_ff)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, trg_mask, src_mask):\n",
    "        # shape(x) = [B x TRG_seq_len x D]\n",
    "        # shape(encoder_outputs) = [B x SRC_seq_len x D]\n",
    "\n",
    "        masked_mha, masked_mha_attn_weights = self.mha_1(\n",
    "            x, x, x, mask=trg_mask)\n",
    "        # shape(masked_mha) = [B x TRG_seq_len x D]\n",
    "        # shape(masked_mha_attn_weights) = [B x num_heads x TRG_seq_len x TRG_seq_len]\n",
    "\n",
    "        norm1 = self.norm_1(x + masked_mha)\n",
    "        # shape(norm1) = [B x TRG_seq_len x D]\n",
    "\n",
    "        enc_dec_mha, enc_dec_mha_attn_weights = self.mha_2(\n",
    "            norm1, encoder_outputs, encoder_outputs, mask=src_mask)\n",
    "        # shape(enc_dec_mha) = [B x TRG_seq_len x D]\n",
    "        # shape(enc_dec_mha_attn_weights) = [B x num_heads x TRG_seq_len x SRC_seq_len]\n",
    "\n",
    "        norm2 = self.norm_2(norm1 + enc_dec_mha)\n",
    "        # shape(norm2) = [B x TRG_seq_len x D]\n",
    "\n",
    "        ff = self.ff(norm2)\n",
    "        norm3 = self.norm_3(norm2 + ff)\n",
    "        # shape(ff) = [B x TRG_seq_len x D]\n",
    "        # shape(norm3) = [B x TRG_seq_len x D]\n",
    "\n",
    "        return norm3, masked_mha_attn_weights, enc_dec_mha_attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Our Decoder class acts similarly to our Encoder class. It is responsible for embedding and encoding the input for the first layer, and then is simply a for loop over the layers we desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, Embedding: Embeddings, d_model=hp[\"D_MODEL\"], \n",
    "                 num_heads=hp[\"HEADS\"], num_layers=hp[\"LAYERS\"], \n",
    "                 d_ff=hp[\"D_FF\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Embedding = Embedding\n",
    "\n",
    "        self.PE = PositionalEncoding(\n",
    "            d_model)\n",
    "\n",
    "        self.decoders = nn.ModuleList([DecoderLayer(\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            d_ff,\n",
    "            dropout\n",
    "        ) for layer in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, trg_mask, src_mask):\n",
    "        # shape(x) = [B x TRG_seq_len]\n",
    "\n",
    "        embeddings = self.Embedding(x)\n",
    "        encoding = self.PE(embeddings)\n",
    "        # shape(embeddings) = [B x TRG_seq_len x D]\n",
    "        # shape(encoding) = [B x TRG_seq_len x D]\n",
    "        \n",
    "        for decoder in self.decoders:\n",
    "            encoding, masked_mha_attn_weights, enc_dec_mha_attn_weights = decoder(\n",
    "                encoding, encoder_output, trg_mask, src_mask)\n",
    "            # shape(encoding) = [B x TRG_seq_len x D]\n",
    "            # shape(masked_mha_attn_weights) = [B x num_heads x TRG_seq_len x TRG_seq_len]\n",
    "            # shape(enc_dec_mha_attn_weights) = [B x num_heads x TRG_seq_len x SRC_seq_len]\n",
    "\n",
    "        return encoding, masked_mha_attn_weights, enc_dec_mha_attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! The Transformer class! Our Transformer class is simple and will receive data from the train loop. At every pass it will do the following:\n",
    "- Create a source and target mask\n",
    "- Run the Encoder\n",
    "- Run the Decoder\n",
    "- Output logits for token prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_len, trg_vocab_len, d_model=hp[\"D_MODEL\"], d_ff=hp[\"D_FF\"], \n",
    "                 num_layers=hp[\"LAYERS\"], num_heads=hp[\"HEADS\"], dropout=hp[\"P_DROP\"]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        encoder_Embedding = Embeddings(\n",
    "            src_vocab_len, SRC.vocab.stoi[\"<pad>\"], d_model)\n",
    "        decoder_Embedding = Embeddings(\n",
    "            trg_vocab_len, TRG.vocab.stoi[\"<pad>\"], d_model)\n",
    "\n",
    "        self.encoder = Encoder(encoder_Embedding, d_model,\n",
    "                               num_heads, num_layers, d_ff, dropout)\n",
    "        self.decoder = Decoder(decoder_Embedding, d_model,\n",
    "                               num_heads, num_layers, d_ff, dropout)\n",
    "\n",
    "        self.linear_layer = nn.Linear(d_model, trg_vocab_len)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def create_src_mask(self, src):\n",
    "        src_mask = (src != SRC.vocab.stoi[\"<pad>\"]).unsqueeze(-2)\n",
    "        return src_mask\n",
    "\n",
    "    def create_trg_mask(self, trg):\n",
    "        trg_mask = (trg != TRG.vocab.stoi[\"<pad>\"]).unsqueeze(-2)\n",
    "        mask = torch.ones((1, trg.shape[1], trg.shape[1])).triu(1).to(device)\n",
    "        mask = mask == 0\n",
    "        trg_mask = trg_mask & mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # shape(src) = [B x SRC_seq_len]\n",
    "        # shape(trg) = [B x TRG_seq_len]\n",
    "\n",
    "        src_mask = self.create_src_mask(src)\n",
    "        trg_mask = self.create_trg_mask(trg)\n",
    "        # shape(src_mask) = [B x 1 x SRC_seq_len]\n",
    "        # shape(trg_mask) = [B x 1 x TRG_seq_len]\n",
    "\n",
    "        encoder_outputs, encoder_mha_attn_weights = self.encoder(src, src_mask)\n",
    "        # shape(encoder_outputs) = [B x SRC_seq_len x D]\n",
    "        # shape(encoder_mha_attn_weights) = [B x num_heads x SRC_seq_len x SRC_seq_len]\n",
    "        decoder_outputs, _, enc_dec_mha_attn_weights = self.decoder(\n",
    "            trg, encoder_outputs, trg_mask, src_mask)\n",
    "        # shape(decoder_outputs) = [B x SRC_seq_len x D]\n",
    "        # shape(enc_dec_mha_attn_weights) = [B x num_heads x TRG_seq_len x SRC_seq_len]\n",
    "        logits = self.linear_layer(decoder_outputs)\n",
    "        # shape(logits) = [B x TRG_seq_len x TRG_vocab_size]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The authors of the paper used Adam with a decaying learning rate which follows the following algorithm:\n",
    "\n",
    "$$\\text{lr} = \\sqrt{\\frac{1}{\\text{d}_\\text{model}}} \\times min\\left(\\sqrt{\\frac{1}{i}}, i \\times \\text{warmup}^{-1.5}\\right)$$\n",
    "\n",
    "Where $i$ is the global step we're currently on. $\\text{warmup}$ is a hyperparamter which the authors set to 4000, and $\\text{d}_\\text{model}$ is the dimensionality of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-c472233cdc82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m optimizer = torch.optim.Adam(\n\u001b[1;32m----> 2\u001b[1;33m     model.parameters(), lr=hp.LR, betas=(0.9, 0.98), eps=1e-9)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"<pad>\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=hp.LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lr_optimizer(optimizer, step, d_model=hp[\"D_MODEL\"], warmup_steps=4000):\n",
    "    min_arg1 = m.sqrt(1/(step+1))\n",
    "    min_arg2 = step * (warmup_steps**-1.5)\n",
    "    lr = m.sqrt(1/d_model) * min(min_arg1, min_arg2)\n",
    "\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add train loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding\n",
    "\n",
    "So far, we've looked at tokenizing inputs primarily on whitespace and some punctuation rules. __Byte-Pair Encoding__ (BPE) is a conceptually simple and elegant unsupervised technique which allows us to be composable with our tokens while reducing the vocabulary size. It enables us to identify common 'patterns' in natural language and split on that pattern. What we mean by pattern here is common sequences of characters. For example, consider the sentence: `\"They learned byte pair encoding successfully\"`. There are some words in this sentence which contain common sequences of characters:\n",
    "- learned -> learn + _ed_\n",
    "- encoding -> encod + _ing_\n",
    "- successfully -> success + _ful_ + _ly_\n",
    "\n",
    "Now, let's say we had a list of all the common patterns in language (e.g. `[ed, ing, ful, ly]`). Composing our BPE corpus is relatively trivial. For every word in our corpus, we would check if the common pattern was present in that word, and if it is, our tokens for that one word become the root of the word and the patterns in that word. A special symbol (typically `@@`) is used to indicate the start or end of of a sub-word. So given that example sentence above, tokenizing it via BPE gives:\n",
    "- `[they, learn@@, @@ed, byte, pair, encod@@, @@ing, success@@, @@ful@@, @@ly]`\n",
    "- `[they, learned, byte, pair, encoding, successfully]` (tokenzing on whitespace).\n",
    "\n",
    "As we are not using BPE in our models, the implementation details are out of scope for this workshop. However, here are some brilliant resources to demonstrate how to implement BPE given a corpus:\n",
    "- [https://leimao.github.io/blog/Byte-Pair-Encoding/](https://leimao.github.io/blog/Byte-Pair-Encoding/)\n",
    "- [https://nlp.h-its.org/bpemb/](https://nlp.h-its.org/bpemb/)\n",
    "- [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
