{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for Language Modelling  \n",
    "\n",
    "\n",
    "## RNNs Recap\n",
    "\n",
    "RNNs are designed to make use of sequential data, when the current step has some kind of relation with the previous steps. This makes them ideal for applications with a time component (audio, time-series data) and natural language. RNNs are networks for which value of a unit depends on its own previous output as input.\n",
    "\n",
    "An input vector representing the current input element $x_t$ is multiplied by a weight matrix $W$ and then passed through an activation function to compute an activation value for a layer of hidden units. This hidden layer is, in turn, used to calculate a corresponding output, $y_t$. \n",
    "\n",
    "$h_t = g(Uh_{t-1}+Wx_t)$\n",
    "\n",
    "$y_t = a(Vh_t)$\n",
    "\n",
    "The hidden layer from the previous time step $h_{t-1}$ provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. $U$ determine how the network should make use of past context. RNNs do not impose any limit on this prior context. The context includes information dating back to the beginning of the sequence. Three sets of weights are updated at each timestep: $W$, $U$ and $V$.\n",
    "\n",
    "REDO THIS IMAGe\n",
    "![rnn_cell](images/rnn_cell.png)\n",
    "\n",
    "\n",
    "## RNNs for Text Classification Task\n",
    "\n",
    "There are many variations of RNNâ€™s likely Many-One, Many-Many, etc. In our case we aim to classify the input text into positive and negative class. So, set of Many-One LSTM units achieves the task as only one value needs to be outputted for determining the polarity of the review. Usually this is the last RNN hidden state as the one that summarises the whole sequence.\n",
    "\n",
    "\n",
    "Q: Why RNNs better than FFNNs?\n",
    "\n",
    "FFNNs do not take context into account. Each word is represented by its embedding independent of other words. RNNs encodes each new word (token) considering the previous words. Meaning of words can change depending on the context. For example, compare the meanings of the word \"mean\" in those two sentences \"I compute the mean\" and \"His behaviour was mean\".\n",
    "\n",
    "Q: How the model is evaluated ?\n",
    "\n",
    "The natural choise is accuracy - \n",
    "percentage of all the examples that our model labeled correctly. However, it is not good for unbalanced datasets. Imagine a dataset with 999,900 positive examples and 100 negative examples. A very bad classifier can assign positive class to all the examples. This classifier would have 999,900 true negatives and only 100 false negatives and an accuracy of\n",
    "999,900/1,000,000 or 99.99%! \n",
    "\n",
    "Other metrics, more useful for such datasets are: precision, recall and F-measure. Precision measures the percentage of the items that the system labelled as positive and are positive accroding to the gold labels. Recall measures the percentage of items labelled as positive out of all the gold positive items. F-score is the weighted harmonic mean of the precision and recall. \n",
    "\n",
    "\n",
    "We will work with a popular classification task of sentiment analysis, the extraction of the sentiment that a writer expresses toward something he/she describes.\n",
    "\n",
    "![rnn_classification](images/rnn_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "#We will work with a dataset from the torchtext package consists of data processing utilities and popular datasets for NLP\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "# We fix the seeds to get consistent results for each training.\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With TorchText Field we define how our data will be processed: here we will use Spacy for tokenisation\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.long)\n",
    "\n",
    "# We will experiment with a widely used Stanford Treebank dataset and will predict sentiment of movie reviews\n",
    "# Our data will be classified in three labels: positive, negative and neutral\n",
    "# We take the standard split\n",
    "\n",
    "train_data, valid_data, test_data = datasets.SST.splits(\n",
    "            TEXT, LABEL)\n",
    "\n",
    "# Print stat over the data\n",
    "\n",
    "print('train.fields', train_data.fields)\n",
    "print('len(train)', len(train_data))\n",
    "print('vars(train[0])', vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a vocabulary out of tokens available from the pre-trained embedding list and the vocabulary of labels.\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print('Text Vocabulary Length', len(TEXT.vocab))\n",
    "print (\"Label Vocabulary Length: \", len(LABEL.vocab))\n",
    "\n",
    "#We can display the most common words in the vocabulary and their frequencies\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "#We can also see the vocabulary directly using the stoi (string to int)\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# place the tensors on the GPU if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BucketIterator is an iterator that will return a batch of examples of similar lengths, minimizing the amount of padding per example.\n",
    "# Padding refers to fixing the length of inputs (adding a reserved token a certain amount of times to match certain length), usually to the max length within a batch. For exmaple:\n",
    "# i         like  this  movie <pad>\n",
    "# the       movie is    very  good\n",
    "# excellent !     <pad> <pad> <pad>\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "            (train_data, valid_data, test_data), \n",
    "                batch_size = BATCH_SIZE,\n",
    "                sort_within_batch = True,\n",
    "                device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    #  within the __init__ we define the layers: an embedding layer, an RNN, and a linear layer. All the parameters initialized to random values by default, unless explicitly specified.\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # an embedding layer (look-up layer) transforms word indices into word embeddings. It could be initialized with pre-trained embeddings (100D pre-trained GloVe embeddings in ur case) that would be fine-tuned together with other layers.\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "\n",
    "        # biderictional RNN layer. we specify that the batch dimension goes first\n",
    "        # we apply droput technique that sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # the linear layer takes the final hidden state and feeds it through a fully connected layer, where the dimensionality of the output is equal to the output class count. Note that the input hidden dimension to this layer is doubled because of the bidirectionality of the RNN layer\n",
    "        \n",
    "        #for classification in a bidirectional RNN we concatenate the last hidden state from the forward RNN (obtained from final word of the sentence) and the last hidden state from the backward RNN\n",
    "        # for a multi-layed RNN the hidden state output by the first (bottom) RNN at time-step t will be the input to the RNN above it at time step t.\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        #apply the dropout\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        # we permute the first two dimentions so that the batch goes first\n",
    "        text = text.permute(1, 0)\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        # even if we initially batched sentences with similar length and padded them, so  we might end up doing more computations than required. Packing sequences by their decreasing length allows RNNs to compute the hidden states by ignoring padded positions and reduce computational loss \n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence to restore the original order of sentences in the batch\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * 2]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * 2, batch size, hid dim]\n",
    "        \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(hidden)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * 2]\n",
    "            \n",
    "        return self.fc(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.5\n",
    "# get our pad token index from the vocabulary\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    returns accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    class_preds = nn.Softmax(dim=-1)(preds)\n",
    "    class_preds= class_preds.max(-1)[1]\n",
    "    correct = (class_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# we use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
    "    # good practice to include even if do not use here\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # iterate over batches\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # we zero the gradients as they are not removed automatically\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        # compute training accuracy\n",
    "        acc = accuracy(predictions, batch.label)\n",
    "              \n",
    "        # calculate the gradient of each parameter\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the parameters using the gradients and optimizer algorithm \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # normalize everything\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, N_EPOCHS=10):\n",
    "\n",
    "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
    "    # good practice to include even if do not use here\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problems where all timesteps of the input sequence are available, bidirectional RNNs train two instead of one RNNs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. Outputs at the same step are then usually concatenated. This can provide additional useful context to the model.\n",
    "\n",
    "Q: Why is a bi-directional RNN is better than single-direction ?\n",
    "\n",
    "Imagine that you see only the left context: \"We went to ...\" This context is very general and a lot of different words can continue: nouns (London, work, cinema, doctor), verbs (join, support), etc. When we both left and right contexts the word \"sleep\" is becoming evident: \"We went to ... early but still could not wake up on time.\"\n",
    "\n",
    "\n",
    "![bi_rnn_classification](images/bi_rnn_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDIRECTIONAL = True\n",
    "\n",
    "bidirectional_model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(bidirectional_model, train_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basics of Language Modelling\n",
    "\n",
    "Language model is required to represent the text to a form understandable from the machine point of view. Language Modelling (LM) is at the core of Natural Language Processing (NLP). Base of all the NLP tasks: Machine Translation, Spell Correction, Speech Recognition, Summarization, Question Answering, Sentiment analysis etc.  \n",
    "\n",
    "Q: What is the difference between word embeddings and language modelling ?\n",
    "\n",
    "The main difference that word embeddings do not take into account word order. Language models take word order into account. The word order is important. If you do not take the word order into account the representation of the following sentences will be the same: \"It was really not good, on the opposite quite bad.\" and \"It was really not bad, on the opposite quite good.\" However, the meaning of those two sentences is different.\n",
    "\n",
    "You may have heard about BERT. BERT is a general-purpose pre-trained language model. It is pre-trained using a lot of language data from Internet to create a better \"grasp\" of language. It is bidirectional. This means a deeper sense of language context and flow compared to the single-direction language models. You can download it and fine-tune for your NLP problem. \n",
    "\n",
    "\n",
    "## Language Modeling with RNNs\n",
    "\n",
    "Language is a sequence of letters, words, sentences, paragraphs, etc. These units are not independent. When we comprehend and produce spoken language, we are processing continuous input streams of indefinite\n",
    "length. And even when dealing with written text we normally read it sequentially. Thus, RNNs is a perfect fit to model language data because with RNNs we can represent language sequence of any length into a fixed-sized vector.\n",
    " \n",
    "![rnn_lm](images/rnn_lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Language Models\n",
    "\n",
    "Language is very difficult to evaluate since there is no single gold truth: one meaning could be expressed in many valid ways.\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Does it prefer real (=frequently observed) sentences to â€˜ungrammatical/gibberishâ€™ (or rarely observed) ones? \n",
    "Remember that entropy is the average number of bits to encode the information contained in a random variable, so the exponentiation of the entropy (perplexity, $e^{H}$) should be the total amount of all possible information, or more precisely, the weighted average number of choices a random variable has. We evaluate our prediction Q by testing against samples drawn from P: $PPL = e^{H(P,Q)}$.\n",
    "\n",
    "Measure perplexity on an unseen (test) corpus, generally we compare a range of models using this score. The best LM is the one that generates the lowest perplexity on the test corpus.\n",
    "\n",
    "### Human Evaluation\n",
    "\n",
    "Human evaluation is costly, slow and subjective but reliable. Human evaluation of a language model may involve how a hypothesis satisfies the grammatical and lexical norms of a language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with TorchText Field we define how our data will be processed\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "\n",
    "# you will be using the WikiText-2 corpus, which is a popular LM dataset.\n",
    "#The WikiText language modeling dataset is a collection of texts extracted \n",
    "#from Good and Featured articles on Wikipedia.\n",
    "\n",
    "train_data, valid_data, test_data = datasets.WikiText2.splits(TEXT)\n",
    "\n",
    "# print stat over the data\n",
    "\n",
    "print('train.fields', train_data.fields)\n",
    "print('len(train)', len(train_data))\n",
    "\n",
    "# now we build a vocabulary out of tokens available fron the pre-trained embedding list and the vocabulary of labels\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    " \n",
    "print('Text Vocabulary Length', len(TEXT.vocab))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# place the tensors on the GPU if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BPTTIterator (Backpropagation Through Time Iterator) divides the corpus into batches of sequence length bptt\n",
    "# The application of the Backpropagation training algorithm to RNNs applied to sequence data. Each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BPTTIterator.splits(\n",
    "            (train_data, valid_data, test_data), \n",
    "                batch_size = BATCH_SIZE, bptt_len=30,\n",
    "                device = device, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_iterator)); vars(b).keys()\n",
    "print(b)\n",
    "#note that the first dimension is the sequence, and the next is the batch, the target is the original text offset by 1\n",
    "print(b.text[:5, :3])\n",
    "print(b.target[:5, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    #  within the __init__ we define the layers: an embedding layer, an RNN, and a linear layer. All the parameters initialized to random values by default, unless explicitly specified.\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # an embedding layer (look-up layer) transforms word indices into word embeddings. It could be initialized with pre-trained embeddings (100D pre-trained GloVe embeddings in ur case) that would be fine-tuned together with other layers.\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "\n",
    "        # unidirectional RNN layer: for LM modelling we do not see the right context\n",
    "        # note the dimensionality changes\n",
    "        # we apply droput technique that sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # now the dimensionality of the output layer is equal to our vocabulary size\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        #apply the dropout\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "       \n",
    "    def forward(self, text, prev_hidden):\n",
    "         \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        # we permute the first two dimentions so that the batch goes first\n",
    "        text = text.permute(1, 0)\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        all_hidden, last_hidden = self.rnn(embedded, prev_hidden)\n",
    "        \n",
    "        #all_hidden = [sent len, batch size, hid dim]\n",
    "        \n",
    "        #last_hidden = [num layers, batch size, hid dim]\n",
    "        \n",
    "        #take all hidden states to produce an output word per time step\n",
    "        \n",
    "        logits = self.dropout(all_hidden)\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "            \n",
    "        return self.fc(logits), last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(TEXT.vocab)\n",
    "DROPOUT = 0.5\n",
    "# get our pad token index from the vocabulary\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# we use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden(hidden):\n",
    "  \"\"\"Wraps hidden states in new Tensors, to declare it not to need gradients. So that the initial hidden state for this batch is constant and doesnâ€™t depend on anything.\"\"\"\n",
    "\n",
    "  if isinstance(hidden, torch.Tensor):\n",
    "    return hidden.detach()\n",
    "  else:\n",
    "    return tuple(save_hidden(v) for v in hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, N_EPOCHS=10):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_items = 0\n",
    "  \n",
    "    model.train()\n",
    "    \n",
    "    # iterate over batches\n",
    "    prev_hidden = torch.zeros(N_LAYERS, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    for batch in iterator:\n",
    "        \n",
    "        # we zero the gradients as they are not removed automatically\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get input text and right-shifted targets we will predict\n",
    "        text, target = batch.text, batch.target\n",
    "        \n",
    "        # permute the dimensions to get the batch size first\n",
    "        target = target.permute(1, 0)\n",
    "        \n",
    "        # starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # otherwise the model would backpropagate all the way to beginning of the dataset.\n",
    "\n",
    "        prev_hidden = save_hidden(prev_hidden)\n",
    "        logits, prev_hidden = model(text, prev_hidden)\n",
    "       \n",
    "        # compute the loss, we reshape inputs to eliminate batching\n",
    "        loss = criterion(logits.view(-1, OUTPUT_DIM), target.reshape(-1))\n",
    "        \n",
    "        # backprop the average loss and update parameters\n",
    "        loss.mean().backward()\n",
    "        \n",
    "        # update the parameters using the gradients and optimizer algorithm \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.detach().sum()\n",
    "        epoch_items += loss.numel()\n",
    "        \n",
    "        # we compute loss per token for an epoch\n",
    "        loss_per_token = epoch_loss / epoch_items\n",
    "        # we compute perplexity\n",
    "        ppl = math.exp(loss_per_token)\n",
    " \n",
    "    return loss_per_token, ppl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_items = 0\n",
    "  \n",
    "    model.eval()\n",
    "    \n",
    "    # we initialise the first hidden state with zeros\n",
    "\n",
    "    prev_hidden = torch.zeros(N_LAYERS, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    \n",
    "    # we do not compute gradients within this block, i.e. no training\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, target = batch.text, batch.target\n",
    "            target = target.permute(1, 0)\n",
    "            logits, prev_hidden = model(text, prev_hidden)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(logits.view(-1, OUTPUT_DIM), target.reshape(-1))\n",
    " \n",
    "\n",
    "            epoch_loss += loss.detach().sum()\n",
    "            epoch_items += loss.numel()\n",
    "            loss_per_token = epoch_loss / epoch_items\n",
    "            ppl = math.exp(loss_per_token)\n",
    "            prev_hidden = save_hidden(prev_hidden)\n",
    "        \n",
    "    return loss_per_token, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, N_EPOCHS=10):\n",
    "\n",
    "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
    "    # good practice to include even if do not use here\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "    \n",
    "        train_loss, train_ppl = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_ppl = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_ppl:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_ppl:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Long short-term memory architectures LSTMs vs. RNNs\n",
    "\n",
    "Remember the vanishing/exploding gradient problem? The gradient signal gets smaller and smaller as it backpropagates further. It is caused by the repeated use of the recurrent weight matrix in RNN. Gradient can be viewed as a measure of the effect of the past on the future. If the gradient becomes vanishingly small over longer distances we can not capture the dependency to the past correctly. For example: \"A patient with a rare sarcoma of soft tissue on the left thigh was presented to the hospital yesterday.\" \"was presented\" depends on \"a patient\", but they are separated by 11 words!\n",
    "\n",
    "![full_lstm](images/lstm_full.png)\n",
    "\n",
    "The key to LSTMs is the cell state $c_t$. It runs straight down the entire chain and allow the information to just flow along it unchanged. LSTM has two \"hidden states\": $c_t$  and $h_t$ . You can think of $c_t$  is the \"internal\" hidden state that retains important information for longer timesteps, whereas $h_t$ is the \"external\" hidden state that exposes that information to the outside world.\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  An LSTM has three of these gates.\n",
    "\n",
    "Forget gate decides what information weâ€™re going to throw away from the cell state. \n",
    "\n",
    "$f_t = \\sigma(W_{if}x_t + W_{hf}h_{t-1}+b_f)$\n",
    "\n",
    "$\\sigma$ squashes input values between 0 and 1, describing how much of each component should be let through. Zero means \"let nothing through\", while a value of one means \"let everything through\".\n",
    "\n",
    "![lstm_ft](images/lstm_ft.png)\n",
    "\n",
    "Input gate decides what new information we are going to store in the cell state. \n",
    "\n",
    "$i_t = \\sigma(W_{ii}x_t + W_{hi}h_{t-1}+b_i)$\n",
    "\n",
    "Next, a tanh layer creates a vector of new candidate values, $g_t$, that could be added to the state. tanh squashes the output values to be between âˆ’1 and 1. \n",
    "\n",
    "$g_t = tanh(W_{ig}x_t + W_{hg}h_{t-1}+b_g)$ (this equation equal to vanilla RNN if we remove gates)\n",
    "\n",
    "The next step combines these two to create an update to the state. Pointwise multiplication operation (*) decides on the parts we output.\n",
    "\n",
    "$c_t = f_t * c_{t-1} + i_t * g_t$\n",
    "\n",
    "![lstm_it_cand](images/lstm_it_cand.png)\n",
    "\n",
    "Finally, the output gate decides how much information goes to the output:\n",
    "\n",
    "$o_t = \\sigma(W_{io}x_t + W_{ho}h_{t-1}+b_o)$\n",
    "\n",
    "$h_t = o_t * tanh(c_t)$\n",
    "\n",
    "![lstm_ot](images/lstm_ot.png)\n",
    "\n",
    "\n",
    "Q: How does this help with the vanishing gradient problem ?\n",
    "\n",
    "Whereas the RNN computes the new hidden state from scratch based on the previous hidden state and the input, the LSTM computes the new hidden state by choosing what to add to the current state. This allows skipping multiplicative gradient paths. Multiple additive paths are hard to converge to ~0.\n",
    "\n",
    "Q: How is the output of the network is computed ?\n",
    "\n",
    "Typically the output layer outputs a vector with the same dimensionality as the vocabulary size of our language model. Those values are inputted into the softmax function that normalises them and outputs a probability distribution over our vocabulary:\n",
    "\n",
    "$y = g(z_i) =  \\frac{e^{z_i}}{\\sum^k_{j=1}e^{z_j}}$\n",
    "\n",
    "Let us have a vocabulary of three words and the output vector $\\mathsf{z}= [2.0, \\; 1.0, \\; 0.1]$. The result of the application of softmax will look like:\n",
    "\n",
    "$y = [\\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.1}}, \\; \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.1}}, \\; \\frac{e^{0.1}}{e^{2.0} + e^{1.0} + e^{0.1}}] = [0.66,\\; 0.24, \\; 0.1]$\n",
    "\n",
    "Q: What is the training loss for LM tasks ?\n",
    "\n",
    "Language models are typically trained to minimise cross-entropy. The entropy of a distribution P measures how many bits you need on average to encode data from P with the code that is optimal for P:\n",
    "\n",
    " $H(P)\\ = - \\sum_ iP(y_i)\\log P(y_i)$\n",
    "\n",
    "To write a number in bits, we need to take a log base 2 of N.\n",
    "\n",
    "The cross-entropy of a distribution P with respect to a distribution Q measures how many bits you need on average to encode data from P with the code that is optimal for Q:\n",
    "\n",
    "$H(P,Q)\\ = \\ - \\sum_ iP(y_i)\\log Q(y_i)$\n",
    "\n",
    "Also useful to measure how well the predicted distribution Q models P.\n",
    "\n",
    "For prediction problems, like predicting words in language modeling, we often have just probability of 1 for the true word (for the rest of the words it is 0, remember the one-hot encoding). \n",
    "\n",
    "So we usually minimise the negative log likehood (NLL) of correct words:\n",
    "\n",
    "$ NLL = -\\sum_i\\log Q(y_i)$\n",
    "\n",
    "where $y$ is the correct word. This is equavalent to maximising probability of correct words.\n",
    "\n",
    "Q: How are language models trained ?\n",
    "\n",
    "Very often the so-called \"teacher forcing\" is used. It works by using the actual ground true outputs from the training dataset at the current time step t as input in the next time step t+1, rather than the output generated by the network. This makes learning faster and the model more stable. The model is not going to get punished for every subsequent word it generates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manual_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forget_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.input_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.candidate_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, prev_hidden):\n",
    "\n",
    "        # shape(x) = [B, T, input_size]\n",
    "        # shape(prev_hidden) = ([1, B, hidden_size], [1, B, hidden_size])\n",
    "\n",
    "\n",
    "        bs, sequence_length, _ = x.size()\n",
    "\n",
    "        # At t=0, h_t and c_t will be initialized to a vector of 0s\n",
    "        h_t = prev_hidden[0].squeeze(0)\n",
    "        c_t = prev_hidden[1].squeeze(0)\n",
    "\n",
    "        hidden_states = torch.zeros(bs, sequence_length, self.hidden_size).to('cuda')\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            # shape(x_t) = [B, input_size]\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            # shape(concat_h_x) = [B, hidden_size+input_size]\n",
    "            concat_h_x = torch.cat((h_t, x_t), dim=-1)\n",
    "\n",
    "            # shape(f_t) = [B, hidden_size]\n",
    "            f_t = self.forget_gate(concat_h_x)\n",
    "\n",
    "            # shape(c_prime_t) = [B, hidden_size]\n",
    "            c_prime_t = c_t * f_t\n",
    "\n",
    "            # shape(i_t) = [B, hidden_size]\n",
    "            # shape(cand_t) = [B, hidden_size]\n",
    "            i_t = self.input_gate(concat_h_x)\n",
    "            cand_t = self.candidate_gate(concat_h_x)\n",
    "\n",
    "            # shape(c_t) = [B, hidden_size]\n",
    "            c_t = c_prime_t + (i_t * cand_t)\n",
    "\n",
    "            # shape(o_t) = [B, hidden_size]\n",
    "            # shape(h_t) = [B, hidden_size]\n",
    "            o_t = self.output_gate(concat_h_x)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_states[:, t, :] = h_t\n",
    "\n",
    "        return hidden_states, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    #  within the __init__ we define the layers: an embedding layer, an RNN, and a linear layer. All the parameters initialized to random values by default, unless explicitly specified.\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pad_idx, is_manual=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # an embedding layer (look-up layer) transforms word indices into word embeddings. It could be initialized with pre-trained embeddings (100D pre-trained GloVe embeddings in ur case) that would be fine-tuned together with other layers.\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "\n",
    "        # unidirectional RNN layer: for LM modelling we do not see the right context\n",
    "        # note the dimensionality changes\n",
    "        # we apply droput technique that sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
    "        \n",
    "        if is_manual:\n",
    "            self.lstm = Manual_LSTM(embedding_dim, hidden_dim)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # now the dimensionality of the output layer is equal to our vocabulary size\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        #apply the dropout\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "       \n",
    "    def forward(self, text, prev_hidden):\n",
    "         \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        # we permute the first two dimentions so that the batch goes first\n",
    "        text = text.permute(1, 0)\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        all_hidden, last_hidden = self.lstm(embedded, prev_hidden)\n",
    "        \n",
    "        #all_hidden = [sent len, batch size, hid dim]\n",
    "        \n",
    "        #last_hidden = [num layers, batch size, hid dim]\n",
    "        \n",
    "        #take all hidden states to produce an output word per time step\n",
    "\n",
    "        logits = self.dropout(all_hidden)\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "            \n",
    "        return self.fc(logits), last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_lstm = LSTM(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            is_manual=True)\n",
    "\n",
    "lstm = LSTM(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_items = 0\n",
    "  \n",
    "    model.train()\n",
    "    \n",
    "    # iterate over batches\n",
    "    # 1 is the number of layers in our LSTM.\n",
    "    prev_ht = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    prev_ct = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "\n",
    "    prev_hidden = (prev_ht, prev_ct)\n",
    "    for batch in iterator:\n",
    "        \n",
    "        # we zero the gradients as they are not removed automatically\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get input text and right-shifted targets we will predict\n",
    "        text, target = batch.text, batch.target\n",
    "        \n",
    "        # permute the dimensions to get the batch size first\n",
    "        target = target.permute(1, 0)\n",
    "        \n",
    "        # starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # otherwise the model would backpropagate all the way to beginning of the dataset.\n",
    "\n",
    "        prev_hidden = save_hidden(prev_hidden)\n",
    "\n",
    "        logits, prev_hidden = model(text, prev_hidden)\n",
    "       \n",
    "        # compute the loss, we reshape inputs to eliminate batching\n",
    "        loss = criterion(logits.view(-1, OUTPUT_DIM), target.reshape(-1))\n",
    "        \n",
    "        # backprop the average loss and update parameters\n",
    "        loss.mean().backward()\n",
    "        \n",
    "        # update the parameters using the gradients and optimizer algorithm \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.detach().sum()\n",
    "        epoch_items += loss.numel()\n",
    "        \n",
    "        # we compute loss per token for an epoch\n",
    "        loss_per_token = epoch_loss / epoch_items\n",
    "        # we compute perplexity\n",
    "        ppl = math.exp(loss_per_token)\n",
    " \n",
    "    return loss_per_token, ppl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_items = 0\n",
    "  \n",
    "    model.eval()\n",
    "    \n",
    "    # we initialise the first hidden state with zeros\n",
    "\n",
    "    prev_ht = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "    prev_ct = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "\n",
    "    prev_hidden = (prev_ht, prev_ct)\n",
    "    \n",
    "    # we do not compute gradients within this block, i.e. no training\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, target = batch.text, batch.target\n",
    "            target = target.permute(1, 0)\n",
    "            logits, prev_hidden = model(text, prev_hidden)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(logits.view(-1, OUTPUT_DIM), target.reshape(-1))\n",
    " \n",
    "\n",
    "            epoch_loss += loss.detach().sum()\n",
    "            epoch_items += loss.numel()\n",
    "            loss_per_token = epoch_loss / epoch_items\n",
    "            ppl = math.exp(loss_per_token)\n",
    "            prev_hidden = save_hidden(prev_hidden)\n",
    "        \n",
    "    return loss_per_token, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model, N_EPOCHS=10):\n",
    "\n",
    "    # to ensure the dropout (explained later) is \"turned on\" while training\n",
    "    # good practice to include even if do not use here\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "    \n",
    "        train_loss, train_ppl = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_ppl = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_ppl:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_ppl:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(manual_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs vs. GRUs\n",
    "\n",
    "Gated Recurrent Unit (GRU) combines the forget and input gates into a single \"update gate\" (z). So we have only two gates: update and reset. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models. Candidate state $g_t$ is able to suppress $h_t$. The final state is a convex combination: of the $g_t$ and $h_{t-1}$ with coefficients of $(1 - z_t)$ and $z_t$ respectively.\n",
    "\n",
    "$r_t = \\sigma(W_{ir}x_t + W_{hr}h_{t-1}+b_r)$\n",
    "\n",
    "$z_t = \\sigma(W_{iz}x_t + W_{hz}h_{t-1}+b_z)$\n",
    "\n",
    "$g_t = tanh(W_{ig}x_t + r_t * (W_{hg}h_{t-1}+b_g))$\n",
    "\n",
    "$h_t = (1 - z_t)* g_t + z_t * h_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.gru = nn.GRU(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence model\n",
    "https://arxiv.org/abs/1409.3215 \\\n",
    "So far we have encountered some classification tasks where the inputs are of variable length. We use Recurrent Neural Networks (RNN/LSTM/GRU) to do predictions. However, when it comes to text generation, the length of outputs might also be random. In this case, we use a sequence-to-sequence model. \\\n",
    "\n",
    "![seq2seq](images/seq2seq.png)\n",
    "\n",
    "A sequence-to-sequence (seq2seq) model is a model that consists of two components called **Encoder** and **Decoder**. Commonly, two recurrent neural networks are used as the encoder and the decoder. The input is fed into the encoder RNN token by token, producing a fix-lengthed vector (the final hidden state) that encodes the context of all input sequence. We refer to this vector as the **context vector**. The decoder uses this context vector as the initialization of its first hidden state and inits the input with the $<sos>$ token, generating the outputs token by token.\n",
    "\n",
    "Seq2seq model is often used in NLP tasks where the lengths of both input and output are not fixed, e.g. machine translation, dialogue system. In the following part, we are going to build a vanilla seq2seq model with LSTM as encoder/decoder module on the machine translation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html \\\n",
    "https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# import essential libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "\n",
    "writer = SummaryWriter('runs/seq2seq')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Encoder\n",
    "We have three layers in the encoder: an embedding layer (with dropout), a RNN layer, and a linear layer. As we have known from the word representation session, we can apply a embedding layer and distributed word representation is trained jointly with the model. \n",
    "\n",
    "If we want to have a bidirectional encoder to encode both forward and backward contexts in the input, the hidden dimension of the RNN layer is doubled. Therefore, the linear layer is here to keep the same dimensionality between the encoder output and decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, layers, PAD_IDX=1, bidirectional=False, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.PAD_IDX = PAD_IDX\n",
    "        \n",
    "        \n",
    "        # If we use a bidirectional encoder to encode both forward and backward context,\n",
    "        # the dimension of the hidden state will double\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if bidirectional:\n",
    "            ff_input_dim = 2 * hidden_dim\n",
    "        else:\n",
    "            ff_input_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, self.hidden_dim, layers, dropout=dropout, \\\n",
    "                           bidirectional=bidirectional, bias=False)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(ff_input_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    # x: (seq_len, batch_size)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[1]\n",
    "\n",
    "        # x: (seq_len, batch_size, emb_dim)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        # outputs: (seq_len, batch_size, input_dim)\n",
    "        # h_n: (layers*directions, batch_size, hidden_dim)\n",
    "        outputs, (h_n, c_n) = self.rnn(x)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # concatenate the forward and backward hidden states\n",
    "            h_n = torch.cat((h_n[0::2,:,:], h_n[1::2,:,:]), dim = -1)\n",
    "            c_n = torch.cat((c_n[0::2,:,:], c_n[1::2,:,:]), dim = -1)\n",
    "        \n",
    "        h_n = self.ff(h_n)\n",
    "        c_n = self.ff(c_n)\n",
    "        \n",
    "        return outputs, (h_n, c_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder has four layers: an embedding layer (with dropout), a unidirectional RNN layer and two linear layers. The decoder is always unidirectional in that we only generate the outputs from left to right. \n",
    "\n",
    "Remember that the embedding layer is actually a matrix of all word vectors, therefore it is the same as a linear layer. Assuming we have the same dimensionality for the embedding layer and the output linear layer, we can perform a weight tying by sharing all the parameters in the two layers. This step would reduce the model size and might improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, layers, weight_tying=True, PAD_IDX=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        '''\n",
    "        TODO define the embedding layer, rnn layer and the two linear layers, with correct dimensions\n",
    "        '''\n",
    "        self.embedding = nn.Embedding()\n",
    "        self.rnn = nn.LSTM() # we don't set bidirectional here\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # This linear layer is to ensure the output layer has the same dimensionality with the embedding layer\n",
    "        self.out = nn.Linear()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        TODO apply weight tying by sharing the weights of the embedding and output layer\n",
    "        '''\n",
    "        # share the weights for embedding layer and output layer\n",
    "        if weight_tying:\n",
    "            self.embedding.weight =\n",
    "\n",
    "        \n",
    "    # x: (batch_size)\n",
    "    def forward(self, x, hidden):\n",
    "        # we expand the dim of sequence length\n",
    "        # x: (1, batch_size)\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        '''\n",
    "        TODO feed the input to the embedding layer and dropout layer\n",
    "        '''\n",
    "        # embed: (1, batch_size, emb_dim)\n",
    "        embed = \n",
    "\n",
    "        '''\n",
    "        TODO feed the word vector to the RNN layer, initializing hidden state with the encoder hidden state\n",
    "        '''\n",
    "        # output: (1, batch_size, hidden_dim)\n",
    "        # h_n: (layers, batch_size, hidden_dim)\n",
    "        output, hidden = \n",
    "        \n",
    "        '''\n",
    "        TODO pass through the two linear layers\n",
    "        '''\n",
    "        # output: (1, batch_size, emb_dim)\n",
    "        output = \n",
    "        \n",
    "        # output: (batch_size, output_dim)\n",
    "        output = \n",
    "        output = output.squeeze(0)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-3ef7e81c53c7>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-3ef7e81c53c7>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    dec_output =\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device='cpu'):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    # src: [seq_len, batch_size]\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size, output_dim).to(self.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # initialize output sequence with '<sos>'\n",
    "        dec_output = trg[0,:]\n",
    "        \n",
    "        # decoder token by token\n",
    "        for t in range(1, max_len):\n",
    "            dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "            outputs[t] = dec_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            pred_next = dec_output.argmax(1)\n",
    "            \n",
    "            '''\n",
    "            TODO use the ground truth token if using teacher forcing\n",
    "            '''\n",
    "            dec_output = \n",
    "        return outputs\n",
    "\n",
    "    # greedy search for actual translation\n",
    "    def greedy_search(self, src, sos_idx, max_len=50):\n",
    "        src = src.to(self.device)\n",
    "        batch_size = src.shape[1]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size).to(self.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        \n",
    "        dec_output = torch.zeros(batch_size, dtype=torch.int64).to(device)\n",
    "        dec_output.fill_(sos_idx)\n",
    "        \n",
    "        outputs[0] = dec_output\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "            \n",
    "            dec_output = dec_output.argmax(1)\n",
    "\n",
    "            outputs[t] = dec_output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished our seq2seq model, let's build a toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=4\n",
    "OUTPUT_DIM=4\n",
    "EMB_DIM=10\n",
    "HIDDEN_DIM=6\n",
    "LAYERS=2\n",
    "\n",
    "# define the encoder and decoder, and build the model\n",
    "'''\n",
    "TODO define an encoder and a decoder, and then define the model\n",
    "'''\n",
    "enc = \n",
    "dec = \n",
    "model = \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading with Torchtext\n",
    "https://pytorch.org/text/ \\\n",
    "Now we are running a machine translation model on actual dataset: Multi30k. Multi30k is a dataset for multi-modal machine translation. We'll only use the texts in this dataset and we load the dataset with *Torchtext*, which can help us with all the pre-processing and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# torchtext will pre-process the data, including tokenization, padding, stoi, etc.\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "# print the number of examples in train/valid/test sets\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocab of our training set, ignoring word with frequency less than 2\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train/valid/test iterators, which will batch the data for us\n",
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "x = vars(test_data.examples[0])['src']\n",
    "y = vars(test_data.examples[0])['trg']\n",
    "print(\"Source example:\", \" \".join(x))\n",
    "print(\"Target example:\", \" \".join(y))\n",
    "print(\"Padded target:\", TRG.pad([y]))\n",
    "print(\"Tensorized target:\", TRG.process([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, optimizer and criterion\n",
    "This is our model hyperparameters. In actual training, we might need to tune the hyperparameters on the validation set before evaluating on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM=256\n",
    "HIDDEN_DIM=512\n",
    "LAYERS=1\n",
    "DROPOUT=0.5\n",
    "BIDIRECTIONAL=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO get indexes of the padding tokens in source and target vocabulary\n",
    "'''\n",
    "# padding token\n",
    "SRC_PAD = \n",
    "TRG_PAD = \n",
    "\n",
    "# build model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=SRC_PAD, bidirectional=BIDIRECTIONAL, dropout=DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=TRG_PAD, dropout=DROPOUT)\n",
    "model = Seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer will update the gradient everytime we back-propagate. We are using Adam as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.001\n",
    "# set optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *CrossEntropyLoss* as our loss function, which will calculate the log softmax and the negative log-likelihood. We pass the padding token in the target vocab to the criterion so that it will ignore the loss for this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our training loop.\n",
    "1. We iterate over the training iterator and get a batch of training examples\n",
    "2. The input is passed through the model and it returns the predictions\n",
    "3. We calculate the loss between the model predictions and the ground truths\n",
    "4. We back-propagate the loss and the optimizer will update the gradients\n",
    "\n",
    "To avoid exploding gradient, we clip the gradients to a maximum value every training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip, num_epoch):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        # set gradients to zero to avoid accumulating the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(src, trg)\n",
    "        \n",
    "        # exclude <sos> token\n",
    "        # outputs: (seq_len * batch_size, output_dim)\n",
    "        # trg : (seq_len * batch_size)\n",
    "        outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(outputs, trg)\n",
    "        \n",
    "        writer.add_scalar('training loss',\n",
    "                            loss.item(),\n",
    "                            num_epoch * len(iterator) + i)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Batch:\\t {0} / {1},\\t loss: {2:2.3f}'.format(i, len(iterator), loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        # clip grad to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluating loop is similar to the training loop, except that we don't want to do back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, criterion):\n",
    "    # In eval model, layers such as Dropout, BatchNorm will work in eval model\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    # this prevents the back-propagation\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            # during test time, we have no correct trg so we turn off teacher forcing\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(outputs, trg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score\n",
    "\n",
    "An automatic metric that assumes that the closer a machine translation is to a professional human translation, the better it is. Rather strong assumption considering in how many different \"correct\" ways a sentence could be translated. For example, the French sentence \"\n",
    "Courage!\" could be translated into English as \"Cheer up!\", \"Go for it!\", \"Chin up!\", etc.\n",
    "BLEU computes N-gram matching between system output and one or more reference (human) translations. N-gram is simply a sequence of N words within a given window and when computing the N-grams you typically move one word forward. Typically values between of N between 1 and 5 are considered. According to the formula $m = N-n+1$, in the sentence \"I like chocolate and vanilla ice cream\" there are:\n",
    "\n",
    "- 7 unigrams (1-grams)\n",
    "- 6 bigrams (2-grams)\n",
    "- 5 trigrams (3-grams)\n",
    "- 4 quadrigrams (4-grams)\n",
    "\n",
    "BLEU rewards same words in equal order. It is the most widely used metric. The final score ranges from 0-100, the higher the score, the more the translation correlates to a human translation. BLEU computes geometirc mean and is a document-level metric: if a higher order n-gram precision (e.g., n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, even if some lower order n-grams are matched:\n",
    "\n",
    "$BLEU = brevity\\_penalty \\cdot exp(\\sum^N_{n=1}\\log modified\\_precision_n)$\n",
    "\n",
    "The brevity penalty penalizes short translations. The default configuration uses N = 4 and uniform weights.\n",
    "\n",
    "![bleu1](images/bleu1.png)\n",
    "![bleu2](images/bleu2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, converting a batch of tensors to the text form\n",
    "def get_text_from_tensor(tensor, field, eos='<eos>'):\n",
    "    batch_output = []\n",
    "    for i in range(tensor.shape[1]):\n",
    "        sequence = tensor[:,i]\n",
    "        words = []\n",
    "        for tok_idx in sequence:\n",
    "            tok_idx = int(tok_idx)\n",
    "            token = field.vocab.itos[tok_idx]\n",
    "\n",
    "            if token == '<sos>':\n",
    "                continue\n",
    "            elif token == '<eos>' or token == '<pad>':\n",
    "                break\n",
    "            else:\n",
    "                words.append(token)\n",
    "        words = \" \".join(words)\n",
    "        batch_output.append(words)\n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def test_bleu(model, iterator, trg_field):\n",
    "    model.eval()\n",
    "\n",
    "    ref = []\n",
    "    hyp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            outputs = model.greedy_search(src, trg_field.vocab.stoi['<sos>'])\n",
    "            \n",
    "            hyp += get_text_from_tensor(outputs, trg_field)\n",
    "            ref += get_text_from_tensor(trg, trg_field)\n",
    "            \n",
    "    # expand dim of reference list\n",
    "    # sys = ['translation_1', 'translation_2']\n",
    "    # ref = [['truth_1', 'truth_2'], ['another truth_1', 'another truth_2']]\n",
    "    ref = [ref]\n",
    "    return sacrebleu.corpus_bleu(hyp, ref, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start our training! We keep the checkpoint with the highest valid BLEU as our best checkpoint.\n",
    "\n",
    "**The training is heavily dependent on GPU, so it might take years to train on CPU. You may skip this block and load our pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 30\n",
    "CLIP = 1\n",
    "\n",
    "best_bleu = float('Inf')\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('Start training Epoch {}:'.format(i+1))\n",
    "    \n",
    "    '''\n",
    "    TODO calculate training loss and valid loss using the function we defined above\n",
    "    '''\n",
    "    train_loss = \n",
    "    valid_loss = \n",
    "    bleu = test_bleu(model, test_iterator, TRG)\n",
    "    \n",
    "    writer.add_scalar('valid loss',\n",
    "                valid_loss,\n",
    "                i)\n",
    "    writer.add_scalar('valid ppl',\n",
    "                      math.exp(valid_loss),\n",
    "                     i)\n",
    "    writer.add_scalar('valid BLEU',\n",
    "                bleu.score,\n",
    "                i)\n",
    "    \n",
    "    if bleu.score > best_bleu:\n",
    "        best_bleu = bleu.score\n",
    "        torch.save(model.state_dict(), 'checkpoint_best-seq2seq.pt')\n",
    "    \n",
    "    print('Epoch {0} train loss: {1:.3f} | Train PPL: {2:7.3f}'.format(i+1, train_loss, math.exp(train_loss)))\n",
    "    print('Epoch {0} valid loss: {1:.3f} | Valid PPL: {2:7.3f}'.format(i+1, valid_loss, math.exp(valid_loss)))\n",
    "    print('Epoch {0} valid BLEU: {1:3.3f}'.format(i+1, bleu.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoint_best-seq2seq.pt'))\n",
    "\n",
    "'''\n",
    "TODO calculate bleu score on the test set\n",
    "'''\n",
    "test_bleu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
