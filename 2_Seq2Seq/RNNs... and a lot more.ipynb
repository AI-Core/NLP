{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by [Julia Ive](mailto:j.ive@imperial.ac.uk), [Zhenhao Li](mailto:zhenhao.li18@imperial.ac.uk), and [Nihir](mailto:nv419@ic.ac.uk).\n",
    "\n",
    " [The AI Core](https://theaicore.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [RNNs Recap and Primer](#RNNs-Recap-and-Primer)\n",
    " - [RNNs for text classification](#RNNs-for-Text-Classification-Task)\n",
    " - [Bi-directional RNNs](#BiDirectional-RNNs)\n",
    "- [RNNs for Language Modelling](#RNNs-for-Language-Modelling)\n",
    " - [Evaluation of Language Models](#Evaluation-of-Language-Models)\n",
    " - [Long short term memory architectures LSTMs vs. RNNs](#Long-short-term-memory-architectures-LSTMs-vs.-RNNs)\n",
    "- [Sequence to sequence model](#Sequence-to-sequence-model)\n",
    " - [BLEU Score](#BLEU-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs Recap and Primer\n",
    "\n",
    "\n",
    "## RNNs Recap\n",
    "\n",
    "RNNs are designed to make use of sequential data, when the current step has some kind of relation with the previous steps. This makes them ideal for applications with a time component (audio, time-series data) and natural language. RNNs are networks for which value of a unit depends on its own previous output as input.\n",
    "\n",
    "An input vector representing the current input element $x_t$ is multiplied by a weight matrix $W$ and then passed through an activation function to compute an activation value for a layer of hidden units. This hidden layer is, in turn, used to calculate a corresponding output, $y_t$. \n",
    "\n",
    "$h_t = g(Uh_{t-1}+Wx_t)$\n",
    "\n",
    "$y_t = a(Vh_t)$\n",
    "\n",
    "The hidden layer from the previous time step $h_{t-1}$ provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. $U$ determine how the network should make use of past context. RNNs do not impose any limit on this prior context. The context includes information dating back to the beginning of the sequence. Three sets of weights are updated at each timestep: $W$, $U$ and $V$.\n",
    "\n",
    "\n",
    "![rnn_cell](images/rnn_cell.png)\n",
    "\n",
    "\n",
    "## RNNs for Text Classification Task\n",
    "\n",
    "There are many variations of RNN’s likely Many-One, Many-Many, etc. We will work with a popular classification task of sentiment analysis, the extraction of the sentiment that a writer expresses toward something he/she describes. In our case we aim to classify the input text into positivem, negative or neutral class. For example, a positive sentence \"I loved the movie\", a negative \"I hated the movie\" and a neutral \"The movie was about Australia\". So, set of Many-One LSTM units achieves the task as only one value needs to be outputted for determining the polarity of the review. Usually this is the last RNN hidden state as the one that summarises the whole sequence.\n",
    "\n",
    "\n",
    "Q: Why RNNs better than FFNNs?\n",
    "\n",
    "FFNNs do not take context into account. Each word is represented by its embedding independent of other words. RNNs encodes each new word (token) considering the previous words. Meaning of words can change depending on the context. For example, compare the meanings of the word \"mean\" in those two sentences \"I compute the mean\" and \"His behaviour was mean\".\n",
    "\n",
    "\n",
    "![rnn_classification](images/rnn_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "#We will work with a dataset from the torchtext package consists of data processing utilities and popular datasets for NLP\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "# We fix the seeds to get consistent results for each training.\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Helper function to print time between epochs\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading trainDevTestTrees_PTB.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\sst\\trainDevTestTrees_PTB.zip: 100%|███████████████████████████████████████████| 790k/790k [00:02<00:00, 266kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n",
      "train.fields: {'text': <torchtext.data.field.Field object at 0x0000024AE8CE0508>, 'label': <torchtext.data.field.LabelField object at 0x0000024AE94C05C8>}\n",
      "len(train): 8544\n",
      "vars(train[0]): {'text': ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '`', '`', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean', '-', 'claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# With TorchText Field we define how our data will be processed: here we will use Spacy for tokenisation\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', lower=True)\n",
    "LABEL = data.LabelField(dtype = torch.long)\n",
    "\n",
    "# We will experiment with a widely used Stanford Treebank dataset and will predict sentiment of movie reviews\n",
    "# Our data will be classified in three labels: positive, negative and neutral\n",
    "# We take the standard split\n",
    "\n",
    "train_data, valid_data, test_data = datasets.SST.splits(\n",
    "            TEXT, LABEL)\n",
    "\n",
    "# Print stat over the data\n",
    "\n",
    "print('train.fields:', train_data.fields)\n",
    "print('len(train):', len(train_data))\n",
    "print('vars(train[0]):', vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Vocabulary Length 15490\n",
      "Label Vocabulary Length:  3\n",
      "[('.', 8041), ('the', 7353), (',', 7131), ('a', 5305), ('and', 4516), ('of', 4456), ('to', 3050), ('-', 2737), ('is', 2565), (\"'s\", 2544), ('it', 2428), ('that', 1955), ('in', 1916), ('as', 1299), ('but', 1172), ('film', 1166), ('with', 1139), ('for', 1037), ('movie', 1016), ('this', 998)]\n",
      "defaultdict(None, {'positive': 0, 'negative': 1, 'neutral': 2})\n"
     ]
    }
   ],
   "source": [
    "# Now we build a vocabulary out of tokens available from the pre-trained embedding list and the vocabulary of labels.\n",
    "\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.50d\")\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print('Text Vocabulary Length', len(TEXT.vocab))\n",
    "print (\"Label Vocabulary Length: \", len(LABEL.vocab))\n",
    "\n",
    "#We can display the most common words in the vocabulary and their frequencies\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "\n",
    "#We can also see the vocabulary directly using the stoi (string to int)\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# place the tensors on the GPU if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BucketIterator is an iterator that will return a batch of examples of similar lengths, minimizing the amount of padding per example.\n",
    "# Padding refers to fixing the length of inputs (adding a reserved token a certain amount of times to match certain length), usually to the max length within a batch. For exmaple:\n",
    "# i         like  this  movie <pad>\n",
    "# the       movie is    very  good\n",
    "# excellent !     <pad> <pad> <pad>\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "            (train_data, valid_data, test_data), \n",
    "            batch_size = BATCH_SIZE,\n",
    "            sort_within_batch = True,\n",
    "            device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BucketIterator object at 0x0000024AF6815AC8>\n",
      "\n",
      "[torchtext.data.batch.Batch of size 64 from SST]\n",
      "\t[.text]:[torch.LongTensor of size 22x64]\n",
      "\t[.label]:[torch.LongTensor of size 64]\n",
      "\n",
      "Demo batch `text` shape: torch.Size([22, 64])\n",
      "Demo batch `text` transpose shape: torch.Size([64, 22])\n",
      "Demo batch `text` sample: \n",
      " tensor([[   83,   262,     4,   106,   134,   259,     4,    10,  1060,     4,\n",
      "          1172,     6,  1544,    39,   729,     5,  1224,    14,     3,   215,\n",
      "           295,     2],\n",
      "        [   29,    79,  1023,    69,  1429,   102,    41,     3,   710,  2023,\n",
      "             4,  1447,     9, 15275, 12243,  4485,    28,   167, 10162,     6,\n",
      "          3081,     2],\n",
      "        [    3,  4230,  1861,    13, 10847,  6334,    11,  1373,    94,    10,\n",
      "          3711,    14,   414,     8,  2165,    60,   115,   143,   757,     6,\n",
      "          1119,     2]])\n",
      "\n",
      "Demo batch `label` shape: torch.Size([64])\n",
      "Demo batch `label` sample: tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_iterator)\n",
    "\n",
    "for batch in train_iterator:\n",
    "    demo_batch = batch\n",
    "    break\n",
    "    \n",
    "print(demo_batch)\n",
    "\n",
    "print()\n",
    "\n",
    "# Note that demo_batch.text has a shape of [sentence length x batch size]\n",
    "print(\"Demo batch `text` shape:\", demo_batch.text.shape)\n",
    "# We can simply reshape this into the more familiar [batch size x sentence length]\n",
    "print(\"Demo batch `text` transpose shape:\", demo_batch.text.T.shape)\n",
    "print(\"Demo batch `text` sample: \\n\", demo_batch.text.T[:3, :])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Demo batch `label` shape:\", demo_batch.label.shape)\n",
    "# shape(demo_batch.label.shape) = [batch size]\n",
    "print(\"Demo batch `label` sample:\", demo_batch.label[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RNN class\n",
    "\n",
    "Within the constructor we define the layers:\n",
    " - An **embedding layer** which acts as a lookup table to map our tokens to their vector \n",
    " - An **RNN** \n",
    " - A **linear layer**. This layer receives the last hidden state from the RNN and outputs logits of `output dim` dimensionality \n",
    "\n",
    "All the parameters initialized to random values by default, unless explicitly specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 bidirectional, dropout, pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # An embedding layer (look-up layer) transforms word indicies into word embeddings. \n",
    "        # Here, we initialize our model with pre-trained embeddings (100D pre-trained GloVe embeddings in our case).\n",
    "        # This layer will fine-tune these embeddings, specific to this model/dataset.\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "        # We can also train the embeddings from scratch:\n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # An RNN layer. we specify that the batch dimension goes first\n",
    "        # We have a bidirectional flag which indicates whether the model is unidirectional or bidirectional\n",
    "        # RNNs can be stacked - i.e. have multiple layers. Here, we will only look at the 1 layer case.\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          num_layers=1)\n",
    "\n",
    "        # The linear layer takes the final hidden state and feeds it through a fully connected layer.\n",
    "          # The dimensionality of the output is equal to the output class count.\n",
    "          # For classification in a bidirectional RNN we concatenate:\n",
    "            #  - The last hidden state from the forward RNN (obtained from final word of the sentence)\n",
    "            #  - The last hidden state from the backward RNN (obtained from the first word of the sentence)\n",
    "          # Due to the concatenation, our hidden size is doubled.\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            linear_hidden_in = hidden_dim * 2\n",
    "        else:\n",
    "            linear_hidden_in = hidden_dim\n",
    "\n",
    "        # The classification (linear) layer\n",
    "        self.fc = nn.Linear(linear_hidden_in, output_dim)\n",
    "        \n",
    "\n",
    "        # We apply dropout technique that sets a random set of activations of a layer to zero.\n",
    "          # This prevents the network from learning to rely on specific weights and helps to prevent overfitting. \n",
    "          # Note that the dropout layer is only used during training, and not during test time.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # ACRONYMS:\n",
    "          # B = Batch size\n",
    "          # T = Max sentence length\n",
    "          # E = Embedding dimension\n",
    "          # D = Hidden dimension\n",
    "          # O = Output dimension\n",
    "\n",
    "        # shape(text) = [B, T]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # shape(embedded) = [B, T, E]\n",
    "        \n",
    "        # An RNN in PyTorch returns two values:\n",
    "        # (1) All hidden states of the last RNN layer\n",
    "        # (2) Hidden state of the last timestep for every layer\n",
    "          # Note: we are only using 1 layer\n",
    "        all_hidden, last_hidden = self.rnn(embedded)\n",
    "        # shape(all_hidden) = [B, T, D*num_directions]\n",
    "        # shape(last_hidden) = [num_layers*num_directions, B, D].  num_layers = 1\n",
    "        # NOTE. If we were to NOT use the `batch_first` flag, shape of all_hidden would be [T, B, D*num_directions]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Concat the final forward (hidden[0,:,:]) and backward (hidden[1,:,:]) hidden layers\n",
    "            last_hidden = torch.cat((last_hidden[0, :, :], last_hidden[1, :, :]), dim=-1)\n",
    "            # shape(last_hidden) = [B, D*2]\n",
    "\n",
    "        else:\n",
    "            last_hidden = last_hidden.squeeze(0)\n",
    "            # shape(last_hidden) = [B, D]\n",
    "\n",
    "        # Our predictions.\n",
    "        logits = self.fc(self.dropout(last_hidden))\n",
    "        # shape(logits) = [B, O]\n",
    "        \n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.3\n",
    "# get our pad token index from the vocabulary\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise the RNN now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How the model is evaluated?\n",
    "\n",
    "The natural choise is accuracy - \n",
    "percentage of all the examples that our model labeled correctly. However, it is not good for unbalanced datasets. Imagine a dataset with 999,900 positive examples and 100 negative examples. A very bad classifier can assign positive class to all the examples. This classifier would have 999,900 true negatives and only 100 false negatives and an accuracy of\n",
    "999,900/1,000,000 or 99.99%! \n",
    "\n",
    "Other metrics, more useful for such datasets are: precision, recall and F-measure. Precision measures the percentage of the items that the system labelled as positive and are positive accroding to the gold labels. Recall measures the percentage of items labelled as positive out of all the gold positive items. F-score is the weighted harmonic mean of the precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    returns accuracy per batch\n",
    "    \"\"\"\n",
    "\n",
    "    class_preds = nn.Softmax(dim=-1)(preds)\n",
    "    class_preds = class_preds.max(-1)[1]\n",
    "    correct = (class_preds == y).float() # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# we use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizier, criterion, N_EPOCHS=15):\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "\n",
    "        # To ensure the dropout is \"turned on\" while training\n",
    "          # (good practice to include in your projects even if it is not used)\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        # `batch` is a tuple of Tensors: (TEXT, LABEL)\n",
    "        for batch in train_iterator:\n",
    "                        \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text = batch.text\n",
    "            labels = batch.label\n",
    "            # shape(text) = [T, B]\n",
    "            # shape(label) = [B]\n",
    "            \n",
    "            # We reshape text to [B, T]. \n",
    "            # This is purely so we can think about the shapes of the Tensors more consistently\n",
    "            text = text.T\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            # compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "        \n",
    "            # compute training accuracy\n",
    "            acc = accuracy(predictions, labels)\n",
    "              \n",
    "            # calculate the gradient of each parameter\n",
    "            loss.backward()\n",
    "        \n",
    "            # update the parameters using the gradients and optimizer algorithm \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "        average_epoch_loss = epoch_loss / len(train_iterator)\n",
    "        average_epoch_acc = epoch_acc / len(train_iterator)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        average_epoch_valid_loss, average_epoch_valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {average_epoch_loss:.3f} | Train Acc: {average_epoch_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {average_epoch_valid_loss:.3f} |  Val. Acc: {average_epoch_valid_acc*100:.2f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # Turn on evaluate mode.\n",
    "      # De-activates dropout and batch normalization (which we will cover in a future session)\n",
    "    model.eval()\n",
    "\n",
    "    # We do not compute gradients within this block, i.e. no training\n",
    "    # https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/2\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            labels = batch.label\n",
    "\n",
    "            text = text.T\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.046 | Train Acc: 45.45%\n",
      "\t Val. Loss: 0.994 |  Val. Acc: 53.00%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.001 | Train Acc: 52.96%\n",
      "\t Val. Loss: 1.003 |  Val. Acc: 52.98%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.986 | Train Acc: 54.26%\n",
      "\t Val. Loss: 0.958 |  Val. Acc: 54.98%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.983 | Train Acc: 54.47%\n",
      "\t Val. Loss: 0.953 |  Val. Acc: 56.98%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.976 | Train Acc: 55.32%\n",
      "\t Val. Loss: 1.009 |  Val. Acc: 50.21%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.975 | Train Acc: 54.83%\n",
      "\t Val. Loss: 0.962 |  Val. Acc: 55.41%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.981 | Train Acc: 54.79%\n",
      "\t Val. Loss: 0.967 |  Val. Acc: 56.64%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.974 | Train Acc: 54.92%\n",
      "\t Val. Loss: 0.957 |  Val. Acc: 56.73%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.966 | Train Acc: 55.99%\n",
      "\t Val. Loss: 0.993 |  Val. Acc: 52.98%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.970 | Train Acc: 54.97%\n",
      "\t Val. Loss: 0.955 |  Val. Acc: 56.89%\n",
      "Epoch: 11 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.966 | Train Acc: 55.33%\n",
      "\t Val. Loss: 1.022 |  Val. Acc: 51.08%\n",
      "Epoch: 12 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.975 | Train Acc: 54.55%\n",
      "\t Val. Loss: 0.953 |  Val. Acc: 56.56%\n",
      "Epoch: 13 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.964 | Train Acc: 55.73%\n",
      "\t Val. Loss: 0.956 |  Val. Acc: 56.62%\n",
      "Epoch: 14 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.966 | Train Acc: 55.19%\n",
      "\t Val. Loss: 0.983 |  Val. Acc: 56.03%\n",
      "Epoch: 15 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.968 | Train Acc: 55.01%\n",
      "\t Val. Loss: 0.949 |  Val. Acc: 57.43%\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problems where all timesteps of the input sequence are available, bidirectional RNNs train two instead of one RNNs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. Outputs at the same step are then usually concatenated. This can provide additional useful context to the model.\n",
    "\n",
    "Q: Why is a bi-directional RNN is better than single-direction ?\n",
    "\n",
    "Imagine that you see only the left context: \"We went to ...\" This context is very general and a lot of different words can continue: nouns (London, work, cinema, doctor), verbs (join, support), etc. When we both left and right contexts the word \"sleep\" is becoming evident: \"We went to ... early but still could not wake up on time.\"\n",
    "\n",
    "\n",
    "![bi_rnn_classification](images/bi_rnn_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIDIRECTIONAL = True\n",
    "\n",
    "## bidirectional_model ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## call train with the bidirectional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(bidirectional_model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for Language Modelling\n",
    "\n",
    "Language model is required to represent the text to a form understandable from the machine point of view. Language Modelling (LM) is at the core of Natural Language Processing (NLP). Base of all the NLP tasks: Machine Translation, Spell Correction, Speech Recognition, Summarization, Question Answering, Sentiment analysis etc.  \n",
    "\n",
    "Language is a sequence of letters, words, sentences, paragraphs, etc. These units are not independent. When we comprehend and produce spoken language, we are processing continuous input streams of indefinite\n",
    "length. And even when dealing with written text we normally read it sequentially. Thus, RNNs is a perfect fit to model language data because with RNNs we can represent language sequence of any length into a fixed-sized vector.\n",
    " \n",
    "Q: What is the difference between word embeddings and language modelling?\n",
    "\n",
    "The main difference that word embeddings do not take into account word order. Language models take word order into account. The word order is important. If you do not take the word order into account the representation of the following sentences will be the same: \"It was really not good, on the opposite quite bad.\" and \"It was really not bad, on the opposite quite good.\" However, the meaning of those two sentences is different.\n",
    "\n",
    "![rnn_lm](images/rnn_lm.png)\n",
    "\n",
    "\n",
    "You may have heard about BERT. BERT is a general-purpose pre-trained language model. It is pre-trained using a lot of language data from Internet to create a better \"grasp\" of language. It is bidirectional. This means a deeper sense of language context and flow compared to the single-direction language models. You can download it and fine-tune for your NLP problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With TorchText Field we define how our data will be processed\n",
    "TEXT = data.Field(tokenize = 'spacy', init_token = '<sos>')\n",
    "\n",
    "# We will be using the WikiText-2 corpus, which is a popular LM dataset.\n",
    "# The WikiText language modeling dataset is a collection of texts extracted \n",
    "  # from good and featured articles on Wikipedia.\n",
    "# It contains about 2 million words \n",
    "train_data, valid_data, test_data = datasets.WikiText2.splits(TEXT)\n",
    "\n",
    "# Data stats\n",
    "print('train.fields', train_data.fields)\n",
    "print('len(train)', len(train_data))\n",
    "\n",
    "# Build a vocabulary out of tokens available from the pre-trained embeddings list and the vocabulary of labels\n",
    "TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\")\n",
    "print('Text Vocabulary Length', len(TEXT.vocab))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# place the tensors on the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of the Backpropagation training algorithm to RNNs applied to sequence data. Each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPTTIterator (Backpropagation Through Time Iterator)\n",
    "# divides the corpus into batches of [sequence length, bptt_len]\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BPTTIterator.splits(\n",
    "            (train_data, valid_data, test_data), \n",
    "                batch_size = BATCH_SIZE, bptt_len=30,\n",
    "                device = device, repeat=False)\n",
    "\n",
    "for batch in train_iterator:\n",
    "    demo_batch = batch\n",
    "    break\n",
    "    \n",
    "print(demo_batch)\n",
    "\n",
    "print()\n",
    "\n",
    "# Note that the first dimension is the sequence, and the next is the batch.\n",
    "  # We can reshape this to [batch size, sentence length] as we did earlier with a transpose.\n",
    "# The target is the original text offset by 1\n",
    "print(\"Demo batch `text`:\\n\", demo_batch.text[:5, :3])\n",
    "print(\"Demo batch `target`:\\n\", demo_batch.target[:5, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    # variant is a flag which is either: \"rnn\", \"lstm\", \"manual_lstm\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, pad_idx, variant):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.variant = variant\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors)\n",
    "\n",
    "        # UNIDIRECTIONAL RNN layer: For LM modelling we do not see/have access to the right context\n",
    "        \n",
    "        if variant == \"rnn\":\n",
    "            ##self.rnn =\n",
    "        elif variant == \"lstm\":\n",
    "            self.rnn = nn.LSTM(embedding_dim, \n",
    "                               hidden_dim, \n",
    "                               batch_first=True)\n",
    "        elif variant == \"manual_lstm\":\n",
    "            self.rnn = Manual_LSTM(embedding_dim, hidden_dim)\n",
    "        else:\n",
    "            raise ValueError(\"Expected `variant` to be one of 'rnn', 'lstm', or 'manual_lstm'\")\n",
    "            \n",
    "        ##self.fc =\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "       \n",
    "    def forward(self, text, prev_hidden):\n",
    "         \n",
    "        # shape(text) = [B, T]\n",
    "        \n",
    "        # If vanilla RNN:\n",
    "            # shape(prev_hidden) = [1, B, D] where 1 = num_layers*num_directions\n",
    "        # If LSTM:\n",
    "            # prev_hidden is a tuple of previous hidden states and cell states: (ALL_HIDDEN_STATES, ALL_CELL_STATES)\n",
    "            # shape(ALL_HIDDEN_STATES)=shape(ALL_CELL_STATES) = [1, B, D] where 1 = num_layers*num_directions\n",
    "            \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # shape(embedded) = [B, T, E]\n",
    "        \n",
    "        all_hidden, last_hidden = self.rnn(embedded, prev_hidden)        \n",
    "        # shape(all_hidden) = [B, T, D]\n",
    "        # shape(last_hidden) = [num layers, B, T]\n",
    "        \n",
    "        # Take all hidden states to produce an output word per time step\n",
    "        ##logits =\n",
    "        # shape(logits) = [B, O]\n",
    "            \n",
    "        return logits, last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(TEXT.vocab)\n",
    "DROPOUT = 0.5\n",
    "# get our pad token index from the vocabulary\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            variant=\"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Use the Adam optimizer\n",
    "##optimizer =\n",
    "\n",
    "# we use the cross-entropy loss\n",
    "## criteron =\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to detach the hidden state or else the model will try to backpropagate to the beginning of the dataset, requiring a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden(hidden):\n",
    "  \"\"\"Wraps hidden states in new Tensors, to declare it not to need gradients. So that the initial hidden state for this batch is constant and doesn’t depend on anything.\"\"\"\n",
    "\n",
    "  if isinstance(hidden, torch.Tensor):\n",
    "    return hidden.detach()\n",
    "  else:\n",
    "    return tuple(save_hidden(v) for v in hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Language Models\n",
    "\n",
    "Language is very difficult to evaluate since there is no single gold truth: one meaning could be expressed in many valid ways.\n",
    "\n",
    "\n",
    "### Human Evaluation\n",
    "\n",
    "Human evaluation is costly, slow and subjective but reliable. Human evaluation of a language model may involve how a hypothesis satisfies the grammatical and lexical norms of a language.\n",
    "\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "Does it prefer real (=frequently observed) sentences to ‘ungrammatical/gibberish’ (or rarely observed) ones? \n",
    "Remember that entropy is the average number of bits to encode the information contained in a random variable, so the exponentiation of the entropy (perplexity, $e^{H}$) should be the total amount of all possible information, or more precisely, the weighted average number of choices a random variable has. We evaluate our prediction Q by testing against samples drawn from P: $PPL = e^{CrossEntropy}$.\n",
    "\n",
    "Measure perplexity on an unseen (test) corpus, generally we compare a range of models using this score. The best LM is the one that generates the lowest perplexity on the test corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(loss_per_token):\n",
    "    return math.exp(loss_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizer, criterion, N_EPOCHS=10, is_lstm=False, force_stop=False):\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_items = 0\n",
    "        \n",
    "        # The `1` is the number of layers * number of directions.\n",
    "        # i.e. we have 1 layer and we are moving in 1 direction\n",
    "        # More info: https://pytorch.org/docs/stable/nn.html#rnn\n",
    "        prev_hidden = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "        if is_lstm:\n",
    "            prev_ht = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "            prev_ct = torch.zeros(1, BATCH_SIZE, HIDDEN_DIM, device=device)\n",
    "            prev_hidden = (prev_ht, prev_ct)\n",
    "\n",
    "        \n",
    "        # `batch` is a tuple of Tensors: (TEXT, TARGET)\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            \n",
    "            if force_stop:\n",
    "                print(\"Currently processing train batch {} of {}\".format(i, len(train_iterator)))\n",
    "                if i % 7 == 0 and i != 0:\n",
    "                    break\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ## text=\n",
    "            ## targets\n",
    "            # shape(text) = [T, B]\n",
    "            # shape(target) = [T, B]\n",
    "            \n",
    "            # We reshape text and target to [B, T]. \n",
    "            text = text.T\n",
    "            targets = targets.T\n",
    "            # shape(text) = [B, T]\n",
    "            # shape(target) = [B, T]\n",
    "            \n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # Otherwise the model would backpropagate all the way to beginning of the dataset.\n",
    "            prev_hidden = save_hidden(prev_hidden)\n",
    "            \n",
    "            ##logits, prev_hidden =\n",
    "            \n",
    "            # Compute the loss\n",
    "            # We reshape inputs to eliminate batching\n",
    "            loss = criterion(logits.view(-1, OUTPUT_DIM), targets.reshape(-1))\n",
    "        \n",
    "            # backprop the average loss and update parameters\n",
    "            # Why average loss?\n",
    "            loss.mean().backward()\n",
    "        \n",
    "            # update the parameters using the gradients and optimizer algorithm \n",
    "            ##call the optimizer\n",
    "            \n",
    "            \n",
    "            epoch_loss += loss.detach().sum()\n",
    "            epoch_items += loss.numel()\n",
    "        \n",
    "        # We compute loss per token for an epoch\n",
    "        train_loss_per_token = epoch_loss / epoch_items\n",
    "        # We compute perplexity\n",
    "        train_ppl = perplexity(train_loss_per_token)\n",
    "\n",
    "        end_time = time.time()        \n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        valid_loss_per_token, valid_ppl = evaluate(model, \n",
    "                                                   valid_iterator, \n",
    "                                                   criterion,\n",
    "                                                   is_lstm=is_lstm,\n",
    "                                                   force_stop=force_stop)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss_per_token:.3f} | Train Perplexity: {train_ppl:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss_per_token:.3f} |  Val. Perplexity: {valid_ppl:.3f}')\n",
    "        \n",
    "        if force_stop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, is_lstm=False, force_stop=False):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_items = 0\n",
    "    \n",
    "    # we initialise the first hidden state with zeros\n",
    "    ## Initialise the previous hidden states of the RNNs\n",
    "    ##prev_hidden = \n",
    "    if is_lstm:\n",
    "        ##prev_ht =\n",
    "        ##prev_ct = \n",
    "        ##prev_hidden = \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            if force_stop and i % 3 == 0 and i != 0:\n",
    "                print(\"Currently processing valid batch {} of {}\".format(i, len(train_iterator)))\n",
    "                break\n",
    "\n",
    "            text, target = batch.text, batch.target\n",
    "            text, target = text.T, target.T\n",
    "            logits, prev_hidden = model(text, prev_hidden)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = criterion(logits.view(-1, OUTPUT_DIM), target.reshape(-1))\n",
    "\n",
    "            prev_hidden = save_hidden(prev_hidden)\n",
    "\n",
    "            epoch_loss += loss.detach().sum()\n",
    "            epoch_items += loss.numel()\n",
    "\n",
    "        loss_per_token = epoch_loss / epoch_items\n",
    "        ppl = math.exp(loss_per_token)\n",
    "            \n",
    "        \n",
    "    return loss_per_token, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(rnn_model, train_iterator, valid_iterator, optimizer, criterion, force_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Long short-term memory architectures LSTMs vs. RNNs\n",
    "\n",
    "Remember the vanishing/exploding gradient problem? The gradient signal gets smaller and smaller as it backpropagates further. It is caused by the repeated use of the recurrent weight matrix in RNN. Gradient can be viewed as a measure of the effect of the past on the future. If the gradient becomes vanishingly small over longer distances we can not capture the dependency to the past correctly. For example: \"A patient with a rare sarcoma of soft tissue on the left thigh was presented to the hospital yesterday.\" \"was presented\" depends on \"a patient\", but they are separated by 11 words!\n",
    "\n",
    "![full_lstm](images/lstm_full.png)\n",
    "\n",
    "The key to LSTMs is the cell state $c_t$. It runs straight down the entire chain and allow the information to just flow along it unchanged. LSTM has two \"hidden states\": $c_t$  and $h_t$ . You can think of $c_t$  is the \"internal\" hidden state that retains important information for longer timesteps, whereas $h_t$ is the \"external\" hidden state that exposes that information to the outside world.\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.  An LSTM has three of these gates.\n",
    "\n",
    "Forget gate decides what information we’re going to throw away from the cell state. \n",
    "\n",
    "$f_t = \\sigma(W_{if}x_t + W_{hf}h_{t-1}+b_f)$\n",
    "\n",
    "$\\sigma$ squashes input values between 0 and 1, describing how much of each component should be let through. Zero means \"let nothing through\", while a value of one means \"let everything through\".\n",
    "\n",
    "![lstm_ft](images/lstm_ft.png)\n",
    "\n",
    "Input gate decides what new information we are going to store in the cell state. \n",
    "\n",
    "$i_t = \\sigma(W_{ii}x_t + W_{hi}h_{t-1}+b_i)$\n",
    "\n",
    "Next, a tanh layer creates a vector of new candidate values, $g_t$, that could be added to the state. tanh squashes the output values to be between −1 and 1. \n",
    "\n",
    "$g_t = tanh(W_{ig}x_t + W_{hg}h_{t-1}+b_g)$ (this equation equal to vanilla RNN if we remove gates)\n",
    "\n",
    "The next step combines these two to create an update to the state. Pointwise multiplication operation (*) decides on the parts we output.\n",
    "\n",
    "$c_t = f_t * c_{t-1} + i_t * g_t$\n",
    "\n",
    "![lstm_it_cand](images/lstm_it_cand.png)\n",
    "\n",
    "Finally, the output gate decides how much information goes to the output:\n",
    "\n",
    "$o_t = \\sigma(W_{io}x_t + W_{ho}h_{t-1}+b_o)$\n",
    "\n",
    "$h_t = o_t * tanh(c_t)$\n",
    "\n",
    "![lstm_ot](images/lstm_ot.png)\n",
    "\n",
    "\n",
    "Q: How does this help with the vanishing gradient problem ?\n",
    "\n",
    "Whereas the RNN computes the new hidden state from scratch based on the previous hidden state and the input, the LSTM computes the new hidden state by choosing what to add to the current state. This allows skipping multiplicative gradient paths. Multiple additive paths are hard to converge to ~0.\n",
    "\n",
    "Q: How are language models trained ?\n",
    "\n",
    "Very often the so-called \"teacher forcing\" is used. It works by using the actual ground true outputs from the training dataset at the current time step t as input in the next time step t+1, rather than the output generated by the network. This makes learning faster and the model more stable. The model is not going to get punished for every subsequent word it generates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manual_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.forget_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_size+input_size, hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        ##self.input_gate =\n",
    "        ##self.candidate_gate = \n",
    "        ##self.output_gate = \n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "\n",
    "        # shape(x) = [B, T, input_size]\n",
    "        # shape(prev_hidden) = ([1, B, hidden_size], [1, B, hidden_size]) where 1 = num_layers * num_directions\n",
    "\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        \n",
    "        # At t=0, h_t and c_t will be initialized to a vector of 0s\n",
    "        h_t = prev_hidden[0].squeeze(0)\n",
    "        c_t = prev_hidden[1].squeeze(0)\n",
    "\n",
    "        hidden_states = torch.zeros(batch_size, sequence_length, self.hidden_size).to(device)\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            # shape(x_t) = [B, input_size]\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            # shape(concat_h_x) = [B, hidden_size+input_size]\n",
    "            ##concat_h_x = \n",
    "\n",
    "            # shape(f_t) = [B, hidden_size]\n",
    "            f_t = self.forget_gate(concat_h_x)\n",
    "\n",
    "            # shape(c_prime_t) = [B, hidden_size]\n",
    "            ##c_prime_t =\n",
    "\n",
    "            # shape(i_t) = [B, hidden_size]\n",
    "            # shape(cand_t) = [B, hidden_size]\n",
    "            i_t = self.input_gate(concat_h_x)\n",
    "            ##cand_t = \n",
    "\n",
    "            # shape(c_t) = [B, hidden_size]\n",
    "            ##c_t =\n",
    "\n",
    "            # shape(o_t) = [B, hidden_size]\n",
    "            # shape(h_t) = [B, hidden_size]\n",
    "            ##o_t =\n",
    "            ##h_t =\n",
    "\n",
    "            hidden_states[:, t, :] = h_t\n",
    "\n",
    "        h_t, c_t = h_t.unsqueeze(0), c_t.unsqueeze(0)\n",
    "        return hidden_states, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_lstm = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            variant=\"manual_lstm\")\n",
    "\n",
    "lstm = RNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM,\n",
    "            DROPOUT, \n",
    "            PAD_IDX,\n",
    "            variant=\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(manual_lstm.parameters())\n",
    "train(manual_lstm, train_iterator, valid_iterator, optimizer, criterion, is_lstm=True, force_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(lstm.parameters())\n",
    "train(lstm, train_iterator, valid_iterator, optimizer, criterion, is_lstm=True, force_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the RNN, Manual LSTM and LSTM fare up against each other?\n",
    "\n",
    "![vanilla_rnn_lm](images/vanilla_rnn_lm.png)\n",
    "![manual_lstm_lm](images/manual_lstm_lm.png)\n",
    "![lstm_lm](images/lstm_lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs vs. GRUs\n",
    "\n",
    "Gated Recurrent Unit (GRU) combines the forget and input gates into a single \"update gate\" (z). So we have only two gates: update and reset. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models. Candidate state $g_t$ is able to suppress $h_t$. The final state is a convex combination: of the $g_t$ and $h_{t-1}$ with coefficients of $(1 - z_t)$ and $z_t$ respectively.\n",
    "\n",
    "$r_t = \\sigma(W_{ir}x_t + W_{hr}h_{t-1}+b_r)$\n",
    "\n",
    "$z_t = \\sigma(W_{iz}x_t + W_{hz}h_{t-1}+b_z)$\n",
    "\n",
    "$g_t = tanh(W_{ig}x_t + r_t * (W_{hg}h_{t-1}+b_g))$\n",
    "\n",
    "$h_t = (1 - z_t)* g_t + z_t * h_{t-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.gru = nn.GRU(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence model\n",
    "https://arxiv.org/abs/1409.3215 \\\n",
    "So far we have encountered some classification tasks where the inputs are of variable length. We use Recurrent Neural Networks (RNN/LSTM/GRU) to do predictions. However, when it comes to text generation, the length of outputs might also be random. In this case, we use a sequence-to-sequence model. \\\n",
    "\n",
    "![seq2seq](images/seq2seq.png)\n",
    "\n",
    "A sequence-to-sequence (seq2seq) model is a model that consists of two components called **Encoder** and **Decoder**. Commonly, two recurrent neural networks are used as the encoder and the decoder. The input is fed into the encoder RNN token by token, producing a fix-lengthed vector (the final hidden state) that encodes the context of all input sequence. We refer to this vector as the **context vector**. The decoder uses this context vector as the initialization of its first hidden state and inits the input with the $<sos>$ token, generating the outputs token by token.\n",
    "\n",
    "Seq2seq model is often used in NLP tasks where the lengths of both input and output are not fixed, e.g. machine translation, dialogue system. In the following part, we are going to build a vanilla seq2seq model with LSTM as encoder/decoder module on the machine translation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html \\\n",
    "https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential libraries\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are running a machine translation model on an actual dataset: Multi30k. Multi30k is a dataset for multi-modal machine translation. We'll only use the texts in this dataset and we load the dataset with *Torchtext*, which can help us with all the pre-processing and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# torchtext will pre-process the data, including tokenization, padding, stoi, etc.\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "# print the number of examples in train/valid/test sets\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocab of our training set, ignoring word with frequency less than 2\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train/valid/test iterators, which will batch the data for us\n",
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "x = vars(test_data.examples[0])['src']\n",
    "y = vars(test_data.examples[0])['trg']\n",
    "print(\"Source example:\", \" \".join(x))\n",
    "print(\"Target example:\", \" \".join(y))\n",
    "print(\"Padded target:\", TRG.pad([y]))\n",
    "print(\"Tensorized target:\", TRG.process([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Encoder\n",
    "We have three layers in the encoder: an embedding layer (with dropout), a RNN layer, and a linear layer. As we have known from the word representation session, we can apply a embedding layer and distributed word representation is trained jointly with the model. \n",
    "\n",
    "If we want to have a bidirectional encoder to encode both forward and backward contexts in the input, the hidden dimension of the RNN layer is doubled. Therefore, the linear layer is here to keep the same dimensionality between the encoder output and decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, layers, PAD_IDX=1, bidirectional=False, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.PAD_IDX = PAD_IDX\n",
    "        \n",
    "        \n",
    "        # If we use a bidirectional encoder to encode both forward and backward context,\n",
    "        # the dimension of the hidden state will double\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if bidirectional:\n",
    "            ff_input_dim = 2 * hidden_dim\n",
    "        else:\n",
    "            ff_input_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, self.hidden_dim, layers, dropout=dropout, \\\n",
    "                           bidirectional=bidirectional, bias=False)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(ff_input_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # shape(x) = [T, B]\n",
    "\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        # shape(x) = [T, B, E]\n",
    "\n",
    "        outputs, (h_n, c_n) = self.rnn(x)\n",
    "        # shape(outputs) = [T, B, D*num_directions]\n",
    "          # if we used the `batch_first` flag, shape(outputs) would be [B, T, D*num_directions]\n",
    "        # shape(h_n)=shape(c_n) = [num_layers*num_directions, B, D]\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # concatenate the forward and backward hidden states\n",
    "            h_n = torch.cat((h_n[0::2,:,:], h_n[1::2,:,:]), dim = -1)\n",
    "            c_n = torch.cat((c_n[0::2,:,:], c_n[1::2,:,:]), dim = -1)\n",
    "        \n",
    "        h_n = self.ff(h_n)\n",
    "        c_n = self.ff(c_n)\n",
    "        \n",
    "        return outputs, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder has four layers: an embedding layer (with dropout), a unidirectional RNN layer and two linear layers. The decoder is always unidirectional in that we only generate the outputs from left to right. \n",
    "\n",
    "Remember that the embedding layer is actually a matrix of all word vectors, therefore it is the same as a linear layer. Assuming we have the same dimensionality for the embedding layer and the output linear layer, we can perform a weight tying by sharing all the parameters in the two layers. This step would reduce the model size and might improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, layers, weight_tying=True, PAD_IDX=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        '''\n",
    "        TODO define the embedding layer, rnn layer and the two linear layers, with correct dimensions\n",
    "        '''\n",
    "        self.embedding = nn.Embedding()\n",
    "        self.rnn = nn.LSTM() # we don't set bidirectional here\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # This linear layer is to ensure the output layer has the same dimensionality with the embedding layer\n",
    "        self.out = nn.Linear()\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        TODO apply weight tying by sharing the weights of the embedding and output layer\n",
    "        '''\n",
    "        # share the weights for embedding layer and output layer\n",
    "        if weight_tying:\n",
    "            self.embedding.weight =\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        # shape(x) = [B]\n",
    "\n",
    "        # we expand the dim of sequence length\n",
    "        x = x.unsqueeze(0)\n",
    "        # shape(x) = [1, B]\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        TODO feed the input to the embedding layer and dropout layer\n",
    "        '''\n",
    "        embed = \n",
    "        # shape(embed) = [1, B, E]\n",
    "\n",
    "        \n",
    "        '''\n",
    "        TODO feed the word vector to the RNN layer, initializing hidden state with the encoder hidden state\n",
    "        '''\n",
    "        output, hidden = \n",
    "        # shape(output) = [1, B, D]\n",
    "        # shape(h_n)=shape(c_n) = [num_layers*num_directions, B, D]\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        TODO pass output through the two linear layers\n",
    "        '''\n",
    "        ff_out = \n",
    "        # shape(ff_out) = [1, B, E]\n",
    "        ##sqeeuze the output:\n",
    "        ff_out =\n",
    "        # shape(ff_out) = [B, E]\n",
    "        \n",
    "        logits = \n",
    "        # shape(logits) = [B, O]\n",
    "        return logits, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device='cpu'):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    # src: [seq_len, batch_size]\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size, output_dim).to(self.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # initialize output sequence with '<sos>'\n",
    "        dec_output = trg[0,:]\n",
    "        \n",
    "        # decoder token by token\n",
    "        for t in range(1, max_len):\n",
    "            dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "            outputs[t] = dec_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            pred_next = dec_output.argmax(1)\n",
    "            \n",
    "            '''\n",
    "            TODO use the ground truth token if using teacher forcing\n",
    "            '''\n",
    "            dec_output = \n",
    "        return outputs\n",
    "\n",
    "    # greedy search for actual translation\n",
    "    def greedy_search(self, src, sos_idx, max_len=50):\n",
    "        src = src.to(self.device)\n",
    "        batch_size = src.shape[1]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size).to(self.device)\n",
    "        \n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        \n",
    "        dec_output = torch.zeros(batch_size, dtype=torch.int64).to(device)\n",
    "        dec_output.fill_(sos_idx)\n",
    "        \n",
    "        outputs[0] = dec_output\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "            \n",
    "            dec_output = dec_output.argmax(1)\n",
    "\n",
    "            outputs[t] = dec_output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished our seq2seq model, let's build a toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=4\n",
    "OUTPUT_DIM=4\n",
    "EMB_DIM=10\n",
    "HIDDEN_DIM=6\n",
    "LAYERS=2\n",
    "\n",
    "# define the encoder and decoder, and build the model\n",
    "'''\n",
    "TODO define an encoder and a decoder, and then define the model\n",
    "'''\n",
    "enc = \n",
    "dec = \n",
    "model = \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, optimizer and criterion\n",
    "\n",
    "This is our model hyperparameters. In actual training, we might need to tune the hyperparameters on the validation set before evaluating on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM=256\n",
    "HIDDEN_DIM=512\n",
    "LAYERS=1\n",
    "DROPOUT=0.5\n",
    "BIDIRECTIONAL=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO get indexes of the padding tokens in source and target vocabulary\n",
    "'''\n",
    "# padding token\n",
    "SRC_PAD = \n",
    "TRG_PAD = \n",
    "\n",
    "# build model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=SRC_PAD, bidirectional=BIDIRECTIONAL, dropout=DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=TRG_PAD, dropout=DROPOUT)\n",
    "model = Seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer will update the gradient everytime we back-propagate. We are using Adam as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.001\n",
    "# set optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *CrossEntropyLoss* as our loss function, which will calculate the log softmax and the negative log-likelihood. We pass the padding token in the target vocab to the criterion so that it will ignore the loss for this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our training loop.\n",
    "1. We iterate over the training iterator and get a batch of training examples\n",
    "2. The input is passed through the model and it returns the predictions\n",
    "3. We calculate the loss between the model predictions and the ground truths\n",
    "4. We back-propagate the loss and the optimizer will update the gradients\n",
    "\n",
    "To avoid exploding gradient, we clip the gradients to a maximum value every training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip, num_epoch):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        src, trg = src.T, trg.T\n",
    "\n",
    "        # set gradients to zero to avoid accumulating the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(src, trg)\n",
    "        \n",
    "        # exclude <sos> token\n",
    "        # outputs: (seq_len * batch_size, output_dim)\n",
    "        # trg : (seq_len * batch_size)\n",
    "        outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(outputs, trg)\n",
    "        \n",
    "        writer.add_scalar('training loss',\n",
    "                            loss.item(),\n",
    "                            num_epoch * len(iterator) + i)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Batch:\\t {0} / {1},\\t loss: {2:2.3f}'.format(i, len(iterator), loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        # clip grad to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluating loop is similar to the training loop, except that we don't want to do back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, criterion):\n",
    "    # In eval model, layers such as Dropout, BatchNorm will work in eval model\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    # this prevents the back-propagation\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            # during test time, we have no correct trg so we turn off teacher forcing\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(outputs, trg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, converting a batch of tensors to the text form\n",
    "def get_text_from_tensor(tensor, field, eos='<eos>'):\n",
    "    batch_output = []\n",
    "    for i in range(tensor.shape[1]):\n",
    "        sequence = tensor[:,i]\n",
    "        words = []\n",
    "        for tok_idx in sequence:\n",
    "            tok_idx = int(tok_idx)\n",
    "            token = field.vocab.itos[tok_idx]\n",
    "\n",
    "            if token == '<sos>':\n",
    "                continue\n",
    "            elif token == '<eos>' or token == '<pad>':\n",
    "                break\n",
    "            else:\n",
    "                words.append(token)\n",
    "        words = \" \".join(words)\n",
    "        batch_output.append(words)\n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score\n",
    "\n",
    "An automatic metric that assumes that the closer a machine translation is to a professional human translation, the better it is. Rather strong assumption considering in how many different \"correct\" ways a sentence could be translated. For example, the French sentence \"\n",
    "Courage!\" could be translated into English as \"Cheer up!\", \"Go for it!\", \"Chin up!\", etc.\n",
    "BLEU computes N-gram matching between system output and one or more reference (human) translations. N-gram is simply a sequence of N words within a given window and when computing the N-grams you typically move one word forward. Typically values between of N between 1 and 5 are considered. According to the formula $m = N-n+1$, in the sentence \"I like chocolate and vanilla ice cream\" there are:\n",
    "\n",
    "- 7 unigrams (1-grams)\n",
    "- 6 bigrams (2-grams)\n",
    "- 5 trigrams (3-grams)\n",
    "- 4 quadrigrams (4-grams)\n",
    "\n",
    "BLEU rewards same words in equal order. It is the most widely used metric. The final score ranges from 0-100, the higher the score, the more the translation correlates to a human translation. BLEU computes geometirc mean and is a document-level metric: if a higher order n-gram precision (e.g., n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, even if some lower order n-grams are matched:\n",
    "\n",
    "$BLEU = brevity\\_penalty \\cdot exp(\\sum^N_{n=1}\\log modified\\_precision_n)$\n",
    "\n",
    "The brevity penalty penalizes short translations. The default configuration uses N = 4 and uniform weights.\n",
    "\n",
    "![bleu1](images/bleu1.png)\n",
    "![bleu2](images/bleu2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def test_bleu(model, iterator, trg_field):\n",
    "    model.eval()\n",
    "\n",
    "    ref = []\n",
    "    hyp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            outputs = model.greedy_search(src, trg_field.vocab.stoi['<sos>'])\n",
    "            \n",
    "            hyp += get_text_from_tensor(outputs, trg_field)\n",
    "            ref += get_text_from_tensor(trg, trg_field)\n",
    "            \n",
    "    # expand dim of reference list\n",
    "    # sys = ['translation_1', 'translation_2']\n",
    "    # ref = [['truth_1', 'truth_2'], ['another truth_1', 'another truth_2']]\n",
    "    ref = [ref]\n",
    "    return sacrebleu.corpus_bleu(hyp, ref, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start our training! We keep the checkpoint with the highest valid BLEU as our best checkpoint.\n",
    "\n",
    "**The training is heavily dependent on GPU, so it might take years to train on CPU. You may skip this block and load our pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 30\n",
    "CLIP = 1\n",
    "\n",
    "best_bleu = float('Inf')\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('Start training Epoch {}:'.format(i+1))\n",
    "    \n",
    "    '''\n",
    "    TODO calculate training loss and valid loss using the function we defined above\n",
    "    '''\n",
    "    train_loss = \n",
    "    valid_loss = \n",
    "    bleu = test_bleu(model, test_iterator, TRG)\n",
    "    \n",
    "    writer.add_scalar('valid loss',\n",
    "                valid_loss,\n",
    "                i)\n",
    "    writer.add_scalar('valid ppl',\n",
    "                      math.exp(valid_loss),\n",
    "                     i)\n",
    "    writer.add_scalar('valid BLEU',\n",
    "                bleu.score,\n",
    "                i)\n",
    "    \n",
    "    if bleu.score > best_bleu:\n",
    "        best_bleu = bleu.score\n",
    "        torch.save(model.state_dict(), 'checkpoint_best-seq2seq.pt')\n",
    "    \n",
    "    print('Epoch {0} train loss: {1:.3f} | Train PPL: {2:7.3f}'.format(i+1, train_loss, math.exp(train_loss)))\n",
    "    print('Epoch {0} valid loss: {1:.3f} | Valid PPL: {2:7.3f}'.format(i+1, valid_loss, math.exp(valid_loss)))\n",
    "    print('Epoch {0} valid BLEU: {1:3.3f}'.format(i+1, bleu.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoint_best-seq2seq.pt'))\n",
    "\n",
    "'''\n",
    "TODO calculate bleu score on the test set\n",
    "'''\n",
    "test_bleu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
