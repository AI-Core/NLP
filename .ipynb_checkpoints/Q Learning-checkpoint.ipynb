{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "### Monte-Carlo (MC) methods for computing action-value functions and improving policies\n",
    "\n",
    "\n",
    "Monte-Carlo methods are those based on repeated sampling to estimate a quantity. \n",
    "\n",
    "Similarly to what we did using DP, we can do MC based implementations of policy and value iteration by sampling experienced values of states, rather than simulating them with a model.\n",
    "\n",
    "The goal of using MC methods is to avoing the need for a model - if we don't have to look ahead from each state, then we can remove the model.\n",
    "\n",
    "However, even if we find an optimal value function, we will still need to use a model to extract an optimal policy to understand what states are reachable from others. It's no good having a chess piece next you your opponent's King if you don't know how that piece can move.\n",
    "\n",
    "The way we chose actions greedily when we had a model, was by using it to consider all possible actions and then taking the one that gave us the best expected return.\n",
    "\n",
    "As such, what would be more useful would be to use MC methods to estimate the action-value (Q) function. This tells us how good any particular action is from a certain state. If we know how good each action is, then we don't need a model - as long as we know all possible actions, we can just plug them into our q function and then take the one which for which our q function returns the largest value.\n",
    "\n",
    "### Q-functions - how good is taking a certain action in a certain state?\n",
    "![](images/q_def.jpg)\n",
    "\n",
    "![](images/q_optimality_derivation.JPG)\n",
    "\n",
    "### Computing action-value functions using Monte-Carlo methods\n",
    "\n",
    "For this method, we will use Monte-Carlo sampling to estimate the q-value of each state by running many episodes, and then doing backup from the terminal state.\n",
    "\n",
    "The q-value is the <strong>mean</strong> expected future reward following an action from a given state.\n",
    "Rather than storing all of our experience and taking the mean over them, we can use each experience to update an exponentially weighted average forget that exprience.\n",
    "\n",
    "![](./images/exp-avg.jpg)\n",
    "\n",
    "![](images/q_learning_algorithm.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from GriddyEnv import GriddyEnv\n",
    "\n",
    "env = GriddyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_agent(policy, n=5):\n",
    "    try:\n",
    "        for trial_i in range(n):\n",
    "            observation = env.reset()\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                env.render()\n",
    "                policy_action = policy(observation)\n",
    "                observation, reward, done, info = env.step(policy_action)\n",
    "                time.sleep(0.5)\n",
    "                t+=1\n",
    "            env.render()\n",
    "            time.sleep(1.5)\n",
    "            print(\"Episode {} finished after {} timesteps\".format(trial_i, t))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 1 timesteps\n",
      "Episode 1 finished after 190 timesteps\n",
      "Episode 2 finished after 44 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(random_policy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, return_action_val=False):\n",
    "    action_values=[] #store the value of each action from this state\n",
    "    for test_action in range(4): #for each action\n",
    "        key = pickle.dumps(np.array((*np.copy(state).flatten(), test_action))) #calculate the key for our dictionary\n",
    "        if key not in q_table: q_table[key] = 0 #if unseen state-action pair, initialize q value to 0\n",
    "        action_values.append(q_table[key]) #append the value of this action to a list\n",
    "    policy_action = np.argmax(action_values) #get an action by performing argmax operation\n",
    "    if return_action_val: return policy_action, action_values[policy_action] #if flag, return value of action aswell\n",
    "    return policy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epsilon_greedy_policy(policy):\n",
    "    def epsilon_greedy_policy(state):\n",
    "        action = env.action_space.sample() if np.random.rand()<epsilon else policy(state) #epsilon greedy policy\n",
    "        return action\n",
    "    return epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(episode_mem, q_table, discount_factor=0.95, alpha=0.2):\n",
    "    all_diffs=[] #store difference between new and old q values\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #iterate over the memories in reverse chronological order\n",
    "        if i==len(episode_mem)-1: #if terminal state\n",
    "            calculated_q = mem[\"reward\"] #set q = the reward in that memory\n",
    "#             print(\"TERMINAL\")\n",
    "        else:#if non-terminal state\n",
    "            _, next_obs_q = greedy_policy(mem[\"new_observation\"], True) #get q value of next state\n",
    "            calculated_q = mem[\"reward\"] + discount_factor*next_obs_q #calculate new q value estimate for this state-action pair\n",
    "        \n",
    "        key = pickle.dumps(np.array((*mem['observation'].flatten(), mem['action']))) #get key of current state-action pair\n",
    "        if key not in q_table: q_table[key]=0 #if unseen state-action pair, initialize q value to 0\n",
    "        \n",
    "        new_val = q_table[key] + alpha*(calculated_q-q_table[key]) #update q with a step of size alpha to new q value\n",
    "        diff =  abs(q_table[key] - new_val) #calculate difference between old and new q values estimate\n",
    "        q_table[key] = new_val\n",
    "        all_diffs.append(diff)\n",
    "    return q_table, np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_episode=0\n",
    "epsilon = 1 #initialize epsilon\n",
    "q_table = {} #initialize q table\n",
    "discount_factor=0.95\n",
    "alpha=0.1\n",
    "\n",
    "env = GriddyEnv()\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global q_table\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*=0.97 #decay epsilon\n",
    "            q_table, q_delta = update_q_table(episode_mem, q_table, alpha=alpha) #update our q table using the current episode memory\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={}. Q_Delta={}\".format(i_episode, t, epsilon, q_delta))#, end='\\r')\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 116 timesteps. Epsilon=0.97. Q_Delta=0.001256176552026234\n",
      "Episode 2 finished after 14 timesteps. Epsilon=0.9409. Q_Delta=0.011155691926136855\n",
      "Episode 3 finished after 4 timesteps. Epsilon=0.912673. Q_Delta=0.02782124762014797\n",
      "Episode 4 finished after 24 timesteps. Epsilon=0.8852928099999999. Q_Delta=0.005936725821414135\n",
      "Episode 5 finished after 12 timesteps. Epsilon=0.8587340256999999. Q_Delta=0.00847951082756136\n",
      "Episode 6 finished after 12 timesteps. Epsilon=0.8329720049289999. Q_Delta=0.010122369842371295\n",
      "Episode 7 finished after 15 timesteps. Epsilon=0.8079828447811299. Q_Delta=0.014483423692065999\n",
      "Episode 8 finished after 1 timesteps. Epsilon=0.783743359437696. Q_Delta=0.09\n",
      "Episode 9 finished after 19 timesteps. Epsilon=0.7602310586545651. Q_Delta=0.016744741042380017\n",
      "Episode 10 finished after 14 timesteps. Epsilon=0.7374241268949281. Q_Delta=0.0072452075274847426\n",
      "Episode 11 finished after 8 timesteps. Epsilon=0.7153014030880802. Q_Delta=0.01363075596519716\n",
      "Episode 12 finished after 2 timesteps. Epsilon=0.6938423609954378. Q_Delta=0.044121267000000006\n",
      "Episode 13 finished after 4 timesteps. Epsilon=0.6730270901655747. Q_Delta=0.02904932372625001\n",
      "Episode 14 finished after 10 timesteps. Epsilon=0.6528362774606075. Q_Delta=0.012123444012000457\n",
      "Episode 15 finished after 3 timesteps. Epsilon=0.6332511891367892. Q_Delta=0.0324965629706192\n",
      "Episode 16 finished after 12 timesteps. Epsilon=0.6142536534626856. Q_Delta=0.007096472388057346\n",
      "Episode 17 finished after 2 timesteps. Epsilon=0.595826043858805. Q_Delta=0.034320755025000024\n",
      "Episode 18 finished after 14 timesteps. Epsilon=0.5779512625430409. Q_Delta=0.007367449542728857\n",
      "Episode 19 finished after 3 timesteps. Epsilon=0.5606127246667496. Q_Delta=0.023218487348333317\n",
      "Episode 20 finished after 3 timesteps. Epsilon=0.5437943429267471. Q_Delta=0.02140678257435\n",
      "Episode 21 finished after 1 timesteps. Epsilon=0.5274805126389447. Q_Delta=0.03486784400999998\n",
      "Episode 22 finished after 13 timesteps. Epsilon=0.5116560972597763. Q_Delta=0.009470037943351628\n",
      "Episode 23 finished after 3 timesteps. Epsilon=0.496306414341983. Q_Delta=0.01670675718960001\n",
      "Episode 24 finished after 21 timesteps. Epsilon=0.48141722191172354. Q_Delta=0.0075100732309840695\n",
      "Episode 25 finished after 4 timesteps. Epsilon=0.4669747052543718. Q_Delta=0.011435920269128009\n",
      "Episode 26 finished after 5 timesteps. Epsilon=0.4529654640967406. Q_Delta=0.01101091599736812\n",
      "Episode 27 finished after 16 timesteps. Epsilon=0.4393765001738384. Q_Delta=0.005744226245337248\n",
      "Episode 28 finished after 2 timesteps. Epsilon=0.42619520516862325. Q_Delta=0.041935403250747\n",
      "Episode 29 finished after 2 timesteps. Epsilon=0.4134093490135645. Q_Delta=0.01748246386993149\n",
      "Episode 30 finished after 6 timesteps. Epsilon=0.4010070685431576. Q_Delta=0.007194415331219521\n",
      "Episode 31 finished after 12 timesteps. Epsilon=0.38897685648686287. Q_Delta=0.004191046416921338\n",
      "Episode 32 finished after 2 timesteps. Epsilon=0.377307550792257. Q_Delta=0.012744716161180086\n",
      "Episode 33 finished after 6 timesteps. Epsilon=0.36598832426848926. Q_Delta=0.012784552414365923\n",
      "Episode 34 finished after 2 timesteps. Epsilon=0.35500867454043455. Q_Delta=0.01367471078550124\n",
      "Episode 35 finished after 4 timesteps. Epsilon=0.3443584143042215. Q_Delta=0.007968132410192222\n",
      "Episode 36 finished after 1 timesteps. Epsilon=0.3340276618750948. Q_Delta=0.00984770902183607\n",
      "Episode 37 finished after 7 timesteps. Epsilon=0.32400683201884195. Q_Delta=0.004002669934851406\n",
      "Episode 38 finished after 6 timesteps. Epsilon=0.3142866270582767. Q_Delta=0.010520432868910556\n",
      "Episode 39 finished after 1 timesteps. Epsilon=0.3048580282465284. Q_Delta=0.047829690000000036\n",
      "Episode 40 finished after 3 timesteps. Epsilon=0.2957122873991326. Q_Delta=0.009368227411989833\n",
      "Episode 41 finished after 4 timesteps. Epsilon=0.2868409187771586. Q_Delta=0.009044918518487377\n",
      "Episode 42 finished after 5 timesteps. Epsilon=0.27823569121384384. Q_Delta=0.007943688925147883\n",
      "Episode 43 finished after 2 timesteps. Epsilon=0.2698886204774285. Q_Delta=0.007600393757660556\n",
      "Episode 44 finished after 3 timesteps. Epsilon=0.26179196186310566. Q_Delta=0.01706636933337598\n",
      "Episode 45 finished after 4 timesteps. Epsilon=0.2539382030072125. Q_Delta=0.013264377591186935\n",
      "Episode 46 finished after 1 timesteps. Epsilon=0.2463200569169961. Q_Delta=0.03486784400999998\n",
      "Episode 47 finished after 4 timesteps. Epsilon=0.23893045520948622. Q_Delta=0.004920398612040932\n",
      "Episode 48 finished after 3 timesteps. Epsilon=0.23176254155320164. Q_Delta=0.005357461626605636\n",
      "Episode 49 finished after 2 timesteps. Epsilon=0.2248096653066056. Q_Delta=0.0063928921556560825\n",
      "Episode 50 finished after 7 timesteps. Epsilon=0.21806537534740741. Q_Delta=0.0028385734058194796\n"
     ]
    }
   ],
   "source": [
    "train(epsilon_greedy_policy, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 5 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning\n",
    "\n",
    "Instead of using a table to store our q values for each state, which becomes computationally inefficient when we have a large state space, we can use a neural network. This is not a problem for NNs as we don't need to store the value for each action-state pair explicity. We also improve the performance of our agent in unseen states. This is because, in the tabular method, we assume a q value of 0 for unseen action-state pairs while our network will make a guess based on similar state-action pairs it has seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(4*4*3, 32) #input layer\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 4) #output layer\n",
    "    def forward(self, obs):\n",
    "        obs = obs.view(-1, 4*4*3) #flatten input\n",
    "        x = F.relu(self.fc1(obs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def create_optimizer(self, lr=0.001):\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(q_network):\n",
    "    def greedy_policy(state, return_action_val=False):\n",
    "        action_values = q_network(torch.tensor(state).double()).detach().numpy()\n",
    "        policy_action = np.argmax(action_values)\n",
    "        if return_action_val: return policy_action, action_values[0][policy_action]\n",
    "        return policy_action\n",
    "    return greedy_policy\n",
    "\n",
    "def create_stochastic_policy(q_network):\n",
    "    def stochastic_policy(state, return_action_val=False):\n",
    "        action_values = q_network(torch.tensor(state).double()).detach().numpy()\n",
    "        action_probs = F.softmax(torch.tensor(action_values), dim=-1)\n",
    "        policy_action = torch.distributions.Categorical(action_probs).sample().item()\n",
    "        if return_action_val: return policy_action, action_values[0][policy_action]\n",
    "        return policy_action\n",
    "    return stochastic_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_network(episode_mem, q_network, discount_factor=0.95, alpha=0.2):\n",
    "    all_diffs=[] #store difference between new and old q values\n",
    "    for i, mem in reversed(list(enumerate(episode_mem))): #iterate over the memories in reverse chronological order\n",
    "        if i==len(episode_mem)-1: #if terminal state\n",
    "            calculated_q = mem['reward'] #set q = the reward in that memory\n",
    "        else:#if non-terminal state\n",
    "            _, next_obs_q = greedy_policy(mem['new_observation'], return_action_val=True) #get q value of next state\n",
    "            calculated_q = mem['reward']+discount_factor*next_obs_q #calculate new q value estimate for this state-action pair\n",
    "\n",
    "        predicted_old_q = q_network(torch.DoubleTensor(mem[\"observation\"]))[0, mem[\"action\"]] #what does our network predict for the current state-value pair\n",
    "        label_new_q = predicted_old_q + alpha*(calculated_q - predicted_old_q) #what should our label be for the network given our new prediction of q\n",
    "#         print(\"PREDICTED_OLD_Q\", predicted_old_q)\n",
    "#         print(\"LABEL_NEW_Q\", label_new_q)\n",
    "        cost = F.mse_loss(predicted_old_q, label_new_q) #calculate cost\n",
    "        cost.backward() #calculate gradients\n",
    "        q_network.optimizer.step() #update weights\n",
    "        q_network.optimizer.zero_grad() #reset gradients\n",
    "        all_diffs.append(abs(label_new_q.item()-predicted_old_q.item()))\n",
    "    return np.mean(all_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPER-PARAMS\n",
    "epsilon = 1\n",
    "i_episode=0\n",
    "discount_factor=0.95\n",
    "alpha=0.1\n",
    "lr = 0.001\n",
    "\n",
    "env = GriddyEnv(4, 4)\n",
    "q_network = QNetwork().double()\n",
    "q_network.create_optimizer(lr)\n",
    "greedy_policy = create_greedy_policy(q_network)\n",
    "stochastic_policy = create_stochastic_policy(q_network)\n",
    "epsilon_greedy_policy = create_epsilon_greedy_policy(greedy_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep(policy, n_episodes=100):\n",
    "    global epsilon\n",
    "    global q_network\n",
    "    global i_episode\n",
    "    try:\n",
    "        for _ in range(n_episodes):\n",
    "            observation = env.reset()\n",
    "            episode_mem = []\n",
    "            done=False\n",
    "            t=0\n",
    "            while not done:\n",
    "                action = policy(observation)\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                episode_mem.append({'observation':observation,\n",
    "                                    'action':action,\n",
    "                                    'reward':reward,\n",
    "                                    'new_observation':new_observation,\n",
    "                                    'done':done})\n",
    "                observation=new_observation\n",
    "                t+=1\n",
    "            epsilon*=0.995\n",
    "            q_delta = update_q_network(episode_mem, q_network) #update our q network using the current episode memory\n",
    "            i_episode+=1\n",
    "            print(\"Episode {} finished after {} timesteps. Epsilon={}. Q_Delta={}\".format(i_episode, t, epsilon, q_delta))#, end='\\r')\n",
    "            #print(value_table_viz(value_table))\n",
    "            #print()\n",
    "            #env.render(value_table_viz(value_network, observation))\n",
    "        env.close()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 70 timesteps. Epsilon=0.995. Q_Delta=0.014152768416282987\n",
      "Episode 2 finished after 47 timesteps. Epsilon=0.990025. Q_Delta=0.010358751371657236\n",
      "Episode 3 finished after 69 timesteps. Epsilon=0.985074875. Q_Delta=0.00615819107064181\n",
      "Episode 4 finished after 28 timesteps. Epsilon=0.9801495006250001. Q_Delta=0.00960992080270559\n",
      "Episode 5 finished after 13 timesteps. Epsilon=0.9752487531218751. Q_Delta=0.014833565741811845\n",
      "Episode 6 finished after 58 timesteps. Epsilon=0.9703725093562657. Q_Delta=0.007878868259380513\n",
      "Episode 7 finished after 20 timesteps. Epsilon=0.9655206468094844. Q_Delta=0.011171537660923585\n",
      "Episode 8 finished after 17 timesteps. Epsilon=0.960693043575437. Q_Delta=0.009307370871833871\n",
      "Episode 9 finished after 28 timesteps. Epsilon=0.9558895783575597. Q_Delta=0.0076208309762662695\n",
      "Episode 10 finished after 1 timesteps. Epsilon=0.9511101304657719. Q_Delta=0.06207209938070046\n",
      "Episode 11 finished after 33 timesteps. Epsilon=0.946354579813443. Q_Delta=0.005710772001988218\n",
      "Episode 12 finished after 40 timesteps. Epsilon=0.9416228069143757. Q_Delta=0.006015755355566393\n",
      "Episode 13 finished after 29 timesteps. Epsilon=0.9369146928798039. Q_Delta=0.0066220441923900055\n",
      "Episode 14 finished after 22 timesteps. Epsilon=0.9322301194154049. Q_Delta=0.007715407323094442\n",
      "Episode 15 finished after 42 timesteps. Epsilon=0.9275689688183278. Q_Delta=0.004431246007261043\n",
      "Episode 16 finished after 33 timesteps. Epsilon=0.9229311239742362. Q_Delta=0.007445929016139153\n",
      "Episode 17 finished after 38 timesteps. Epsilon=0.918316468354365. Q_Delta=0.006637264391569653\n",
      "Episode 18 finished after 92 timesteps. Epsilon=0.9137248860125932. Q_Delta=0.004510539947375174\n",
      "Episode 19 finished after 47 timesteps. Epsilon=0.9091562615825302. Q_Delta=0.006017030287266301\n",
      "Episode 20 finished after 39 timesteps. Epsilon=0.9046104802746175. Q_Delta=0.007095754874698877\n",
      "Episode 21 finished after 34 timesteps. Epsilon=0.9000874278732445. Q_Delta=0.006767696194998507\n",
      "Episode 22 finished after 40 timesteps. Epsilon=0.8955869907338783. Q_Delta=0.004121157678694523\n",
      "Episode 23 finished after 39 timesteps. Epsilon=0.8911090557802088. Q_Delta=0.0055401009296921315\n",
      "Episode 24 finished after 1 timesteps. Epsilon=0.8866535105013078. Q_Delta=0.018672293245104665\n",
      "Episode 25 finished after 7 timesteps. Epsilon=0.8822202429488013. Q_Delta=0.003962401146190049\n",
      "Episode 26 finished after 38 timesteps. Epsilon=0.8778091417340573. Q_Delta=0.007129402220199039\n",
      "Episode 27 finished after 128 timesteps. Epsilon=0.8734200960253871. Q_Delta=0.005509808975473205\n",
      "Episode 28 finished after 40 timesteps. Epsilon=0.8690529955452602. Q_Delta=0.005293899743908012\n",
      "Episode 29 finished after 77 timesteps. Epsilon=0.8647077305675338. Q_Delta=0.005732092032973972\n",
      "Episode 30 finished after 5 timesteps. Epsilon=0.8603841919146962. Q_Delta=0.014474182742712305\n",
      "Episode 31 finished after 20 timesteps. Epsilon=0.8560822709551227. Q_Delta=0.0064825636991737825\n",
      "Episode 32 finished after 10 timesteps. Epsilon=0.851801859600347. Q_Delta=0.0052403733360295805\n",
      "Episode 33 finished after 10 timesteps. Epsilon=0.8475428503023453. Q_Delta=0.00723667821120455\n",
      "Episode 34 finished after 1 timesteps. Epsilon=0.8433051360508336. Q_Delta=0.005807993691457902\n",
      "Episode 35 finished after 7 timesteps. Epsilon=0.8390886103705794. Q_Delta=0.004245942462215784\n",
      "Episode 36 finished after 27 timesteps. Epsilon=0.8348931673187264. Q_Delta=0.005619107812551195\n",
      "Episode 37 finished after 24 timesteps. Epsilon=0.8307187014821328. Q_Delta=0.0034592431108593363\n",
      "Episode 38 finished after 6 timesteps. Epsilon=0.8265651079747222. Q_Delta=0.005763730588010163\n",
      "Episode 39 finished after 25 timesteps. Epsilon=0.8224322824348486. Q_Delta=0.005877519060949177\n",
      "Episode 40 finished after 23 timesteps. Epsilon=0.8183201210226743. Q_Delta=0.006699061702180989\n",
      "Episode 41 finished after 24 timesteps. Epsilon=0.8142285204175609. Q_Delta=0.0031399758590817902\n",
      "Episode 42 finished after 10 timesteps. Epsilon=0.810157377815473. Q_Delta=0.004759034282854268\n",
      "Episode 43 finished after 11 timesteps. Epsilon=0.8061065909263957. Q_Delta=0.0039269362061321285\n",
      "Episode 44 finished after 15 timesteps. Epsilon=0.8020760579717637. Q_Delta=0.004309333304582941\n",
      "Episode 45 finished after 6 timesteps. Epsilon=0.798065677681905. Q_Delta=0.0023279489567705016\n",
      "Episode 46 finished after 16 timesteps. Epsilon=0.7940753492934954. Q_Delta=0.003726538266690317\n",
      "Episode 47 finished after 32 timesteps. Epsilon=0.7901049725470279. Q_Delta=0.002507220114420304\n",
      "Episode 48 finished after 11 timesteps. Epsilon=0.7861544476842928. Q_Delta=0.003999564136953964\n",
      "Episode 49 finished after 77 timesteps. Epsilon=0.7822236754458713. Q_Delta=0.002493351081076918\n",
      "Episode 50 finished after 7 timesteps. Epsilon=0.778312557068642. Q_Delta=0.0059289954311848424\n",
      "Episode 51 finished after 13 timesteps. Epsilon=0.7744209942832988. Q_Delta=0.005206109227914492\n",
      "Episode 52 finished after 5 timesteps. Epsilon=0.7705488893118823. Q_Delta=0.004267717314955855\n",
      "Episode 53 finished after 80 timesteps. Epsilon=0.7666961448653229. Q_Delta=0.005196728789548485\n",
      "Episode 54 finished after 7 timesteps. Epsilon=0.7628626641409962. Q_Delta=0.010840723440455153\n",
      "Episode 55 finished after 78 timesteps. Epsilon=0.7590483508202912. Q_Delta=0.004565919373549952\n",
      "Episode 56 finished after 21 timesteps. Epsilon=0.7552531090661897. Q_Delta=0.0031331758382766744\n",
      "Episode 57 finished after 4 timesteps. Epsilon=0.7514768435208588. Q_Delta=0.004397551160185947\n",
      "Episode 58 finished after 6 timesteps. Epsilon=0.7477194593032545. Q_Delta=0.0019446927891866121\n",
      "Episode 59 finished after 9 timesteps. Epsilon=0.7439808620067382. Q_Delta=0.002802057228653096\n",
      "Episode 60 finished after 6 timesteps. Epsilon=0.7402609576967045. Q_Delta=0.003211979273328095\n",
      "Episode 61 finished after 11 timesteps. Epsilon=0.736559652908221. Q_Delta=0.002307710326301817\n",
      "Episode 62 finished after 2 timesteps. Epsilon=0.7328768546436799. Q_Delta=0.002711257226832353\n",
      "Episode 63 finished after 57 timesteps. Epsilon=0.7292124703704616. Q_Delta=0.0021538162781242006\n",
      "Episode 64 finished after 1 timesteps. Epsilon=0.7255664080186093. Q_Delta=0.004472997600986184\n",
      "Episode 65 finished after 6 timesteps. Epsilon=0.7219385759785162. Q_Delta=0.0020361881129346373\n",
      "Episode 66 finished after 8 timesteps. Epsilon=0.7183288830986236. Q_Delta=0.002627718340664706\n",
      "Episode 67 finished after 4 timesteps. Epsilon=0.7147372386831305. Q_Delta=0.0017947259812675642\n",
      "Episode 68 finished after 15 timesteps. Epsilon=0.7111635524897149. Q_Delta=0.001789505691494749\n",
      "Episode 69 finished after 2 timesteps. Epsilon=0.7076077347272662. Q_Delta=0.0026461027012845695\n",
      "Episode 70 finished after 15 timesteps. Epsilon=0.7040696960536299. Q_Delta=0.0026988744958905317\n",
      "Episode 71 finished after 6 timesteps. Epsilon=0.7005493475733617. Q_Delta=0.002567364016242598\n",
      "Episode 72 finished after 37 timesteps. Epsilon=0.697046600835495. Q_Delta=0.00213326537482237\n",
      "Episode 73 finished after 17 timesteps. Epsilon=0.6935613678313175. Q_Delta=0.0034975491653946736\n",
      "Episode 74 finished after 7 timesteps. Epsilon=0.6900935609921609. Q_Delta=0.0026512246244230525\n",
      "Episode 75 finished after 1 timesteps. Epsilon=0.6866430931872001. Q_Delta=0.0021430371931462355\n",
      "Episode 76 finished after 18 timesteps. Epsilon=0.6832098777212641. Q_Delta=0.0022625016147273277\n",
      "Episode 77 finished after 9 timesteps. Epsilon=0.6797938283326578. Q_Delta=0.0022416722958736005\n",
      "Episode 78 finished after 6 timesteps. Epsilon=0.6763948591909945. Q_Delta=0.0016643469439191667\n",
      "Episode 79 finished after 10 timesteps. Epsilon=0.6730128848950395. Q_Delta=0.002133147719550621\n",
      "Episode 80 finished after 4 timesteps. Epsilon=0.6696478204705644. Q_Delta=0.0007947167643661268\n",
      "Episode 81 finished after 28 timesteps. Epsilon=0.6662995813682115. Q_Delta=0.001608996353044952\n",
      "Episode 82 finished after 16 timesteps. Epsilon=0.6629680834613705. Q_Delta=0.0023856580538345884\n",
      "Episode 83 finished after 5 timesteps. Epsilon=0.6596532430440636. Q_Delta=0.0020163421571666616\n",
      "Episode 84 finished after 4 timesteps. Epsilon=0.6563549768288433. Q_Delta=0.0026434074158677157\n",
      "Episode 85 finished after 8 timesteps. Epsilon=0.653073201944699. Q_Delta=0.002763186753446334\n",
      "Episode 86 finished after 11 timesteps. Epsilon=0.6498078359349755. Q_Delta=0.0017572207138471469\n",
      "Episode 87 finished after 14 timesteps. Epsilon=0.6465587967553006. Q_Delta=0.0015627393113247393\n",
      "Episode 88 finished after 16 timesteps. Epsilon=0.6433260027715241. Q_Delta=0.0014646397622517307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 89 finished after 8 timesteps. Epsilon=0.6401093727576664. Q_Delta=0.001637049124397577\n",
      "Episode 90 finished after 6 timesteps. Epsilon=0.6369088258938781. Q_Delta=0.0017658351382095667\n",
      "Episode 91 finished after 18 timesteps. Epsilon=0.6337242817644086. Q_Delta=0.002048604561407868\n",
      "Episode 92 finished after 2 timesteps. Epsilon=0.6305556603555866. Q_Delta=0.00018815838078201086\n",
      "Episode 93 finished after 1 timesteps. Epsilon=0.6274028820538087. Q_Delta=0.0003945377850897991\n",
      "Episode 94 finished after 13 timesteps. Epsilon=0.6242658676435396. Q_Delta=0.0022978634603470706\n",
      "Episode 95 finished after 5 timesteps. Epsilon=0.6211445383053219. Q_Delta=0.002792366290694037\n",
      "Episode 96 finished after 1 timesteps. Epsilon=0.6180388156137953. Q_Delta=0.005141525851555695\n",
      "Episode 97 finished after 22 timesteps. Epsilon=0.6149486215357263. Q_Delta=0.002408364883414328\n",
      "Episode 98 finished after 19 timesteps. Epsilon=0.6118738784280476. Q_Delta=0.002061191101032761\n",
      "Episode 99 finished after 8 timesteps. Epsilon=0.6088145090359074. Q_Delta=0.0017280925947227438\n",
      "Episode 100 finished after 3 timesteps. Epsilon=0.6057704364907278. Q_Delta=0.0016858621019605706\n"
     ]
    }
   ],
   "source": [
    "train_deep(epsilon_greedy_policy, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished after 3 timesteps\n",
      "Episode 1 finished after 1 timesteps\n",
      "Episode 2 finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "visualise_agent(greedy_policy, n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
