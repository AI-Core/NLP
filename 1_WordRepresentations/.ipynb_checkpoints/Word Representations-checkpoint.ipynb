{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to NLP, Pre-processing, and Word Representations\n",
    "\n",
    "### Workshop contents:\n",
    "- [A very brief overview of pre-neural NLP](#Pre-neural-NLP)\n",
    "- How do we get computers to represent our data?\n",
    "- Distributed Representations\n",
    "- Pre-processing and tokenization: Cleaning your corpus\n",
    "- Word2Vec\n",
    "    - Skip-gram\n",
    "    - Negative Sampling\n",
    "- SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-neural NLP\n",
    "blah blah blah, neural networks are better. With NLP you can do cool things: one-to-one, one-to-many, many-to-one, many-to-many"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we get computers to represent our data?\n",
    "Let's assume we're working with a many-to-one classification problem, for example classifying user written movie reviews between 1-5. How can we feed raw text into a model:\n",
    "\n",
    "![1_raw_text_model](attachment:images/1_raw_text_model.png)\n",
    "\n",
    "\n",
    "One possible solution is to one-hot our **corpus** based on our **vocabulary**:\n",
    "![2_corpus_vocab](attachment:images/2_corpus_vocab.png)\n",
    "\n",
    "Let's build this:\n",
    "\n",
    "\n",
    "One-hot encoding? Problems with one-hot (no concept of sequence, fixed length input, high dimensional etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations\n",
    "Toy example with non-textual data, e.g. personality test or animal cuteness/size. Introduce word vectors along with the cool things you can do with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and tokenization: Cleaning your corpus\n",
    "Now that we know how we want to represent words, let's do it! However, before we do so we need to clean our dataset. Oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "Two variants, CBOW and Skip-gram. We'll focus on Skip-gram because X, Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
