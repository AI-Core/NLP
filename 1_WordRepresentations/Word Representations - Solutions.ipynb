{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written by [Nihir](mailto:nv419@ic.ac.uk) (Twitter/IG: @nvedd)\n",
    "\n",
    "For [AI Core](theaicore.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to NLP, Pre-processing, and Word Representations\n",
    "\n",
    "### Workshop contents:\n",
    "- [A very brief overview of pre-neural NLP](#Introduction-to-NLP)\n",
    "- [How do we get computers to represent our data?](#How-do-we-get-computers-to-represent-our-natural-language-data?)\n",
    "- [Distributed Representations](#Distributed-Representations)\n",
    "- [Pre-processing and tokenization: Cleaning your corpus](#Pre-processing-and-tokenization:-Cleaning-your-corpus)\n",
    "- [Word2Vec](#Word2Vec)\n",
    "    - [Skipgram](#Skipgram)\n",
    "- [SpaCy](#SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "_Natural language processing_ (NLP) is an interdisciplinary field concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. There are a wide variety of tasks that NLP covers, such as Translation, Natural Language Generation, Entity Detection, Sentiment Classification, and so forth.\n",
    "\n",
    "Research into NLP started in the 50s. Even though today's systems are (obviously) vastly better than what they were previously, NLP is still considered \"unsolved\" and modern research is evolving at a rapid pace. One facet as to why it is challenging is due to the ambiguity of language. For example, the word \"mean\" has multiple definitions: \"signify\", \"unkind\", and \"average\". We also find ourselves using idioms a lot in language - phrases for which the meaning doesn't directly represent sequence of words (e.g. over the moon). Specifically looking at translation, word order differences also prove to be a problem:\n",
    "- DE: Gestern bin ich in London gewesen\n",
    "- Word-By-Word EN: ‘Yesterday have I to London been’\n",
    "- Ground Truth EN: Yesterday I have been to London \n",
    "\n",
    "The history of NLP can essentially be broken down into three approaches: Rule-based, Statistical, and Neural:\n",
    "- Rule-based (1950 - 1999)\n",
    " - Hand-crafted rules to model linguistic intuitions \n",
    "- Corpus-based \n",
    " - Example-based (EBMT) - MT Translation by analogy: if this segment has been translated before, copy its translation\n",
    " - Statistical (2000-2015)\n",
    "   - Statistical models used to push the “translation by analogy” paradigm to its extreme \n",
    "   - Language-independent\n",
    "   - Low cost\n",
    " - Neural - a.k.a. Deep learning (2014-)\n",
    "   - Learning __representations (features) & models__ from data\n",
    "   \n",
    "Neural approaches to NLP in particular allow us to solve the following kinds of problems:\n",
    "![types_rnn](images/types_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we get computers to represent our natural language data?\n",
    "Let's assume we're working with a many-to-one classification problem, for example classifying user written movie reviews between 1-5. How can we feed raw text into a model:\n",
    "\n",
    "![1_raw_text_model](images/1_raw_text_model.png)\n",
    "\n",
    "\n",
    "One possible solution is to one-hot our **corpus** based on our **vocabulary**:\n",
    "![2_corpus_vocab](images/2_corpus_vocab.png)\n",
    "\n",
    "Let's build this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[[\"this movie had a brilliant story line with great action\"],\n",
    "[\"some parts were not so great but overall pretty ok\"],\n",
    "[\"my dog went to sleep watching this piece of trash\"]] # we'll cover how to deal with emoji later\n",
    "\n",
    "# Let's tokenize our corpus. \n",
    "  # Tokeniziation involves splitting the corpus from the sentence level to the word level\n",
    "\n",
    "def get_tokenized_corpus(corpus):\n",
    "    return [sentence[0].split(\" \") for sentence in corpus]\n",
    "\n",
    "tokenized_corpus = get_tokenized_corpus(corpus)\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code to load in a __vocabulary__. A vocabulary is simply a __list of unique words__ which we can perform lookups against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = open(\"google-10000-english.txt\", \"r\")\n",
    "vocabulary = [word.strip() for word in vocab_file.readlines()]\n",
    "print(\"First five entries of vocabulary:\", vocabulary[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3d_onehot](images/one_hot31010000.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN\n",
    "\n",
    "# Let's one-hot the tokenized corpus\n",
    "import numpy as np\n",
    "\n",
    "def get_onehot_corpus(corpus, tokenized_corpus, vocabulary):\n",
    "\n",
    "    CORPUS_SIZE = len(corpus)\n",
    "    MAX_LEN_SEQUENCE = max(len(x) for x in tokenized_corpus)\n",
    "    VOCAB_LEN = len(vocabulary)\n",
    "\n",
    "    onehot_corpus = np.zeros((CORPUS_SIZE, MAX_LEN_SEQUENCE, VOCAB_LEN))\n",
    "    for corpus_idx, tokenized_sentence in enumerate(tokenized_corpus):\n",
    "        for sequence_idx, token in enumerate(tokenized_sentence):\n",
    "            token_vocab_idx = vocabulary.index(token)\n",
    "            onehot_corpus[corpus_idx, sequence_idx, token_vocab_idx] = 1\n",
    "            \n",
    "    return onehot_corpus\n",
    "\n",
    "onehot_corpus = get_onehot_corpus(corpus, tokenized_corpus, vocabulary)\n",
    "print(onehot_corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = [[\"good movie\"], [\"this movie had a significant amount of flaws\"]]\n",
    "corpus.extend(new_reviews)\n",
    "\n",
    "tokenized_corpus = get_tokenized_corpus(corpus)\n",
    "onehot_corpus = get_onehot_corpus(corpus, tokenized_corpus, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh... How do you think we can get around this? Discuss with someone around you the issues of one-hot encoding.\n",
    "\n",
    "Dog and hound. Dog and potato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Representations\n",
    "__\"You shall know a word by the company it keeps\" - Firth (1957).__\n",
    "What does this mean?\n",
    "\n",
    "Before focus on words themselves, let's look at concepts with a 2-dimension representations.\n",
    "Animal cuteness and size:  \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Animal</th>\n",
    "    <th>Cuteness</th>\n",
    "    <th>Size</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Lion</td>\n",
    "    <td>80</td>\n",
    "    <td>50</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Elephant</td>\n",
    "    <td>75</td>\n",
    "    <td>95</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hyena</td>\n",
    "    <td>10</td>\n",
    "    <td>30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mouse</td>\n",
    "    <td>60</td>\n",
    "    <td>8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pig</td>\n",
    "    <td>30</td>\n",
    "    <td>30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Horse</td>\n",
    "    <td>50</td>\n",
    "    <td>65</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Dolphin</td>\n",
    "    <td>90</td>\n",
    "    <td>45</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wasp</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Giraffe</td>\n",
    "    <td>60</td>\n",
    "    <td>80</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Dog</td>\n",
    "    <td>95</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Alligator</td>\n",
    "    <td>8</td>\n",
    "    <td>40</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mole</td>\n",
    "    <td>30</td>\n",
    "    <td>12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Black Widow</td>\n",
    "    <td>100</td>\n",
    "    <td>30</td>\n",
    "  </tr>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "animal_labels = [\"Lion\", \"Elephant\", \"Hyena\", \"Mouse\", \"Pig\", \"Horse\", \"Dolphin\", \"Wasp\", \"Giraffe\", \"Dog\", \"Alligator\", \"Mole\", \"Scarlett Johansson\"]\n",
    "animal_cuteness = [80, 75, 10, 60, 30, 50, 90, 1, 60, 95, 8, 30, 100]\n",
    "animal_size = [50, 95, 30, 8, 30, 65, 45, 1, 80, 20, 40, 12, 30]\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=animal_cuteness, y=animal_size,\n",
    "    text=animal_labels,\n",
    "    mode='markers+text',\n",
    "    marker_size=70)\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Animal Cuteness vs Animal Size\",\n",
    "    xaxis_title=\"Animal Cuteness\",\n",
    "    yaxis_title=\"Animal Size\",\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows us that the closest animal to the Alligator is the Hyena. Let's calculate the (Euclidean) distance between the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def distance_2d(x1, y1, x2, y2):\n",
    "    return math.sqrt((x1-x2)**2 + (y1-y2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to return us the animal information we need\n",
    "def get_animal_info(animal_name):\n",
    "    if animal_name in animal_labels:\n",
    "        animal_idx = animal_labels.index(animal_name)\n",
    "        animal_cuteness_ = animal_cuteness[animal_idx]\n",
    "        animal_size_ = animal_size[animal_idx]\n",
    "        return animal_cuteness_, animal_size_\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alligator_cuteness, alligator_size = get_animal_info(\"Alligator\")\n",
    "hyena_cuteness, hyena_size = get_animal_info(\"Hyena\")\n",
    "elephant_cuteness, elephant_size = get_animal_info(\"Elephant\")\n",
    "\n",
    "print(\"DISTANCE BETWEEN ALLIGATOR AND HYENA\", distance_2d(alligator_cuteness, alligator_size, hyena_cuteness, hyena_size))\n",
    "print(\"DISTANCE BETWEEN ALLIGATOR AND ELEPHANT\", distance_2d(alligator_cuteness, alligator_size, elephant_cuteness, elephant_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visual representation allows us to reason about many things. We can ask, for example, what's halfway between a Mosquito and a Horse (a Pig). We can also ask about differences. For example, the difference between a Mole and Mouse is 30 units of cuteness and a couple units in size.\n",
    "\n",
    "The concept of difference allows us to reason about analogies. This means that we can say that animal_1 is to animal_2 the way that animal_3 is to animal_4. For example, we can say `Pig is to Horse as Mouse is to ???`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_cuteness, horse_size = get_animal_info(\"Horse\")\n",
    "pig_cuteness, pig_size = get_animal_info(\"Pig\")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[pig_cuteness, horse_cuteness], y=[pig_size, horse_size]))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the x and y difference between Pig and Horse\n",
    "pig_horse_diff_cuteness = abs(pig_cuteness - horse_cuteness)\n",
    "pig_horse_diff_size = abs(pig_size - horse_size)\n",
    "\n",
    "# now let's apply the analogy to Mouse:\n",
    "  # Pig is to Horse as Mouse is to ???\n",
    "mouse_cuteness, mouse_size = get_animal_info(\"Mouse\")\n",
    "fig.add_trace(go.Scatter(x=[mouse_cuteness, mouse_cuteness + pig_horse_diff_cuteness], y=[mouse_size, mouse_size + pig_horse_diff_size]))\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# Lion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These concepts also work in higher dimensions. Before focusing on words themselves, let's briefly add another dimension to our dataset and look at one or two more concepts:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Animal</th>\n",
    "    <th>Cuteness</th>\n",
    "    <th>Size</th>\n",
    "    <th>Ferocity</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Lion</td>\n",
    "    <td>80</td>\n",
    "    <td>50</td>\n",
    "    <td>85</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Elephant</td>\n",
    "    <td>75</td>\n",
    "    <td>95</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hyena</td>\n",
    "    <td>10</td>\n",
    "    <td>30</td>\n",
    "    <td>90</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mouse</td>\n",
    "    <td>60</td>\n",
    "    <td>8</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pig</td>\n",
    "    <td>30</td>\n",
    "    <td>30</td>\n",
    "    <td>10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Horse</td>\n",
    "    <td>50</td>\n",
    "    <td>65</td>\n",
    "    <td>30</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Dolphin</td>\n",
    "    <td>90</td>\n",
    "    <td>45</td>\n",
    "    <td>20</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wasp</td>\n",
    "    <td>2</td>\n",
    "    <td>1</td>\n",
    "    <td>100</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Giraffe</td>\n",
    "    <td>60</td>\n",
    "    <td>80</td>\n",
    "    <td>65</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Dog</td>\n",
    "    <td>95</td>\n",
    "    <td>20</td>\n",
    "    <td>15</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Alligator</td>\n",
    "    <td>8</td>\n",
    "    <td>40</td>\n",
    "    <td>90</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mole</td>\n",
    "    <td>30</td>\n",
    "    <td>12</td>\n",
    "    <td>15</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Black Widow</td>\n",
    "    <td>100</td>\n",
    "    <td>30</td>\n",
    "    <td>69</td>\n",
    "  </tr>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to remind us ;)\n",
    "animal_labels = animal_labels\n",
    "animal_cuteness = animal_cuteness\n",
    "animal_size = animal_size\n",
    "animal_ferocity = [85, 20, 90, 1, 10, 30, 20, 100, 65, 15, 90, 15, 69]\n",
    "\n",
    "\n",
    "# nothing particularly important... just used for visualisation purposes\n",
    "import statistics\n",
    "animal_mean_stats = [statistics.mean(k) for k in zip(animal_cuteness, animal_size, animal_ferocity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=animal_cuteness, y=animal_size, z=animal_ferocity,\n",
    "    text=animal_labels,\n",
    "    mode='markers+text',\n",
    "    marker=dict(\n",
    "        size=12,\n",
    "        color=animal_mean_stats,                # set color to an array/list of desired values\n",
    "        colorscale='Viridis',   # choose a colorscale\n",
    "        opacity=0.8\n",
    "    ))\n",
    "])\n",
    "\n",
    "fig.update_layout(title=\"Animal Cuteness vs Animal Size vs Animal Ferocity\",\n",
    "    scene = dict(\n",
    "    xaxis_title='Animal Cuteness',\n",
    "    yaxis_title='Animal Size',\n",
    "    zaxis_title='Animal Ferocity')\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's redefine this function to return ferocity as well\n",
    "def get_animal_info(animal_name):\n",
    "    if animal_name in animal_labels:\n",
    "        animal_idx = animal_labels.index(animal_name)\n",
    "        animal_cuteness_ = animal_cuteness[animal_idx]\n",
    "        animal_size_ = animal_size[animal_idx]\n",
    "        animal_ferocity_ = animal_ferocity[animal_idx]\n",
    "        return animal_cuteness_, animal_size_, animal_ferocity_\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll demonstrate how to calculate the distance between vectors in 3d space, show the closest `n` points to a given point (animal) and also show how analogies work in 3 dimensions. The point of this exercise is give you an intuition behind how things can analogusly work in higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nd_distance](images/nd_distance.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "def distance(coord1, coord2):\n",
    "    return math.sqrt(sum([(i - j)**2 for i, j in zip(coord1, coord2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyena_coords = get_animal_info(\"Hyena\")\n",
    "alligator_coords = get_animal_info(\"Alligator\")\n",
    "print(\"Distance between Hyena and Alligator:\", distance(hyena_coords, alligator_coords))\n",
    "\n",
    "dog_coords = get_animal_info(\"Dog\")\n",
    "print(\"Distance between Hyena and Dog:\", distance(hyena_coords, dog_coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closest\n",
    "animal_info = zip(animal_labels, animal_cuteness, animal_size, animal_ferocity)\n",
    "\n",
    "def closest_to(animal_name, n=3):\n",
    "    primary_animal_stats = get_animal_info(animal_name)\n",
    "    distances_from_animal = []\n",
    "    for label, cuteness, size, ferocity in animal_info:\n",
    "        \n",
    "        if label==animal_name:\n",
    "            continue\n",
    "            \n",
    "        secondary_animal_stat = (cuteness, size, ferocity)\n",
    "        distances_from_animal.append((label, distance(primary_animal_stats, secondary_animal_stat)))\n",
    "        \n",
    "    sorted_distances_from_animal = sorted(distances_from_animal, key=lambda x: x[1])\n",
    "    \n",
    "    return sorted_distances_from_animal[:n]\n",
    "    \n",
    "closest_to(\"Horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we could do the same with words? Before introducing the methodology of obtaining these vectors, it's relevant to discuss why word vectors are effective. One of the main reasons adoption is so widespread is because they can be **pretrained**. What this means is that we can use word vectors which have been trained on any corpus (e.g. Wikipedia, Twitter, medical journals, ancient religious texts etc.) in a potentially domain specific downstream task. For example, word vectors obtained by training on ancient religious texts might be used to classify ancient religious texts into a religion; or Twitter data can be used to generate conversation-like agents. That is, each word vector is just a vector to represent that word, and the algorithm we're using to solve the task at hand will learn the embeddings for all the words based on their context in the training corpus (e.g. the \"meaning\" of the word _lit_ would be different if comparing the vector based on a religious text and the vector based on Twitter).\n",
    "\n",
    "Before the methodology, let's analyse some of these pre-trained embeddings. We'll use King, Queen, Royal, Man, Woman, Water, and Earth as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_file = open(\"glove_50d_TRUNCATED.txt\", \"r\", encoding=\"utf8\")\n",
    "glove_vectors_list = [word_and_vector.strip() for word_and_vector in glove_file.readlines()]\n",
    "glove_vectors = {obj.split()[0]: np.asarray(obj.split()[1:], dtype=np.float) for obj in glove_vectors_list}\n",
    "print(glove_vectors[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector = glove_vectors[\"king\"]\n",
    "queen_vector = glove_vectors[\"queen\"]\n",
    "man_vector = glove_vectors[\"man\"]\n",
    "woman_vector = glove_vectors[\"woman\"]\n",
    "water_vector = glove_vectors[\"water\"]\n",
    "earth_vector = glove_vectors[\"earth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Let's visualise these vectors and colour them based on their elemental values\n",
    "# Red is lower, white=0, blue is higher\n",
    "\n",
    "# vectors is a list of tuples: [(vector_name, vector)]\n",
    "def plot_vectors(vectors):\n",
    "    \n",
    "    fig = make_subplots(rows=len(vectors), cols=1)\n",
    "    \n",
    "    for i, vector_tuple in enumerate(vectors):\n",
    "        vector_name = vector_tuple[0]\n",
    "        vector = vector_tuple[1]\n",
    "\n",
    "        normalized_vector = (vector-np.min(vector))/(np.ptp(vector))\n",
    "        x = [\"dimension_\"+str(_) for _ in range(50)]\n",
    "        y = [1] * 50\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=x, y=y, marker=dict(\n",
    "            color=normalized_vector,                # set color to an array/list of desired values\n",
    "            colorscale='rdbu',   # choose a colorscale\n",
    "            opacity=0.8,\n",
    "        )),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.update_yaxes(title_text=vector_name, row=i+1, col=1)\n",
    "        \n",
    "    \n",
    "    fig.update_layout(height=175*len(vectors), xaxis_showgrid=False, yaxis_showgrid=False, showlegend=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.show()\n",
    "\n",
    "plot_vectors([(\"King\", king_vector), (\"Queen\", queen_vector), (\"Man\", man_vector), (\"Woman\", woman_vector), (\"Water\", water_vector), (\"Earth\", earth_vector)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what was done with the animals previously, we can apply analogies to word vectors. In high-dimensional space it is preferrable to use **cosine** distance.\n",
    "\n",
    "![cosine_distance](images/cosine_distance.png)\n",
    "\n",
    "Before applying/finding any analogies, we need a function which finds the closest vector to another vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cosine distance instead of the euclidean distance we coded up earlier on\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find_closest(to_find_closest, exclude_list):\n",
    "    closest_distance = float(\"inf\")\n",
    "    closest_token = None\n",
    "\n",
    "    for token, vector in glove_vectors.items():\n",
    "        distance = cosine(to_find_closest, vector)\n",
    "        if closest_distance > distance and token not in exclude_list:\n",
    "            closest_distance = distance\n",
    "            closest_token = token\n",
    "\n",
    "    return closest_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find_closest = king_vector - man_vector + woman_vector\n",
    "print(find_closest(to_find_closest, [\"king\", \"man\", \"woman\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about.. London is to England as Paris is to X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_england = glove_vectors[\"england\"] - glove_vectors[\"london\"]\n",
    "to_find_closest = london_england + glove_vectors[\"paris\"]\n",
    "print(find_closest(to_find_closest, [\"london\", \"england\", \"paris\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside analogies, we are also able to semantically 'reason':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest(glove_vectors[\"france\"] + glove_vectors[\"capital\"], [\"france\", \"capital\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The takeaway: Distributed vectors group similar words/objects together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and tokenization: Cleaning your corpus\n",
    "Now that we know how we want to represent words, let's do it! However, before we do so we need to clean our dataset. We're going to create our vocabulary from scratch using our corpus. Oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# our corpus is now more in a format you would find a corpus in production\n",
    "corpus = [corpus[idx][0] for idx, sentence in enumerate(corpus)]\n",
    "\n",
    "more_movie_reviews = [\n",
    "    \"this was a BRILLIANT 👍 movie\",\n",
    "    \"💩 film\",\n",
    "    \"this was a terrible movie\",\n",
    "    \"this was a treribel movie\",\n",
    "    \"this was a good 👍 movie\",\n",
    "    \"A moving story about U.S. wildlife.\",\n",
    "    \"Wow. I had not expected wildlife in the US to be so diverse\",\n",
    "    \"Wow - what a MOVIE 👍\",\n",
    "    \"Us here at The Movie Reviewers found this movie exceedingly average\",\n",
    "    \"The Polish people in this film... stunning 👍\",\n",
    "    \"A bit of a polish to this movie would have made it great\",\n",
    "    \"This film didn't exite me as much as much as the prequel.\",\n",
    "    \"this movie doesn't live up to the hype of its trailer\",\n",
    "    \"It's rare that a film is this good??\",\n",
    "    \"This movie was 👍 💩\"\n",
    "]\n",
    "\n",
    "corpus.extend(more_movie_reviews)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did at the beginning of this notebook, we need to tokenize this corpus. Code up a function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    return [review.split() for review in corpus]\n",
    "        \n",
    "tokenized_corpus = tokenize(corpus)\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What issues can you see? Let's count all the distinct tokens now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tokens_count = {}\n",
    "for t_review in tokenized_corpus:\n",
    "    for token in t_review:\n",
    "        if token not in distinct_tokens_count.keys():\n",
    "            distinct_tokens_count[token] = 1\n",
    "        else:\n",
    "            distinct_tokens_count[token] += 1\n",
    "            \n",
    "for token, count in distinct_tokens_count.items():\n",
    "    print(\"{:<14s} {:<10d}\".format(token, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.. that's a lot of information, lets just look at the key points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"movie\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"film\" in token.lower():\n",
    "        print(token, count)        \n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"good\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"polish\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"wildlife\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"us\" in token.lower() or \"u.s.\" in token.lower():\n",
    "        print(token, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so let's quickly run through the issues here. Firstly, we see `movie`, `Movie` and `MOVIE`. How can we resolve this issue during our tokenization step? We could lowercase everything, but what issues can you see that causing? Semantic differences of Polish vs polish etc. What about the two different instances of `film` (`film...`)? Ok, we can remove punctuation, but wouldn't this ruin the `U.S.` acronym? Another solution would be to count `...` as a token, or remove it, but keep `.`s as part of the token if it is instantly followed by a letter (i.e. no space). This part of the pre-processing pipeline is called normalization. Let's do some basic normalization for now, and later on we'll use a library to do this for us ([regex] is a beast in itself). Our normalization step will be to lowercase everything and remove all punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regex\n",
    "\n",
    "# The backslash is an escape character to let the interpreter know we mean to use it as a string literal (- or ') \n",
    "re_punctuation_string = '[\\s,/.?\\-\\']'  # split on spaces (\\s),  commas (,), slash (/), fullstop (.), question marks (?), hyphens (-), and apostrophe (').\n",
    "tokenized_corpus = []\n",
    "for review in corpus:\n",
    "    tokenized_review = re.split(re_punctuation_string, review) # in python's regex, [...] is an alternative to writing .|.|.\n",
    "    tokenized_review = list(filter(None, tokenized_review)) # remove empty strings from list \n",
    "    tokenized_corpus.append([token.lower() for token in tokenized_review]) # Lowercasing everything\n",
    "        \n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_tokens_count = {}\n",
    "for t_review in tokenized_corpus:\n",
    "    for token in t_review:\n",
    "        if token not in distinct_tokens_count.keys():\n",
    "            distinct_tokens_count[token] = 1\n",
    "        else:\n",
    "            distinct_tokens_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"movie\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"film\" in token.lower():\n",
    "        print(token, count)        \n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"good\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"polish\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"wildlife\" in token.lower():\n",
    "        print(token, count)\n",
    "\n",
    "for token, count in distinct_tokens_count.items():\n",
    "    if \"us\" in token.lower() or \"u.s.\" in token.lower():\n",
    "        print(token, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some problems, e.g.: `['a', 'moving', 'story', 'about', 'u', 's', 'wildlife']` and the letters that were split on after apostrophes, e.g.: `s` and `t`. We'll leave this as is for now, and sort this out later. What do you think we need to do the emoji?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"U+\", ord(\"😂\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "\n",
    "Representing words as vectors is often attributed to the quote at the beginning of this section - __you shall know a word by the company it keeps__. Above, we demonstrated how this quote is manifested by the idea of distributed representations. This section discusses and implements one way of obtaining representations.\n",
    "\n",
    "Word2Vec refers to a family of neural network based algorithms which obtain these word vectors/embeddings. One part of this family consists of two different model architectures: Continuous bag-of-words and Skip-gram. The other part of this family consists of two different approaches of dealing with a vocabulary in the order of **millions**: Hierarchical Softmax and Negative Sampling.\n",
    "\n",
    "N.B. Word2Vec is not the only algorithm used to obtain word vectors. [GLoVE](https://nlp.stanford.edu/projects/glove/), [FastText](https://fasttext.cc/), and [pre-trained language models](https://arxiv.org/abs/1810.04805) are alternatives. We will be looking at pre-trained language models later on this course. The reason **Word2Vec** is focused on is because it provides a simple intuition behind how we can use neural networks to reduce dimensionality and then use the lower-dimensional vectors in downstream tasks. Using parameters from one model in another is also part of a paradigm known as _transfer learning_.\n",
    "\n",
    "Recall the problem we are trying to solve - representing our sequences, movie reviews, in a way that we can feed to a (classification) model. Earlier on, we loaded in pre-trained embeddings. Each vector we obtained for a given word was the representation of this word. Let's discuss how to get this representation.\n",
    "\n",
    "Looking at the quote again, we have this notion of \"company\". What do you think this means? It leads to the term we call a context window: the words which are in the **negative** $c$ range to the **positive** $c$ range away from the context word $w_t$.\n",
    "\n",
    "![windows](images/context_window.png)\n",
    "\n",
    "Two variants, Continuous Bag of Words (CBOW) and Skipgram. We'll focus on skipgram because it has shown to outperform CBOW on larger corpus'. After we implement skipgram below, see if you're able to implement CBOW yourself.\n",
    "\n",
    "![image.png](images/cbow_skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the knowledge that the input and output are one-hot, lets derive the objective. Instead of considering the problem where we have four gold labels, lets look at the case where we have one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skipgram_NN](images/skipgram_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skipgram_concepts](images/skipgram_concepts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skipgram_maths](images/skipgram_maths.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what our neural network is going to look like - we just need to figure out how to feed our data into the model. To convert our input to one hot we'll need to build a vocabulary dictionary which maps words to an integer. We're going to use our corpus to build our vocabulary. The reason we didn't do that before was because we loaded in an external vocabulary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "for review in tokenized_corpus:\n",
    "    vocabulary.update(review)\n",
    "\n",
    "print(\"LENGTH OF VOCAB:\", len(vocabulary), \"\\nVOCAB:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map these to the aforementioned dictionaries\n",
    "word2idx = {}\n",
    "n_words = 0\n",
    "\n",
    "for token in vocabulary:\n",
    "    if token not in word2idx:\n",
    "        word2idx[token] = n_words\n",
    "        n_words += 1\n",
    "        \n",
    "assert len(word2idx) == len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, we should extract our contexts and focus words:**\n",
    "- Let's say we're considering the sentence: `['this', 'movie', 'had', 'a', 'brilliant', 'story', 'line', 'with', 'great', 'action']`\n",
    "- For every word in the sentence, we want to get the words which are `window_size` around it.\n",
    "- So if `window_size==2`, for the word `this`, we obtain: `[['this', 'movie'], ['this', 'had']]`\n",
    "- For the word `movie`, we obtain: `[['movie', 'this'], ['movie', 'had'], ['movie', 'a']]`\n",
    "- For the word `had`, we obtain: `[['had', 'this'], ['had', 'movie'], ['had', 'a'], ['had', 'brilliant']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_focus_context_pairs(tokenized_corpus, window_size=2):\n",
    "    focus_context_pairs = []\n",
    "    for sentence in tokenized_corpus:\n",
    "\n",
    "        for token_idx, token in enumerate(sentence):\n",
    "            for w in range(-window_size, window_size+1):\n",
    "                context_word_pos = token_idx + w\n",
    "\n",
    "                if w == 0 or context_word_pos >= len(sentence) or context_word_pos < 0:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    focus_context_pairs.append([token, sentence[context_word_pos]])\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    return focus_context_pairs\n",
    "                \n",
    "focus_context_pairs = get_focus_context_pairs(tokenized_corpus)\n",
    "print(focus_context_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map these to our indicies in preparation to one-hot\n",
    "def get_focus_context_idx(focus_context_pairs):\n",
    "    idx_pairs = []\n",
    "    for pair in focus_context_pairs:\n",
    "        idx_pairs.append([word2idx[pair[0]], word2idx[pair[1]]])\n",
    "    \n",
    "    return idx_pairs\n",
    "\n",
    "idx_pairs = get_focus_context_idx(focus_context_pairs)\n",
    "print(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(indicies, vocab_size=len(vocabulary)):\n",
    "    oh_matrix = np.zeros((len(indicies), vocab_size))\n",
    "    for i, idx in enumerate(indicies):\n",
    "        oh_matrix[i, idx] = 1\n",
    "\n",
    "    return torch.Tensor(oh_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "writer = SummaryWriter('runs/word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Why do you think we don't have an activation function here?\n",
    "        self.projection = nn.Linear(input_size, hidden_dim_size, bias=False)\n",
    "        self.output = nn.Linear(hidden_dim_size, output_size)\n",
    "        \n",
    "    def forward(self, input_token):\n",
    "        x = self.projection(input_token)\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard doesn't handle encoding emojis.\n",
    "# So while we can train our model on emojis (as we've just done)\n",
    "# We gotta convert their unicode string to something displayable on Tensorboard\n",
    "\n",
    "word2idx[\":pile_of_poo:\"] = word2idx.pop(\"\\U0001f4a9\")\n",
    "word2idx[\":thumbs_up:\"] = word2idx.pop(\"\\U0001f44d\")\n",
    "word2idx = {k: v for k, v in sorted(word2idx.items(), key=lambda item: item[1])} # sort dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(word2vec_model, idx_pairs, state_dict_filename, early_stop=False, num_epochs=10, lr=1e-3):\n",
    "\n",
    "    word2vec_model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(word2vec_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        random.shuffle(idx_pairs)\n",
    "\n",
    "        for focus, context in idx_pairs:\n",
    "            print(focus)\n",
    "            oh_inputs = get_one_hot([focus], len(vocabulary))\n",
    "            target = torch.LongTensor([context])\n",
    "\n",
    "            pred_outputs = word2vec_model(oh_inputs)\n",
    "\n",
    "            loss = criterion(pred_outputs, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            word2vec_model.zero_grad()\n",
    "            \n",
    "        ### These lines stop training early\n",
    "            if early_stop: break\n",
    "        if early_stop: break\n",
    "        ###\n",
    "\n",
    "        torch.save(word2vec_model.state_dict(), state_dict_filename)\n",
    "        writer.add_embedding(word2vec_model.projection.weight.T,\n",
    "                             metadata=word2idx.keys(), global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(len(vocabulary), len(vocabulary), 10)\n",
    "train(word2vec, idx_pairs, \"word2vec.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've covered how to write some basic pre-processing rules from scratch, and we've seen some issues that rule based pre-processing can cause. Sometimes it'll simply take too long to code up rules for all edge cases. In downstream tasks, we might also want to augment the data with some more information, for example, their Parts of Speech tag (PoS) - an identifier per word describing the type of word it is (e.g. verb, adjective).\n",
    "\n",
    "SpaCy is an easy to use NLP library which gives us access to neural-models for various linguistic features. In this section we're going to use the Tokenizer to preprocess a larger corpus of text and train a word2vec model on this corpus. Then we're going to use Tensorboard to visualise the embeddings we've just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "GUTENBERG_DIR = \"gutenberg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_books = []\n",
    "for i, book_name in enumerate(os.listdir(GUTENBERG_DIR)):\n",
    "    book_file = open(os.path.join(\n",
    "        GUTENBERG_DIR, book_name), encoding=\"latin-1\")\n",
    "    book = book_file.read()\n",
    "    gutenberg_books.append(book)\n",
    "    book_file.close()\n",
    "    if i == 3:\n",
    "        break\n",
    "\n",
    "gutenberg_book_lines = []\n",
    "for book in tqdm(gutenberg_books):\n",
    "    book_lines = book.split(\"\\n\")\n",
    "    book_lines = list(filter(lambda x: x != \"\", book_lines))\n",
    "    gutenberg_book_lines.append(book_lines)\n",
    "    \n",
    "# print(gutenberg_book_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"tokenized_corpus_gutenberg.pkl\"):\n",
    "    tokenized_corpus = pickle.load(open(\"tokenized_corpus_gutenberg.pkl\", \"rb\"))\n",
    "else:\n",
    "    tokenized_corpus = []\n",
    "    for book_line in tqdm(gutenberg_book_lines):\n",
    "        for line in tqdm(book_line):\n",
    "            doc = nlp(line)\n",
    "            tokenized_corpus.append([token.text.lower()\n",
    "                                     for token in doc if not token.is_punct])\n",
    "\n",
    "    print(tokenized_corpus[0:5])\n",
    "    pickle.dump(tokenized_corpus, open(\"tokenized_corpus_gutenberg.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing which wasn't mentioned before was the discrepency in the words which may be present at test/inference time but not during training. These are known as __out of vocabulary__ tokens. A simple strategy to deal with this is to replace every word in our training set which occurs with less than a certain threshold with an `<OOV>` token. At test time, if a given word isn't in the vocabulary that the model was trained on, we simply replace it with the `<OOV>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokenized_corpus, cutoff_frequency=5):\n",
    "    vocab_freq_dict = dict()\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocab_freq_dict.keys():\n",
    "                vocab_freq_dict[token] = 0\n",
    "\n",
    "            vocab_freq_dict[token] += 1\n",
    "\n",
    "    vocabulary = set()\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if vocab_freq_dict[token] > cutoff_frequency:\n",
    "                vocabulary.add(token)\n",
    "                \n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = get_vocabulary(tokenized_corpus)\n",
    "print(\"LENGTH OF VOCAB:\", len(vocabulary), \"\\nVOCAB:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_token = \"<OOV>\"\n",
    "vocabulary.add(OOV_token)\n",
    "word2idx = {}\n",
    "n_words = 0\n",
    "\n",
    "tokenized_corpus_with_OOV = []\n",
    "for sentence in tokenized_corpus:\n",
    "\n",
    "    tokenized_sentence_with_OOV = []\n",
    "    for token in sentence:\n",
    "        if token in vocabulary:\n",
    "            tokenized_sentence_with_OOV.append(token)\n",
    "        else:\n",
    "            tokenized_sentence_with_OOV.append(OOV_token)\n",
    "    tokenized_corpus_with_OOV.append(tokenized_sentence_with_OOV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in vocabulary:\n",
    "    if token not in word2idx:\n",
    "        word2idx[token] = n_words\n",
    "        n_words += 1\n",
    "\n",
    "assert len(word2idx) == len(vocabulary)\n",
    "\n",
    "# Invert dictionary\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "assert len(idx2word) == len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_context_pairs = get_focus_context_pairs(tokenized_corpus_with_OOV)\n",
    "\n",
    "print(focus_context_pairs[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pairs = get_focus_context_idx(focus_context_pairs)\n",
    "\n",
    "print(idx_pairs[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/word2vec_gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_gutenberg = Word2Vec(len(vocabulary), len(vocabulary), 10)\n",
    "train(w2v_gutenberg, idx_pairs, \"word2vec_gutenberg.pt\", early_stop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise these embeddings using Tensorboard's projector!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we use these word embeddings?\n",
    "\n",
    "Our word embeddings __are__ the `projection` weight matrix that we trained earlier on. To use this in downstream tasks, we can save our weight matrix and initalise the embeddings of our downstream network with the weights we've obtained. This is something we'll do in the next session, but extracting the weight matrix is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix = w2v_gutenberg.projection.weight.T\n",
    "print(weights_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "Next up: LSTMs, Language Modelling, and Translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
