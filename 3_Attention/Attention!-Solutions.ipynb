{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by [Zhenhao Li](mailto:zhenhao.li18@imperial.ac.uk), and [Nihir](mailto:nv419@ic.ac.uk).\n",
    "\n",
    "[The AI Core](https://theaicore.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Last week recap](#Last-week-recap)\n",
    "- [Bi-directional RNNs](#BiDirectional-RNNs)\n",
    "- [Sequence to sequence model](#Sequence-to-sequence-model)\n",
    " - [BLEU Score](#BLEU-Score)\n",
    "- [Attention](#Attention)\n",
    "- [Sequence to sequence with Attention](#Seq2seq-with-Attention)\n",
    "- [Transformer](#Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last week recap:\n",
    "\n",
    "![rnn_classification](images_lecture2/rnn_classification.png)\n",
    "![rnn_lm](images_lecture2/rnn_lm.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# import essential libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiDirectional RNNs\n",
    "\n",
    "In problems where all timesteps of the input sequence are available, bidirectional RNNs train two instead of one RNNs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. Outputs at the same step are then usually concatenated. This can provide additional useful context to the model.\n",
    "\n",
    "Q: Why is a bi-directional RNN is better than single-direction ?\n",
    "\n",
    "Imagine that you see only the left context: \"We went to ...\" This context is very general and a lot of different words can continue: nouns (London, work, cinema, doctor), verbs (join, support), etc. When we both left and right contexts the word \"sleep\" is becoming evident: \"We went to ... early but still could not wake up on time.\"\n",
    "\n",
    "\n",
    "![bi_rnn_classification](images_lecture2/bi_rnn_classification.png)\n",
    "\n",
    "![](images_lecture2/bi_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence model\n",
    "\n",
    "https://arxiv.org/abs/1409.3215 \\\n",
    "So far we have encountered some classification tasks where the inputs are of variable length. We use Recurrent Neural Networks (RNN/LSTM/GRU) to do predictions. However, when it comes to text generation, the length of outputs might also be random. In this case, we use a sequence-to-sequence model. \\\n",
    "![](images_lecture2/seq2seq.png)\n",
    "\n",
    "A sequence-to-sequence (seq2seq) model is a model that consists of two components called **Encoder** and **Decoder**. Commonly, two recurrent neural networks are used as the encoder and the decoder. The input is fed into the encoder RNN token by token, producing a fix-lengthed vector (the final hidden state) that encodes the context of all input sequence. We refer to this vector as the **context vector**. The decoder uses this context vector as the initialization of its first hidden state and inits the input with the $<sos>$ token, generating the outputs token by token.\n",
    "\n",
    "Seq2seq model is often used in NLP tasks where the lengths of both input and output are not fixed, e.g. machine translation, dialogue system. In the following part, we are going to build a vanilla seq2seq model with LSTM as encoder/decoder module on the machine translation task.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html \\\n",
    "https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation-batched.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Encoder\n",
    "We have three layers in the encoder: an embedding layer (with dropout), a RNN layer, and a linear layer. As we have known from the word representation session, we can apply a embedding layer and distributed word representation is trained jointly with the model. \n",
    "\n",
    "If we want to have a bidirectional encoder to encode both forward and backward contexts in the input, the hidden dimension of the RNN layer is doubled. Therefore, the linear layer is here to keep the same dimensionality between the encoder output and decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, layers, PAD_IDX=1, bidirectional=False, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.PAD_IDX = PAD_IDX\n",
    "        \n",
    "        \n",
    "        # If we use a bidirectional encoder to encode both forward and backward context,\n",
    "        # the dimension of the hidden state will double\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if bidirectional:\n",
    "            ff_input_dim = 2 * hidden_dim\n",
    "        else:\n",
    "            ff_input_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, self.hidden_dim, layers, dropout=dropout, \\\n",
    "                           bidirectional=bidirectional, bias=False, batch_first=True)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(ff_input_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    # x: (T, B)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T)\n",
    "        x = x.permute(1, 0)\n",
    "        \n",
    "        # x: (B, T, E)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        # outputs: (B, T, H*directions)\n",
    "        # h_n: (layers*directions, B, H)\n",
    "        outputs, (h_n, c_n) = self.rnn(x)\n",
    "        \n",
    "\n",
    "        if self.bidirectional:\n",
    "            # concatenate the forward and backward hidden states\n",
    "            h_n = torch.cat((h_n[0::2,:,:], h_n[1::2,:,:]), dim = -1)\n",
    "            c_n = torch.cat((c_n[0::2,:,:], c_n[1::2,:,:]), dim = -1)\n",
    "        \n",
    "        # h_n: (layers, B, H)\n",
    "        # c_n: (layers, B, H)\n",
    "        h_n = self.ff(h_n)\n",
    "        c_n = self.ff(c_n)\n",
    "        \n",
    "        # outputs: ()\n",
    "        return outputs, (h_n, c_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder has four layers: an embedding layer (with dropout), a unidirectional RNN layer and two linear layers. The decoder is always unidirectional in that we only generate the outputs from left to right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, layers, PAD_IDX=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, layers, dropout=dropout, batch_first=True) # we don't set bidirectional here\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # This linear layer is to ensure the output layer has the same dimensionality with the embedding layer\n",
    "        self.out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "    # x: (B)\n",
    "    def forward(self, x, hidden):\n",
    "        # we expand the dim of sequence length\n",
    "        # x: (B, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # embed: (B, 1, E)\n",
    "        embed = self.dropout(self.embedding(x))\n",
    "\n",
    "        \n",
    "        # output: (B, 1, H)\n",
    "        # h_n: (layers, B, H)\n",
    "        output, hidden = self.rnn(embed, hidden)\n",
    "        \n",
    "        # output: (B, 1, E)\n",
    "        output = self.ff(output)\n",
    "        \n",
    "        # output: (B, output_dim)\n",
    "        output = self.out(output).squeeze(1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Teacher forcing\n",
    "Teacher forcing is used in training to reduce error propagation and accelerate training. During training, the next input to the decoder can be either the ground truth token or the output by the decoder, determined by a *teacher_force_ratio*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device='cpu', with_attn=False):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.with_attn = with_attn\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        output_dim = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size, output_dim).to(self.device)\n",
    "        \n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # initialize output sequence with '<sos>'\n",
    "        dec_output = trg[0,:]\n",
    "        print(\"DEC_OUTPUT\", dec_output.shape)\n",
    "        \n",
    "        # decoder token by token\n",
    "        for t in range(1, max_len):\n",
    "            if self.with_attn:\n",
    "                dec_output, hidden, _ = self.decoder(dec_output, hidden, enc_outputs)\n",
    "            else:\n",
    "                dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "                print(\"DEC_OUTPUT RNN\", dec_output.shape)\n",
    "                \n",
    "            outputs[t] = dec_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            pred_next = dec_output.argmax(1)\n",
    "            \n",
    "            dec_output = (trg[t] if teacher_force else pred_next)\n",
    "        return outputs\n",
    "\n",
    "    # greedy search for actual translation\n",
    "    def greedy_search(self, src, sos_idx, max_len=50, return_attention=False):\n",
    "        src = src.to(self.device)\n",
    "        batch_size = src.shape[1]\n",
    "        src_len = src.shape[0]\n",
    "        \n",
    "        outputs = torch.zeros(max_len, batch_size).to(self.device)\n",
    "        \n",
    "        enc_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        \n",
    "        dec_output = torch.zeros(batch_size, dtype=torch.int64).to(device)\n",
    "        dec_output.fill_(sos_idx)\n",
    "        \n",
    "        outputs[0] = dec_output\n",
    "        \n",
    "        attentions = torch.zeros(max_len, batch_size, src_len).to(self.device)\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            if self.with_attn:\n",
    "                dec_output, hidden, attention_score = self.decoder(dec_output, hidden, enc_outputs)\n",
    "                attentions[t] = attention_score\n",
    "            else:\n",
    "                dec_output, hidden = self.decoder(dec_output, hidden)\n",
    "            \n",
    "            dec_output = dec_output.argmax(1)\n",
    "\n",
    "            outputs[t] = dec_output\n",
    "            \n",
    "        if return_attention:\n",
    "            return outputs, attentions\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished our seq2seq model, let's build a toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (embedding): Embedding(4, 10, padding_idx=1)\n",
      "    (rnn): LSTM(10, 6, num_layers=2, bias=False, batch_first=True, dropout=0.1)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=6, out_features=6, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (embedding): Embedding(4, 10, padding_idx=1)\n",
      "    (rnn): LSTM(10, 6, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=6, out_features=10, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (out): Linear(in_features=10, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM=4\n",
    "OUTPUT_DIM=4\n",
    "EMB_DIM=10\n",
    "HIDDEN_DIM=6\n",
    "LAYERS=2\n",
    "\n",
    "# define the encoder and decoder, and build the model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS)\n",
    "model = Seq2seq(enc, dec)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading with Torchtext\n",
    "https://pytorch.org/text/ \\\n",
    "Now we are running a machine translation model on actual dataset: Multi30k. Multi30k is a dataset for multi-modal machine translation. We'll only use the texts in this dataset and we load the dataset with *Torchtext*, which can help us with all the pre-processing and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# torchtext will pre-process the data, including tokenization, padding, stoi, etc.\n",
    "SRC = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"de\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "# print the number of examples in train/valid/test sets\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocab of our training set, ignoring word with frequency less than 2\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source example: ein mann mit einem orangefarbenen hut , der etwas anstarrt .\n",
      "Target example: a man in an orange hat starring at something .\n",
      "Padded target: [['<sos>', 'a', 'man', 'in', 'an', 'orange', 'hat', 'starring', 'at', 'something', '.', '<eos>']]\n",
      "Tensorized target: tensor([[   2],\n",
      "        [   4],\n",
      "        [   9],\n",
      "        [   6],\n",
      "        [  21],\n",
      "        [  86],\n",
      "        [  67],\n",
      "        [2599],\n",
      "        [  20],\n",
      "        [ 121],\n",
      "        [   5],\n",
      "        [   3]])\n"
     ]
    }
   ],
   "source": [
    "# build train/valid/test iterators, which will batch the data for us\n",
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "x = vars(test_data.examples[0])['src']\n",
    "y = vars(test_data.examples[0])['trg']\n",
    "print(\"Source example:\", \" \".join(x))\n",
    "print(\"Target example:\", \" \".join(y))\n",
    "print(\"Padded target:\", TRG.pad([y]))\n",
    "print(\"Tensorized target:\", TRG.process([y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, optimizer and criterion\n",
    "This is our model hyperparameters. In actual training, we might need to tune the hyperparameters on the validation set before evaluating on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMB_DIM=256\n",
    "HIDDEN_DIM=512\n",
    "LAYERS=1\n",
    "DROPOUT=0.5\n",
    "BIDIRECTIONAL=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# padding token\n",
    "SRC_PAD = SRC.vocab.stoi['<pad>']\n",
    "TRG_PAD = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "# build model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=SRC_PAD, bidirectional=BIDIRECTIONAL, dropout=DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=TRG_PAD, dropout=DROPOUT)\n",
    "model = Seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization sometimes boost training and the model can converge faster. We initialize the model parameters using a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq(\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(7855, 256, padding_idx=1)\n",
      "    (rnn): LSTM(256, 512, bias=False, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (embedding): Embedding(5893, 256, padding_idx=1)\n",
      "    (rnn): LSTM(256, 512, batch_first=True, dropout=0.5)\n",
      "    (ff): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (out): Linear(in_features=256, out_features=5893, bias=True)\n",
      "  )\n",
      ")\n",
      "The model has 10,412,805 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer will update the gradient everytime we back-propagate. We are using Adam as our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR=0.001\n",
    "# set optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *CrossEntropyLoss* as our loss function, which will calculate the log softmax and the negative log-likelihood. We pass the padding token in the target vocab to the criterion so that it will ignore the loss for this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our training loop.\n",
    "1. We iterate over the training iterator and get a batch of training examples\n",
    "2. The input is passed through the model and it returns the predictions\n",
    "3. We calculate the loss between the model predictions and the ground truths\n",
    "4. We back-propagate the loss and the optimizer will update the gradients\n",
    "\n",
    "To avoid exploding gradient, we clip the gradients to a maximum value every training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/seq2seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, grad_clip, num_epoch):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(src, trg)\n",
    "        \n",
    "        # exclude <sos> token\n",
    "        # outputs: (seq_len * batch_size, output_dim)\n",
    "        # trg : (seq_len * batch_size)\n",
    "        outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(outputs, trg)\n",
    "        \n",
    "        writer.add_scalar('training loss',\n",
    "                            loss.item(),\n",
    "                            num_epoch * len(iterator) + i)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Batch:\\t {0} / {1},\\t loss: {2:2.3f}'.format(i, len(iterator), loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        # clip grad to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluating loop is similar to the training loop, except that we don't want to do back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, criterion):\n",
    "    # In eval model, layers such as Dropout, BatchNorm will work in eval model\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    # this prevents the back-propagation\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            # during test time, we have no correct trg so we turn off teacher forcing\n",
    "            outputs = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            outputs = outputs[1:].view(-1, outputs.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(outputs, trg)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "![](images_lecture2/bleu1.png)\n",
    "![](images_lecture2/bleu2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, converting a batch of tensors to the text form\n",
    "def get_text_from_tensor(tensor, field, eos='<eos>'):\n",
    "    batch_output = []\n",
    "    for i in range(tensor.shape[1]):\n",
    "        sequence = tensor[:,i]\n",
    "        words = []\n",
    "        for tok_idx in sequence:\n",
    "            tok_idx = int(tok_idx)\n",
    "            token = field.vocab.itos[tok_idx]\n",
    "\n",
    "            if token == '<sos>':\n",
    "                continue\n",
    "            elif token == '<eos>' or token == '<pad>':\n",
    "                break\n",
    "            else:\n",
    "                words.append(token)\n",
    "        words = \" \".join(words)\n",
    "        batch_output.append(words)\n",
    "    return batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Could not import signal.SIGPIPE (this is expected on Windows machines)\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def test_bleu(model, iterator, trg_field, with_attention=False):\n",
    "    model.eval()\n",
    "\n",
    "    ref = []\n",
    "    hyp = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            outputs = model.greedy_search(src, trg_field.vocab.stoi['<sos>'], return_attention=with_attention)\n",
    "            \n",
    "            hyp += get_text_from_tensor(outputs, trg_field)\n",
    "            ref += get_text_from_tensor(trg, trg_field)\n",
    "            \n",
    "    # expand dim of reference list\n",
    "    # sys = ['translation_1', 'translation_2']\n",
    "    # ref = [['truth_1', 'truth_2'], ['another truth_1', 'another truth_2']]\n",
    "    ref = [ref]\n",
    "    return sacrebleu.corpus_bleu(hyp, ref, force=True).score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start our training! We keep the checkpoint with the highest valid BLEU as our best checkpoint.\n",
    "\n",
    "**The training is heavily dependent on GPU, so it might take years to train on CPU. You may skip this block and load our pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training Epoch 1:\n",
      "DEC_OUTPUT torch.Size([128])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n",
      "DEC_OUTPUT RNN torch.Size([128, 5893])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b42e904cbb35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Start training Epoch {}:'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbleu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-83335d55abbd>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, grad_clip, num_epoch)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         writer.add_scalar('training loss',\n\u001b[0m\u001b[0;32m     23\u001b[0m                             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                             num_epoch * len(iterator) + i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "CLIP = 1\n",
    "\n",
    "best_bleu = 0\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('Start training Epoch {}:'.format(i+1))\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, i)\n",
    "    valid_loss = eval(model, valid_iterator, criterion)\n",
    "    bleu = test_bleu(model, valid_iterator, TRG)\n",
    "    \n",
    "    writer.add_scalar('valid loss',\n",
    "                valid_loss,\n",
    "                i)\n",
    "    writer.add_scalar('valid ppl',\n",
    "                      math.exp(valid_loss),\n",
    "                     i)\n",
    "    writer.add_scalar('valid BLEU',\n",
    "                bleu.score,\n",
    "                i)\n",
    "    \n",
    "    if bleu.score > best_bleu:\n",
    "        best_bleu = bleu.score\n",
    "        torch.save(model.state_dict(), 'checkpoint_best-seq2seq.pt')\n",
    "    \n",
    "    print('Epoch {0} train loss: {1:.3f} | Train PPL: {2:7.3f}'.format(i+1, train_loss, math.exp(train_loss)))\n",
    "    print('Epoch {0} valid loss: {1:.3f} | Valid PPL: {2:7.3f}'.format(i+1, valid_loss, math.exp(valid_loss)))\n",
    "    print('Epoch {0} valid BLEU: {1:3.3f}'.format(i+1, bleu.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set\n",
    "Finally we can evaluate our best model on the test set.\n",
    "Let's load the pre-trained model and calculate the bleu score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('checkpoint_best-seq2seq.pt', map_location=torch.device(device)))\n",
    "print(test_bleu(model, test_iterator, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate\n",
    "Now we can translate an actual German sentence into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eos_position(tensor, field):\n",
    "    for position, tok_idx in enumerate(tensor):\n",
    "        tok_idx = int(tok_idx)\n",
    "        token = field.vocab.itos[tok_idx]\n",
    "        \n",
    "        if token == '<eos>' or token == '<pad>':\n",
    "            break\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, text, src_field, tgt_field):\n",
    "    tokens = src_field.preprocess(text)\n",
    "    input_tensor = src_field.process([tokens])\n",
    "    \n",
    "    outputs = model.greedy_search(input_tensor, tgt_field.vocab.stoi['<sos>'])\n",
    "    \n",
    "    output_text = get_text_from_tensor(outputs, tgt_field)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'ein mann mit einem orangefarbenen hut, der etwas anstarrt.', SRC, TRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem with simple Seq2seq model\n",
    "The seq2seq model we learned compress the whole input sentence into a fixed-length vector. However, there might be a problem when the input sequence become very long. It is hard to compress all necessary information in this fixed-length context vector, and the final hidden states tend to include recent words information and forget words with long distance. \n",
    "\n",
    "Rather than using a single context vector, we can use all the encoder hidden states, and now we are introducing the attention mechanism.\n",
    "\n",
    "# Attention\n",
    "https://arxiv.org/pdf/1409.0473.pdf\n",
    "https://arxiv.org/pdf/1508.04025.pdf\n",
    "The attention mechanism allows the decoder to search the source sentence and concentrate on the more relevant information (based on previous decoder hidden state), at each decoding step. \\\n",
    "At decoding step $t$, a context vector $c_i$ is computed as a weighted sum of all encoder hidden states.\n",
    "$$ c_i = \\sum^T_t\\alpha_{i,t}h_t$$\n",
    "The weight $\\alpha_{i,t}$ represent the relatedness of source token at position $t$ when the decoder is generating the target token $i$.\n",
    "Then we need to calculate the relatedness by a certain function (alignment function). $$e_{i,t}=a(s_{i-1}, h_t)$$\n",
    "To have the weights summing up to 1, we can apply a softmax over $e$, and we can have $$\\alpha_{i,t}=\\frac{exp(e_{i,t})}{\\sum_{k=1}^Texp(e_{i,k})}$$\n",
    "### Alignment function\n",
    "The alignment function can be of various types. In the original attention paper, a MLP is used: $$a(s_{i-1}, h_t) = v_a^Ttanh(W_as_{i-1}+U_ah_t)$$\n",
    "\n",
    "![](https://insidebigdata.com/wp-content/uploads/2018/09/Eleks_3.png)\n",
    "![](images/seq2seq_attention.png)\n",
    "![](images/attention_module.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq with Attention\n",
    "In the following parts, we are going to implement a sequence-to-sequence model with the attention mechanism and compare it with the previous seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(3*hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    # dec_hidden： （1, B, H)\n",
    "    # enc_outputs: (B, T, 2H)\n",
    "    def forward(self, dec_hidden, enc_outputs):\n",
    "        batch_size = dec_hidden.shape[1]\n",
    "        src_len = enc_outputs.shape[1]\n",
    "        \n",
    "        # dec_hidden: (B, 1, H)\n",
    "        dec_hidden = dec_hidden.permute(1, 0, 2)\n",
    "        # dec_hidden: (B, T, H)\n",
    "        dec_hidden = dec_hidden.repeat(1, src_len, 1)\n",
    "        \n",
    "        # energy: (B, T, H)\n",
    "        energy = torch.tanh(self.W(torch.cat((dec_hidden, enc_outputs), dim=2)))\n",
    "        \n",
    "        # attention: (B, T)\n",
    "        attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        \n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, PAD_IDX=1, dropout=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim+2*hidden_dim, hidden_dim, dropout=dropout, batch_first=True) \n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(3*hidden_dim + emb_dim, emb_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # This linear layer is to ensure the output layer has the same dimensionality with the embedding layer\n",
    "        self.out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "    \n",
    "    # x: (B)\n",
    "    # hidden=(h_n, c_n): (1, B, H)\n",
    "    # enc_outputs=(B, T, 2H)\n",
    "    def forward(self, x, hidden, enc_outputs):\n",
    "        # we expand the dim of sequence length\n",
    "        # x: (B, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # embed: (B, 1, E)\n",
    "        embed = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # attn_score: (B, 1, T)\n",
    "        attn_score = self.attention(hidden[0], enc_outputs).unsqueeze(1)\n",
    "        \n",
    "        # c: (B, 1, 2H)\n",
    "        c = torch.bmm(attn_score, enc_outputs)\n",
    "        \n",
    "        # output: (B, 1, H)\n",
    "        # h_n: (1, B, H)\n",
    "        output, hidden = self.rnn(torch.cat((embed, c), dim=2), hidden)\n",
    "        \n",
    "        # output: (B, 1, E)\n",
    "        output = self.ff(torch.cat((output, c, embed), dim=2))\n",
    "        \n",
    "        # output: (B, output_dim)\n",
    "        output = self.out(output).squeeze(1)\n",
    "        return output, hidden, attn_score.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "We use the same hyperparameters for the attention-based model, but we need to reset the optimizer and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding token\n",
    "SRC_PAD = SRC.vocab.stoi['<pad>']\n",
    "TRG_PAD = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "# build model\n",
    "enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, LAYERS, PAD_IDX=SRC_PAD, bidirectional=BIDIRECTIONAL, dropout=DROPOUT)\n",
    "dec = AttnDecoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, PAD_IDX=TRG_PAD, dropout=DROPOUT)\n",
    "model = Seq2seq(enc, dec, device, with_attn=True).to(device)\n",
    "\n",
    "# initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "LR=0.001\n",
    "# set optimizer and learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_bleu = 0\n",
    "writer = SummaryWriter('runs/seq2seq-attn')\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    print('Start training Epoch {}:'.format(i+1))\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, i)\n",
    "    valid_loss = eval(model, valid_iterator, criterion)\n",
    "    bleu = test_bleu(model, valid_iterator, TRG)\n",
    "    \n",
    "    writer.add_scalar('valid loss',\n",
    "                valid_loss,\n",
    "                i)\n",
    "    writer.add_scalar('valid ppl',\n",
    "                      math.exp(valid_loss),\n",
    "                     i)\n",
    "    writer.add_scalar('valid BLEU',\n",
    "                bleu.score,\n",
    "                i)\n",
    "    \n",
    "    if bleu.score > best_bleu:\n",
    "        best_bleu = bleu.score\n",
    "        torch.save(model.state_dict(), 'checkpoint_best-seq2seq-attn.pt')\n",
    "    \n",
    "    print('Epoch {0} train loss: {1:.3f} | Train PPL: {2:7.3f}'.format(i+1, train_loss, math.exp(train_loss)))\n",
    "    print('Epoch {0} valid loss: {1:.3f} | Valid PPL: {2:7.3f}'.format(i+1, valid_loss, math.exp(valid_loss)))\n",
    "    print('Epoch {0} valid BLEU: {1:3.3f}'.format(i+1, bleu.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint_best-seq2seq-attn.pt', map_location=torch.device(device)))\n",
    "print(test_bleu(model, test_iterator, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate\n",
    "Now we can translate an actual German sentence into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# helper function: plot attention heatmap with plotly\n",
    "def plot_attention(src_text, trg_text, attentions):\n",
    "    layout = go.Layout(\n",
    "    title=\"<b>Heatmap</b>\",\n",
    "    xaxis={\"mirror\" : \"allticks\", 'side': 'top'}, \n",
    "    yaxis={\"mirror\" : \"allticks\", 'side': 'left'}  \n",
    "    )\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "                   z=attentions.cpu().numpy(),\n",
    "                   x=src_text,\n",
    "                   y=trg_text,\n",
    "                   hoverongaps = False), layout=layout)\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, text, src_field, tgt_field):\n",
    "    tokens = src_field.preprocess(text)\n",
    "    input_tensor = src_field.process([tokens])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs, attention = model.greedy_search(input_tensor, tgt_field.vocab.stoi['<sos>'], return_attention=True)\n",
    "        eos_position = get_eos_position(outputs.squeeze(1), TRG)    \n",
    "        output_text = get_text_from_tensor(outputs, tgt_field)\n",
    "    \n",
    "    # valid_attention: (trg_len, src_len)\n",
    "    valid_attention = attention[1:eos_position+1,:,:].squeeze(1)\n",
    "    src = ['<sos>'] + tokens + ['<eos>']\n",
    "    trg = output_text[0].split() + ['<eos>']\n",
    "    plot_attention(src, trg, valid_attention)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(model, 'ein mann mit einem orangefarbenen hut, der etwas anstarrt.', SRC, TRG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "![transformerz](https://cdn.collider.com/wp-content/uploads/2017/06/transformers-5-optimus-prime-bumblebee.jpg)\n",
    "\n",
    "No... not this kind\n",
    "\n",
    "The Transformer is a non-recurrent model which has achieved SoTA results in sequence-to-sequence transduction problems. It is based entirely on attention. The recurrent nature of RNNs means that parallelisation is not possible as every hidden state relies on the hidden state before it. Impressively, because the Transformer is based on FCNNs, the possibility of parallelising the model is possible.\n",
    "\n",
    "![image.png](https://miro.medium.com/max/500/1*do7YDFF2sads0p9BnjzrWA.png)\n",
    "\n",
    "The Transformer follows a typical Encoder/Decoder architecture. It takes an input sequence of symbols $(x_1, ..., x_n)$ and maps this to a sequence of continuous representations $\\mathbf{h} = (h_1, ..., h_n)$. Given $h$, the decoder generates a symbol one element at a time $(y_1, ..., y_m)$.\n",
    "\n",
    "### Encoder\n",
    "Each encoder layer in the Transformer consists of two sub-layers. The first of these layers is the _Multi-Head Self-Attention_ module, and the second is a module called a _Position-Wise Feed-Forward Network_. Immediately following each one of these modules is a _Residual Layer Normalization_.\n",
    "\n",
    "### Decoder\n",
    "A decoder layer is similar to an encoder layer, but it has one extra module inserted: _Masked Multi-Head Attention_. We will discuss the decoder more in a further session. \n",
    "\n",
    "\n",
    "At the heart of the Transformer is this concept known as _self-attention_. Let's look at the Transformer holistically and then see exactly what this is, and why and how it solves sequence-to-sequence tasks so effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Holistic Transformer\n",
    "\n",
    "We are attempting to solve a sequence to sequence translation task: German to English using the Transformer:\n",
    "\n",
    "![whole_transformer](images/transformer.png)\n",
    "\n",
    "The Transformer is comprised of a stack of encoders and a stack of decoders. The output from the final layer of the encoder stack is sent to the decoder. The input to the first encoder is done via a special embedding module. We will look at the decoder and the embedding module in a future session.\n",
    "\n",
    "![transformer_high_level](images/transformer_high_level.png)\n",
    "\n",
    "Within the encoder stack we have a set of connected encoders. The output of one encoder is sent as input to the next encoder. An encoder consists of two sub-layers. The first sub-layer consists of a _Multi-Head Self-Attention_ module, and the second, a _Feed-Forward_ module. Within each module, a _Residual Connection_ followed by a _Layer Normalization_ is immediately applied.\n",
    "\n",
    "![encoder_first_few](images/encoder_first_few.png)\n",
    "\n",
    "Let's deconstruct what this new terminology means one by one. We'll start with _self attention_ and _multi-head self-attention_. Then we'll look at _residual connections_ followed by _layer normalization_. After we've covered the _feed-forward_ module, you've managed to understand most of the techniques in the Transformer! The encoder is simply the 6 aformenetioned things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "At the heart of the Transformer is self-attention. Self-attention is a mechanism that allows each input in a sequence to look at the whole sequence to compute a representation of the sequence.\n",
    "\n",
    "$$Attention(Q,K,V) = softmax \\left( \\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "...what?\n",
    "\n",
    "Ok. Let's look at the following sentence: `the animal didn't cross the street because it was too tired`. What does the `it` refer to in this sentence? The street or the animal? This is trivial for us as humans to answer but not for a machine. Wouldn't it be nice if we could have some way of the computer understanding what `it` referred to?\n",
    "\n",
    "This is what self-attention attempts to do. As the model processes each word in the input sequence, self-attention allows us to look at other words in the input sequence for ideas as to what we want to encode in the representation of this word.\n",
    "\n",
    "To make this clearer, think of how a hidden state in an RNN incorporates the representation of the previous words into the current representation. Self-attention is how the Transformer attempts to use other words (not just the previous words) to encode the meaning of a particular word\n",
    "\n",
    "\n",
    "![encoder_multihead](images/encoder_multihead.png)\n",
    "\n",
    "![vector_attn_score](images/vector_attn_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention in detail\n",
    "\n",
    "Self-attention is a representation which you can think of as a score. The intent is to have a representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. A bit complicated right? Hopefully by the end of this post, this meaning will be clear.\n",
    "\n",
    "Before talking about matrices, let's talk in terms of vectors. We'll have the sequence length be of dimension $T$, and the encoding/embeddings of the word be $D$ dimensional.\n",
    "\n",
    "Note that because I'm focusing on the FIRST Encoder here, the encodings of our sequence is the embeddings. But as you move up the encoder stack, the encoding of the sequence is the output from the previous encoder. Again, this is $D$ dimensional.\n",
    "\n",
    "![word_encodings](images/word_encodings.png)\n",
    "\n",
    "\n",
    "Ok. Now we're going to create three vectors for EACH input: A query vector ($q$), a key vector ($k$), and a value vector ($v$). These will be $d_k$ dimensional. $d_k$ is typically $D/8$. As far as real values go, usually $D=512$, while $d_k=64$. In our example, what is $d_k$?\n",
    "\n",
    "Ok... so how how do we get $q$, $k$, $v$? We learn it of course!\n",
    "\n",
    "how... do we learn it? We need a weights matrix which will transform our encodings into these vectors.\n",
    "This means that $W^Q$, $W^K$ and $W^V$ are all $\\in \\mathbb{R}^{D×d_k}$\n",
    "\n",
    "![qkv_vectors](images/qkv_vectors.png)\n",
    "\n",
    "\n",
    "Ok, that's nice. We understand that $q$, $k$, $v$ are different projections of the same input now; but what do the query, key, value abstractions actually mean? They're useful terms we can use to think about attention. Let's look through the following so we can see the roles they play.\n",
    "\n",
    "Recall what we defined self-attention as earlier: A representation, per input word, which tells us much how much focus each word needs to pay to every word in the sequence. For each word in our sequence, we will calculate a score by taking the dot product of the current word's query vector and the key vector for every word in the sequence.\n",
    "\n",
    "![w1_til_softmax](images/w1_til_softmax.png)\n",
    "\n",
    "Let's take stock of what we've done so far:\n",
    "- The $÷\\sqrt{d_k}$ is simply a practical scaling factor which leads to stabler gradients.\n",
    "- Softmax turns our scores into a probability distribution (each score is now between 0 and 1, and the sum of the scores = 1).\n",
    "\n",
    "We will now use these scores by multiplying them with their value vector. The intention here is that lower scoring words will have less weighting in the self-attention output as these words will now have a sense of \"irrelevantness\" (e.g. a low score like 0.0001 will \"cancel out\" its corresponding value vector).\n",
    "\n",
    "![w1_til_z1](images/w1_til_z1.png)\n",
    "\n",
    "Finally, the output for the current word is the summation of all the $softmax \\times v$ vectors. I.e. a weighted sum:\n",
    "\n",
    "![z_vector](images/z_vector.png)\n",
    "\n",
    "Ok. So that's self-attention in vector form. What about in terms of matrices?\n",
    "\n",
    "- Our input, $X$, is now a matrix of our sequence of words (i.e. $X \\in \\mathbb{R}^{T\\times D}$):\n",
    "![x_matrix](images/X_matrix_input.png)\n",
    "\n",
    "- $Q$, $K$, $V$ are now also matrices $\\in \\mathbb{R}^{T \\times d_k}$.\n",
    "- $W^Q$,$W^K$,$W^V$ stay $\\in \\mathbb{R}^{D \\times d_k}$.\n",
    "- We now simply obtain $Z \\in \\mathbb{R}^{T \\times d_k}$ by plugging $Q$, $K$, $V$ into our Attention formula.\n",
    "\n",
    "![Z_matrix](images/Z_matrix.png)\n",
    "\n",
    "\n",
    "- For the FIRST encoder, Q, K, V are determined by the embeddings of the input words\n",
    "- For the rest of the encoder stack, Q, K, V are determined by the output of the previous encoder\n",
    "- For the decoder stack, Q is determined in a similar fashion to the encoders. K and V, however, are passed from the final encoder to each of the decoders in the decoder stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = torch.Tensor([[[0.0, 0.1, 0.2, 0.3], [1.0, 1.1, 1.2, 1.3], [2.0, 2.1, 2.2, 2.3]]]) # (1, 3, 4)\n",
    "# Q_layer = nn.Linear(4, 3)\n",
    "# K_layer = nn.Linear(4, 3)\n",
    "# V_layer = nn.Linear(4, 3)\n",
    "\n",
    "# Q = Q_layer(encodings)\n",
    "# K = K_layer(encodings)\n",
    "# V = V_layer(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, dk=3):\n",
    "    Q_K_matmul = torch.matmul(Q, K.T)\n",
    "    matmul_scaled = Q_K_matmul/math.sqrt(dk)\n",
    "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attention(Q, K, V):\n",
    "    n_digits = 3\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    print ('Attention weights are:')\n",
    "    print (np.around(temp_attn, 2))\n",
    "    print ('Output is:')\n",
    "    print (np.around(temp_out, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k = torch.Tensor([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]])  # (4, 3)\n",
    "\n",
    "temp_v = torch.Tensor([[   1,0, 1],\n",
    "                      [  10,0, 2],\n",
    "                      [ 100,5, 0],\n",
    "                      [1000,6, 0]])  # (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = torch.Tensor([[0, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = torch.Tensor([[0, 0, 10]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = torch.Tensor([[10, 10, 0]])  # (1, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = torch.Tensor([[0, 0, 10], [0, 10, 0], [10, 10, 0]])  # (3, 3)\n",
    "print_attention(temp_q, temp_k, temp_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
